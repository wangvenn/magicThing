{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Team: \n",
    "        QAOutliers\n",
    "    \n",
    "    Memebers: \n",
    "        Zhanyi Qi\n",
    "        Lianyu Zeng\n",
    "        Yiheng Wang\n",
    "\n",
    "    This cell contains def that we create for our Enhanced Q&A System. \n",
    "    Our Enhanced Q&A System has seven parts:\n",
    "        I.   Sentence Retrieval by algorithm BM25\n",
    "        II.  Extracting Headword from train dataset\n",
    "        III. Question Classification with trained headwords\n",
    "        IV.  Named Entity Recognition processing\n",
    "        V.   Tagging sentence by POS Tagger\n",
    "        VI.  Answer Ranking rules\n",
    "        VII. Output\n",
    "'''\n",
    "import csv\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import nltk\n",
    "import sys \n",
    "import math\n",
    "import operator\n",
    "import re\n",
    "import string\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "'''\n",
    "    Part I. Sentence Retrieval by algorithm BM25\n",
    "    There are 7 def in this part.\n",
    "    1. get_BOW()\n",
    "                input:  a list of word\n",
    "                output: a dictionary of these words\n",
    "    2. lemmatize()\n",
    "                input:  a word\n",
    "                output: lemmatized word\n",
    "    3. input_data()\n",
    "                input:  None \n",
    "                output: train, test, and dev data sets\n",
    "    4. transform_text()\n",
    "                input:  a sentence\n",
    "                output: a list of words from this sentence by tokenizing, lemmatizing, filtering stopwords and \n",
    "                        punctuations\n",
    "    5. get_Docfrequency_SentenceBOW()\n",
    "                input:  a data set from train, test or dev\n",
    "                output: 5 values which are list, dictionary, dictionary, list and list. These values will be used\n",
    "                        in the calculating process in BM25\n",
    "    6. find_max_score_sentence()\n",
    "                input:  the index of this article, the index of this question, k1, k2 and b which are \n",
    "                        the coefficients in BM25 algorithm\n",
    "                output: the best match sentence id\n",
    "    7. BM25()\n",
    "                input:  the index of this article, k1, k2 and b which are the coefficients in BM25 algorithm\n",
    "                output: the list of sentence id that predicted by BM25\n",
    "'''\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "def input_data():\n",
    "    base_path = os.path.join('data/')\n",
    "    train_file = base_path + 'QA_train.json'\n",
    "    train_data = json.load(open(train_file))\n",
    "    test_file = base_path + 'QA_test.json'\n",
    "    test_data = json.load(open(test_file))\n",
    "    dev_file = base_path + 'QA_dev.json'\n",
    "    dev_data = json.load(open(dev_file))\n",
    "    return train_data,test_data,dev_data\n",
    "def transform_text(text):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = [lemmatize(word.lower()) for word in text]\n",
    "    result = []\n",
    "    for word in text:\n",
    "        if word not in stopwords and word not in punctuations:\n",
    "            result.append(word)\n",
    "    return result\n",
    "def get_Docfrequency_SentenceBOW(dataset):\n",
    "    #save dics, each dictionary contains document frequencies for all terms in the same article\n",
    "    question_list = []\n",
    "    #save lists, each list represent an article, saving sentences' bow\n",
    "    total_sentence_bow = []\n",
    "    #save lists, each list represent an article, saving questions' bow\n",
    "    total_question_bow = []\n",
    "    #save lists, each list represent all sentences' lengthes.\n",
    "    sent_lengthes = []\n",
    "    #save a list, each item represents the average length of sentences\n",
    "    avg_lengthes = []  \n",
    "    for article in dataset:\n",
    "        #Docfrequency\n",
    "        article_dic = defaultdict(list)\n",
    "        keyterms = [] #save all distinct terms in questions       \n",
    "        #SentenceBOW\n",
    "        bow_list = []       \n",
    "        #QuestionBOW\n",
    "        que_list = []       \n",
    "        #SentenceLength\n",
    "        sent_len = []        \n",
    "        #TotalLength\n",
    "        total_len = 0       \n",
    "        qas = article['qa']\n",
    "        sentences = article['sentences']\n",
    "        for qa in qas:\n",
    "            question = qa['question']\n",
    "            newquestion = transform_text(question)\n",
    "            #QuestionBOW\n",
    "            que_list.append(get_BOW(newquestion))          \n",
    "            keyterms.extend(newquestion)\n",
    "        keyterms = set(keyterms)       \n",
    "        #save sentences' BOW in list sen_BOW\n",
    "        sen_words = []\n",
    "        for sent in sentences:\n",
    "            sent = transform_text(sent)\n",
    "            #Docfrequency\n",
    "            sen_words.append(sent)         \n",
    "            #SentenceBOW\n",
    "            bow_list.append(get_BOW(sent))         \n",
    "            #SentenceLength\n",
    "            sent_len.append(len(sent))           \n",
    "            #TotalLength\n",
    "            total_len += len(sent)     \n",
    "        #calculate doc frequency    \n",
    "        for term in keyterms:\n",
    "            for i,bow in enumerate(sen_words):\n",
    "                if term in bow:\n",
    "                    article_dic[term].append(i)                   \n",
    "        #Docfrequency\n",
    "        question_list.append(article_dic)\n",
    "        #SentenceBOW\n",
    "        total_sentence_bow.append(bow_list)\n",
    "        #QuestionBOW\n",
    "        total_question_bow.append(que_list)\n",
    "        #SentenceLength\n",
    "        sent_lengthes.append(sent_len)\n",
    "        #AverageLength\n",
    "        avg_lengthes.append(float(total_len)/len(sentences))       \n",
    "    return question_list, total_sentence_bow, total_question_bow, sent_lengthes, avg_lengthes\n",
    "def find_max_n_sentences(articles_index,index,k1,k2,b,n):\n",
    "    #Get the dictionary of this question from QuestionBOW\n",
    "    query_dict = total_question_bow[articles_index][index]\n",
    "    scores = {}\n",
    "    for index in range(len(total_sentence_bow[articles_index])):     \n",
    "        score = 0  \n",
    "        #Get the dictionary of this sentence from SentenceBOW\n",
    "        sentence_dict = total_sentence_bow[articles_index][index]\n",
    "        #Calculate the score of each word in question query\n",
    "        for word in query_dict:\n",
    "            document_fre_list = question_list[articles_index].get(word,None)\n",
    "            N = len(total_sentence_bow[articles_index])\n",
    "            n_qi = 0\n",
    "            if document_fre_list != None:\n",
    "                n_qi = len(document_fre_list)\n",
    "            else:\n",
    "                n_qi = 0\n",
    "            fi = sentence_dict.get(word,0)\n",
    "            qfi = query_dict.get(word,0)\n",
    "            dl = sent_lengthes[articles_index][index]\n",
    "            avgdl = avg_lengthes[articles_index]\n",
    "            \n",
    "            K = k1*(1-b+b*(float(dl)/avgdl)) \n",
    "            W = math.log((N-n_qi+0.5)/(n_qi+0.5))\n",
    "            R = (fi*(k1+1))/(fi+K)*qfi*(k2+1)/(qfi+k2)\n",
    "            score += W*R\n",
    "        scores[index] = score\n",
    "    scores = sorted(scores.items(), key=operator.itemgetter(1),reverse=True)[:n]\n",
    "    return scores\n",
    "def BM25_n(articles_index,k1,k2,b,n):\n",
    "    total_queries = len(total_question_bow[articles_index])\n",
    "    count = 0\n",
    "    correct_id = []\n",
    "    correct_id_weight = []\n",
    "    for index in range(len(total_question_bow[articles_index])):\n",
    "        poss_results = find_max_n_sentences(articles_index,index,k1,k2,b,n)\n",
    "        guess_ids = []\n",
    "        weights = []\n",
    "        #Save several most relative sentences with this question\n",
    "        for sent in poss_results:\n",
    "            guess_ids.append(sent[0])\n",
    "            weights.append(sent[1])\n",
    "        correct_id.append(guess_ids)\n",
    "        correct_id_weight.append(weights)\n",
    "    return correct_id,correct_id_weight\n",
    "'''\n",
    "    Part II. Extracting Headword from train dataset\n",
    "    There are 2 def in this part.\n",
    "    1. getHeadWord()\n",
    "                input:  the question query\n",
    "                output: the list of headwords of this query\n",
    "'''\n",
    "def getHeadWord(text):\n",
    "    text = text.encode('ascii','replace')\n",
    "    pos =  pos_tag(word_tokenize(text))\n",
    "    #Edit the POS tag of 'Which'\n",
    "    if pos[0]==('Which', 'JJ'):\n",
    "        pos[0] = ('Which', 'WHICH')\n",
    "    for i,item in enumerate(pos):   \n",
    "        word = item[0].lower()\n",
    "        ppos = item[1]\n",
    "        if word=='what' and (ppos=='WP' or ppos=='WDT'):\n",
    "            pos[i] = ('what', 'WHAT')   \n",
    "        elif word=='which' and ppos=='JJ':\n",
    "            pos[i] = ('which', 'WHICH')  \n",
    "    #Define the Chunk grammar of question\n",
    "    grammar = r\"\"\"\n",
    "                V: {<V.*>}          # Verb\n",
    "                HEAD:\n",
    "                    {<IN>?<WHAT><V>?<DT>?<JJ.*|CD>*<V>?<IN>?<NN.*>+}\n",
    "                    {<IN>?<WHAT><V>?<DT>?<JJ.*|CD>*<V>?<IN>?<VBG.*>+}\n",
    "                    }<IN>?<WHAT><V>?<DT>?<JJ.*|CD>*<V>?<IN>?{  \n",
    "                \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar) \n",
    "    result = []\n",
    "    tree = cp.parse(pos)\n",
    "    #Get the result from tree\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label()=='HEAD':\n",
    "            phrase = u' '.join([word for word,pos in subtree.leaves()])\n",
    "            phrase_list = phrase.split()\n",
    "            if len(phrase_list)>1:\n",
    "                ph = phrase_list[-1]\n",
    "            else:\n",
    "                ph = phrase\n",
    "            result.append(ph)     \n",
    "    return result\n",
    "'''\n",
    "    Part III. Question Classification with trained headwords\n",
    "    There are 2 def in this part.\n",
    "    1. transfer_pos_question()\n",
    "                input:  pos of a question query\n",
    "                output: edited pos of this question query\n",
    "    2. get_continuous_chunks()\n",
    "                input:  a question query\n",
    "                output: chunk of this question query, like [('WHEN','when ...')...]\n",
    "'''\n",
    "def transfer_pos_question(pos):\n",
    "    new_pos = []\n",
    "    #Edit the pos tag from nltk pos_tag to customized tag\n",
    "    for (word,wtype) in pos:\n",
    "        if word.lower() == 'what' or word.lower() == 'what\\'s':\n",
    "            new_pos.append((word,'WHAT'))\n",
    "        elif word.lower() == 'do' or word.lower() == 'does' or word.lower() == 'did':\n",
    "            new_pos.append((word,'DO'))\n",
    "        #Tag the word from Trainning result\n",
    "        elif word in number_list:\n",
    "            new_pos.append((word,'NUMBER'))\n",
    "        elif word in year_list:\n",
    "            new_pos.append((word,'YEAR'))\n",
    "        elif word in name_list:\n",
    "            new_pos.append((word,'NAME'))\n",
    "        elif word in location_list:\n",
    "            new_pos.append((word,'LOC'))\n",
    "        elif word.lower() == 'is' or word.lower() == 'was' or word.lower() == 'are' or word.lower() == 'were' or word.lower() == 'be':\n",
    "            new_pos.append((word,'BE'))\n",
    "        elif word.lower() == 'when':\n",
    "            new_pos.append((word,'WHEN'))\n",
    "        elif word.lower() == 'where':\n",
    "            new_pos.append((word,'WHERE'))\n",
    "        elif word.lower() == 'can':\n",
    "            new_pos.append((word,'CAN'))\n",
    "        elif word.lower() == 'how':\n",
    "            new_pos.append((word,'HOW'))\n",
    "        elif word.lower() == 'who' or word.lower() == 'whom' or word.lower() == 'whose'  or word.lower() == 'whos':\n",
    "            new_pos.append((word,'WHO'))\n",
    "        elif word.lower() == 'which':\n",
    "            new_pos.append((word,'WHICH'))\n",
    "        elif word.lower() == 'define':\n",
    "            new_pos.append((word,'DEFINE'))\n",
    "        elif word.lower() == 'should':\n",
    "            new_pos.append((word,'SHOULD'))\n",
    "        elif word.lower() == 'why' or word.lower() == 'wy':\n",
    "            new_pos.append((word,'WHY'))\n",
    "        else:\n",
    "            new_pos.append((word,wtype))\n",
    "    return new_pos\n",
    "def get_continuous_chunks(text):\n",
    "    t = copy.deepcopy(text)\n",
    "    #Get pos tag by nltk pos tagger of this qyestion query\n",
    "    pos = pos_tag(nltk.word_tokenize(t))\n",
    "    #Edit the pos tag to customized pos tag\n",
    "    pos = transfer_pos_question(pos)\n",
    "    #Define different grammar for different types of question\n",
    "    grammar = r\"\"\"\n",
    "                WHAT: \n",
    "                    {<WHAT>}\n",
    "                    {<WHICH>}\n",
    "                    {<DEFINE>}\n",
    "                WHO:\n",
    "                    {<WHO>}\n",
    "                    {<WHAT><BE>?<DT>?<JJ|RB>*<NAME>}\n",
    "                    {<WHAT><JJ|RB>*<NN>+<NAME>}\n",
    "                NUMBER:\n",
    "                    {<WHICH><NUMBER>}\n",
    "                    {<HOW><NUMBER>}\n",
    "                    {<WHAT><BE>?<DT>?<JJ>?<NN>*<JJ>?<NUMBER>}\n",
    "                WHEN:\n",
    "                    {<WHICH><YEAR>}\n",
    "                    {<WHAT><BE>?<DT>?<JJ>?<NN>*<JJ>?<YEAR>}\n",
    "                    {<WHEN>}\n",
    "                WHERE:\n",
    "                    {<WHERE>}\n",
    "                    {<WHAT><LOC>}\n",
    "                    {<WHAT><BE>?<DT>?<RBS>?<JJ>?<LOC>}\n",
    "                HOW:\n",
    "                    {<CAN>}\n",
    "                    {<DO>}\n",
    "                    {<SHOULD>}\n",
    "                    {<WHY>}\n",
    "                    {<HOW>}\n",
    "\n",
    "                \"\"\"\n",
    "    #Load nltk RegexpParser to analyse the grammar\n",
    "    cp = nltk.RegexpParser(grammar) \n",
    "    result = []\n",
    "    tree = cp.parse(pos)\n",
    "    #Parse the grammar tree\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() != 'S':\n",
    "            phrase = u''\n",
    "            for word,pos in subtree.leaves():\n",
    "                if word == ',':\n",
    "                    phrase = phrase + word\n",
    "                else:\n",
    "                    phrase = phrase + u' '\n",
    "                    phrase = phrase + word\n",
    "            result.append((subtree.label(),phrase[1:]))         \n",
    "    return result\n",
    "'''\n",
    "    Part IV. Named Entity Recognition processing\n",
    "    There are 5 def in this part.\n",
    "    1. input_NER()\n",
    "                input:  None\n",
    "                output: StanfordNER model file, StanfordNER jar file\n",
    "    2. analyse_NER()\n",
    "                input:  list of sentences that already tagged by edited NER tool\n",
    "                output: remove 'O' tag and combine continuous same tags as one entity\n",
    "    3. parse_NER()\n",
    "                input:  list of sentences that already tagged by Stanford NER tool \n",
    "                output: list of sentences that correct the NER result manully(by some rules)\n",
    "    4. extract_NER()\n",
    "                input:  NER sentence and the model number\n",
    "                output: result of orderred entities from this NER sentence \n",
    "    5. parse_token()\n",
    "                input:  a token of a sentence\n",
    "                output: correct mistake in tokens, and return the new tokens \n",
    "'''\n",
    "def input_NER():\n",
    "    stanford_dir = os.path.join('stanford-ner-2016-10-31')\n",
    "    jarfile = os.path.join(stanford_dir,'stanford-ner.jar')\n",
    "    modelfile = os.path.join(stanford_dir,'classifiers/english.all.3class.distsim.crf.ser.gz')\n",
    "    return modelfile,jarfile\n",
    "def analyse_NER(ner_sentences):\n",
    "    result_sentences = []\n",
    "    for ner_sentence in ner_sentences:\n",
    "        result_sentence = []\n",
    "        perv_type = u'O'\n",
    "        word = u''\n",
    "        conjunction = u''\n",
    "        conjunc_flag = False\n",
    "        for index,(entity,etype) in enumerate(ner_sentence):\n",
    "            if perv_type == u'O' and etype != u'O':              \n",
    "                perv_type = etype\n",
    "                word = entity + u' '\n",
    "            elif word != u'':\n",
    "                if etype == u'O':\n",
    "                    if entity not in conjunction_word:\n",
    "                        result_sentence.append((word[:-1],perv_type))\n",
    "                        word = u''\n",
    "                        perv_type = u'O'\n",
    "                        if conjunction != u'':\n",
    "                            conjunction = u''\n",
    "                            conjunc_flag = False\n",
    "                    else:\n",
    "                        if conjunction != u'':\n",
    "                            conjunction = u''\n",
    "                            conjunc_flag = False\n",
    "                        else:\n",
    "                            conjunction = entity\n",
    "                            conjunc_flag = True\n",
    "                elif etype != perv_type:\n",
    "                    result_sentence.append((word[:-1],perv_type))\n",
    "                    word = entity + u' '\n",
    "                    perv_type = etype\n",
    "                    conjunction = u''\n",
    "                    conjunc_flag = False\n",
    "                elif etype == perv_type:\n",
    "                    if conjunc_flag:\n",
    "                        if conjunction == u',':\n",
    "                            word = word[:-1] + conjunction + u' ' + entity + u' '\n",
    "                        else:\n",
    "                            word = word + conjunction + u' ' + entity + u' '\n",
    "                        conjunction = u''\n",
    "                        conjunc_flag = False\n",
    "                    else:\n",
    "                        if entity in ['%'] or word == u'$ ':\n",
    "                            word = word[:-1] + entity + u' '\n",
    "                        else:\n",
    "                            word = word + entity + u' '\n",
    "        if word != u'':\n",
    "            result_sentence.append((word[:-1],perv_type))\n",
    "        result_sentences.append(result_sentence)      \n",
    "    return result_sentences\n",
    "def parse_NER(ner_sentences):\n",
    "    pattern_number = re.compile(r'([0-9]+|\\%|\\$)')\n",
    "    year_number = re.compile(r'([0-9]{4}s?)')\n",
    "    result_sentences = []\n",
    "    for ner_sentence in ner_sentences:\n",
    "        result_sentence = []\n",
    "        for index,(entity,etype) in enumerate(ner_sentence):\n",
    "            if entity != u'':\n",
    "                entity.replace(u'\\u2013',u'-')\n",
    "                entity.replace(u'\\u2014',u'-')\n",
    "                entity.replace(u'\\u2212',u'-')\n",
    "                entity.replace(u'\\u2044',u'%')\n",
    "                if etype == u'O':\n",
    "                    if year_number.search(entity):\n",
    "                        result_sentence.append((entity,u'YEAR'))\n",
    "                    elif pattern_number.search(entity) or entity in time_word:\n",
    "                        result_sentence.append((entity,u'NUMBER'))\n",
    "                    elif u'-' in entity:\n",
    "                        word_seperate = entity.split(u'-')\n",
    "                        for word in word_seperate:\n",
    "                            if word in time_word:\n",
    "                                result_sentence.append((entity,u'NUMBER'))\n",
    "                                break\n",
    "                    elif entity in location_word:\n",
    "                        result_sentence.append((entity,u'LOCATION'))\n",
    "                    elif index == 0 and entity.lower() not in stopwords:\n",
    "                        result_sentence.append((entity,u'ORGANIZATION'))\n",
    "                    elif index != 0 and entity[0].isupper():\n",
    "                        result_sentence.append((entity,u'ORGANIZATION'))\n",
    "                    else:\n",
    "                        result_sentence.append((entity,etype))\n",
    "                elif entity in ['(',')']:\n",
    "                    result_sentence.append((entity,u'O'))\n",
    "                else:\n",
    "                    result_sentence.append((entity,etype))\n",
    "        result_sentences.append(result_sentence)       \n",
    "    return result_sentences\n",
    "def extract_NER(parse_ner_sentence,mode):\n",
    "    result = []\n",
    "    if mode == 0:\n",
    "        #PERSON\n",
    "        for entity,etype in parse_ner_sentence:\n",
    "            if etype == u'PERSON':\n",
    "                result.append(entity)\n",
    "    elif mode == 1:\n",
    "        #NUMBER\n",
    "        for entity,etype in parse_ner_sentence:\n",
    "            if etype == u'NUMBER':\n",
    "                result.append(entity)\n",
    "    elif mode == 2:\n",
    "        #LOCATION\n",
    "        for entity,etype in parse_ner_sentence:\n",
    "            if etype == u'LOCATION':\n",
    "                result.append(entity)\n",
    "    elif mode == 3:\n",
    "        #ORGANIZATION\n",
    "        for entity,etype in parse_ner_sentence:\n",
    "            if etype == u'ORGANIZATION':\n",
    "                result.append(entity)\n",
    "    elif mode == 4:\n",
    "        #YEAR\n",
    "        for entity,etype in parse_ner_sentence:\n",
    "            if etype == u'YEAR':\n",
    "                result.append(entity)\n",
    "    return result\n",
    "def parse_token(token_sentence):\n",
    "    result = []\n",
    "    for index,word in enumerate(token_sentence):\n",
    "        if index != 0 and index != (len(token_sentence)-1) and word == u'.':\n",
    "            last_word = result[-1]\n",
    "            last_word = last_word + u'.'\n",
    "            result = result[:-1]\n",
    "            result.append(last_word)\n",
    "        else:\n",
    "            result.append(word)\n",
    "    return result\n",
    "'''\n",
    "    Part V. Tagging sentence by POS Tagger\n",
    "    There are 2 def in this part.\n",
    "    1. transfer_pos_sentence()\n",
    "                input:  POS tag of the sentence\n",
    "                output: Edited POS tag of the sentence\n",
    "    2. get_continuous_chunks_sentence()\n",
    "                input:  the sentence, the type of question (0: WHAT)\n",
    "                output: the POS tag, and the result by loading customized chunk grammar \n",
    "'''\n",
    "def transfer_pos_sentence(pos):\n",
    "    new_pos = []   \n",
    "    for (word,wtype) in pos:\n",
    "        if word.lower() == 'and' or word.lower() == 'or':\n",
    "            new_pos.append((word,'POSICC'))\n",
    "        elif word.lower() == 'with':\n",
    "            new_pos.append((word,'WITH'))\n",
    "        elif word.lower() == 'a' or word.lower() == 'an':\n",
    "            new_pos.append((word,'A'))\n",
    "        elif word == '\"':\n",
    "            new_pos.append((word,'\"'))\n",
    "        elif word == 'around':\n",
    "            new_pos.append((word,'AROUND'))\n",
    "        else:\n",
    "            new_pos.append((word,wtype))\n",
    "    return new_pos\n",
    "def get_continuous_chunks_sentence(text,texttype):\n",
    "    t = copy.deepcopy(text)\n",
    "    pos = pos_tag(nltk.word_tokenize(t))\n",
    "    if texttype==0:\n",
    "        #WHAT\n",
    "        pos = transfer_pos_sentence(pos)\n",
    "        grammar = r\"\"\"\n",
    "                    J:\n",
    "                        {<JJ.*><VBN>}\n",
    "                        {<JJ.*><POSICC><JJ.*>}   \n",
    "                        {<JJ.*>+}\n",
    "                        {<NN.*><POS>}\n",
    "                    N:\n",
    "                        {<CD>+<NN.*>}\n",
    "                        {<A>?<NN.*>?<J>?<NN.*>+}\n",
    "                        <\\\">{<A>?<J>?<NN.*>+}<\\\">\n",
    "                    COMBON:\n",
    "                        {(<N><,>)*<N><,>?<POSICC><N>}\n",
    "                    NWC:\n",
    "                        {<N><WITH><COMBON>}\n",
    "                    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar) \n",
    "    result = []\n",
    "    poss = copy.deepcopy(pos)\n",
    "    tree = cp.parse(pos)\n",
    "    flag = 0\n",
    "    for i in range (len(tree)):   \n",
    "        if type(tree[i]) != tuple:\n",
    "            subtree = tree[i]\n",
    "            if texttype==0 and subtree.label() != 'S':\n",
    "                phrase = u''\n",
    "                for word,pos in subtree.leaves():\n",
    "                    if word == ',':\n",
    "                        phrase = phrase + word\n",
    "                    else:\n",
    "                        phrase = phrase + u' '\n",
    "                        phrase = phrase + word\n",
    "                result.append((subtree.label(),phrase[1:]))\n",
    "            elif subtree.label() != 'S':\n",
    "                phrase = u' '.join([word for word,pos in subtree.leaves()])\n",
    "                result.append((subtree.label(),phrase))      \n",
    "    return poss,result\n",
    "'''\n",
    "    Part VI. Answer Ranking rules\n",
    "    There are 4 def in this part.\n",
    "    1. get_open_class_words()\n",
    "                input:  token of the question query\n",
    "                output: removed stopword and punctuation of the list\n",
    "    2. rank_rule_1()\n",
    "                input:  entity and the question query\n",
    "                output: score of rule 1\n",
    "    3. rank_rule_3()\n",
    "                input:  answer sentence, token of the answer snetence, entity and the token of question query \n",
    "                output: score of rule 3\n",
    "    4. screen_out_answer_WHAT()\n",
    "                input:  the list of alternative answers\n",
    "                output: the list of alternative answers after filtering\n",
    "'''  \n",
    "def rank_rule_1(entity,query):\n",
    "    #lower scores for content words also appear in the query\n",
    "    count = 0\n",
    "    length = len(entity)\n",
    "    for word in entity:\n",
    "        word = lemmatize(word.lower())\n",
    "        if word not in stopwords:\n",
    "            if word in query:\n",
    "                count += 1\n",
    "    if length == 0:\n",
    "        score = 0\n",
    "    else:\n",
    "        score = 1 - float(count)/length\n",
    "    return score\n",
    "def get_open_class_words(query):\n",
    "    result = []\n",
    "    for index in range(len(query)):\n",
    "        if query[index] not in stopwords:\n",
    "            if query[index] not in string.punctuation:\n",
    "                result.append(query[index])\n",
    "    return result\n",
    "def rank_rule_3(answer_sentence,sentence,entity,query):\n",
    "    #higher scores for closer distance between an entity and the headword\n",
    "    #step 1: using a filter to extract \"useful\" open-class words\n",
    "    results = get_open_class_words(query)\n",
    "    sent = sentence\n",
    "    original_sent = answer_sentence\n",
    "    entity_loc = []\n",
    "    query_loc = []\n",
    "    for word in entity:\n",
    "        if word in original_sent:\n",
    "            entity_loc.append(original_sent.index(word))\n",
    "    for q in results:\n",
    "        if q in sent:\n",
    "            query_loc.append(sent.index(q))\n",
    "    min_dist = len(original_sent)\n",
    "    if query_loc != []:\n",
    "        for i in query_loc:\n",
    "            for j in entity_loc:\n",
    "                dist = abs(i - j)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    \n",
    "    return 1 - float(min_dist)/len(original_sent)\n",
    "def screen_out_answer_WHAT(result):\n",
    "    answer_list = []\n",
    "    for wtype, word in result:\n",
    "        if wtype != 'J':\n",
    "            answer_list.append(word)\n",
    "    return answer_list\n",
    "\n",
    "'''\n",
    "    Part VII. Output\n",
    "    There are 1 def in this part.\n",
    "    1. output_result()\n",
    "                input:  filename\n",
    "                output: putting result in this file\n",
    "'''\n",
    "def output_result(filename):\n",
    "    predictions_file = open(filename, \"wb\")\n",
    "    open_file_object = csv.writer(predictions_file)\n",
    "    open_file_object.writerow([\"id\",\"answer\"])\n",
    "    for i in range(len(answers)):\n",
    "        open_file_object.writerow([ids[i], answers[i].encode(\"utf-8\")])\n",
    "    predictions_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    This cell is for initializing dataset and tools.\n",
    "'''\n",
    "#Initialize datasets and tools\n",
    "train,test,dev = input_data()\n",
    "model,jar = input_NER()\n",
    "st = StanfordNERTagger(model,jar)\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "#Define dataset for customized function\n",
    "punctuations = [',','\\'\\'','?','\\'','.','%','(',')',';','``']\n",
    "#Customized feature words in sentence\n",
    "time_word = [\n",
    "    'one','two','three','four','five','six','seven','eight','nine',\n",
    "    'January','February','March','April','May','June','July','August','September','October','November','December',\n",
    "    'million','billion',\n",
    "    'minutes','hours','years','times',\n",
    "    'mm','miles','inches','foot','feet',\n",
    "    'late','early','around','over',\n",
    "    'persons','seasons','square',\n",
    "    'spring','summer','fall','autumn','winter'\n",
    "]\n",
    "location_word = [\n",
    "    'southwest','southeast','northwest','northeast'\n",
    "]\n",
    "conjunction_word = ['and','of']\n",
    "#Trained Headword\n",
    "year_headword = {}\n",
    "organization_headword = {}\n",
    "person_headword = {}\n",
    "number_headword = {}\n",
    "location_headword = {}\n",
    "#Threshold to avoid overfitting (the doc frequency of the headword)\n",
    "threshold = 1\n",
    "#Initialize data for BM25 processing\n",
    "question_list, total_sentence_bow, total_question_bow, sent_lengthes, avg_lengthes = get_Docfrequency_SentenceBOW(test)\n",
    "#Initialize the out put file\n",
    "filename = \"result_enhance.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    This cell is for BM25 processing.\n",
    "'''\n",
    "#Set the range of k1, k2 and b\n",
    "k1_list = [0.78]\n",
    "k2_list = [0]\n",
    "b_list = [0.5]\n",
    "#Store the predicting result by BM25 in result_sentences\n",
    "result_sentences = []\n",
    "result_sentences_weight = []\n",
    "test_length = len(test)\n",
    "for k1 in k1_list:\n",
    "    for k2 in k2_list:\n",
    "        for b in b_list:\n",
    "            for i in range(0,test_length):\n",
    "                #the amount of extract sentences\n",
    "                amount = 5\n",
    "                correct_id,correct_id_weight = BM25_n(i,k1,k2,b,amount)\n",
    "                result_sentences.append(correct_id)\n",
    "                result_sentences_weight.append(correct_id_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Extracting Headwords from train dataset\n",
      "0 / 360  trains Start...\n",
      "1 / 360  trains Start...\n",
      "2 / 360  trains Start...\n",
      "3 / 360  trains Start...\n",
      "4 / 360  trains Start...\n",
      "5 / 360  trains Start...\n",
      "6 / 360  trains Start...\n",
      "7 / 360  trains Start...\n",
      "8 / 360  trains Start...\n",
      "9 / 360  trains Start...\n",
      "10 / 360  trains Start...\n",
      "11 / 360  trains Start...\n",
      "12 / 360  trains Start...\n",
      "13 / 360  trains Start...\n",
      "14 / 360  trains Start...\n",
      "15 / 360  trains Start...\n",
      "16 / 360  trains Start...\n",
      "17 / 360  trains Start...\n",
      "18 / 360  trains Start...\n",
      "19 / 360  trains Start...\n",
      "20 / 360  trains Start...\n",
      "21 / 360  trains Start...\n",
      "22 / 360  trains Start...\n",
      "23 / 360  trains Start...\n",
      "24 / 360  trains Start...\n",
      "25 / 360  trains Start...\n",
      "26 / 360  trains Start...\n",
      "27 / 360  trains Start...\n",
      "28 / 360  trains Start...\n",
      "29 / 360  trains Start...\n",
      "30 / 360  trains Start...\n",
      "31 / 360  trains Start...\n",
      "32 / 360  trains Start...\n",
      "33 / 360  trains Start...\n",
      "34 / 360  trains Start...\n",
      "35 / 360  trains Start...\n",
      "36 / 360  trains Start...\n",
      "37 / 360  trains Start...\n",
      "38 / 360  trains Start...\n",
      "39 / 360  trains Start...\n",
      "40 / 360  trains Start...\n",
      "41 / 360  trains Start...\n",
      "42 / 360  trains Start...\n",
      "43 / 360  trains Start...\n",
      "44 / 360  trains Start...\n",
      "45 / 360  trains Start...\n",
      "46 / 360  trains Start...\n",
      "47 / 360  trains Start...\n",
      "48 / 360  trains Start...\n",
      "49 / 360  trains Start...\n",
      "50 / 360  trains Start...\n",
      "51 / 360  trains Start...\n",
      "52 / 360  trains Start...\n",
      "53 / 360  trains Start...\n",
      "54 / 360  trains Start...\n",
      "55 / 360  trains Start...\n",
      "56 / 360  trains Start...\n",
      "57 / 360  trains Start...\n",
      "58 / 360  trains Start...\n",
      "59 / 360  trains Start...\n",
      "60 / 360  trains Start...\n",
      "61 / 360  trains Start...\n",
      "62 / 360  trains Start...\n",
      "63 / 360  trains Start...\n",
      "64 / 360  trains Start...\n",
      "65 / 360  trains Start...\n",
      "66 / 360  trains Start...\n",
      "67 / 360  trains Start...\n",
      "68 / 360  trains Start...\n",
      "69 / 360  trains Start...\n",
      "70 / 360  trains Start...\n",
      "71 / 360  trains Start...\n",
      "72 / 360  trains Start...\n",
      "73 / 360  trains Start...\n",
      "74 / 360  trains Start...\n",
      "75 / 360  trains Start...\n",
      "76 / 360  trains Start...\n",
      "77 / 360  trains Start...\n",
      "78 / 360  trains Start...\n",
      "79 / 360  trains Start...\n",
      "80 / 360  trains Start...\n",
      "81 / 360  trains Start...\n",
      "82 / 360  trains Start...\n",
      "83 / 360  trains Start...\n",
      "84 / 360  trains Start...\n",
      "85 / 360  trains Start...\n",
      "86 / 360  trains Start...\n",
      "87 / 360  trains Start...\n",
      "88 / 360  trains Start...\n",
      "89 / 360  trains Start...\n",
      "90 / 360  trains Start...\n",
      "91 / 360  trains Start...\n",
      "92 / 360  trains Start...\n",
      "93 / 360  trains Start...\n",
      "94 / 360  trains Start...\n",
      "95 / 360  trains Start...\n",
      "96 / 360  trains Start...\n",
      "97 / 360  trains Start...\n",
      "98 / 360  trains Start...\n",
      "99 / 360  trains Start...\n",
      "100 / 360  trains Start...\n",
      "101 / 360  trains Start...\n",
      "102 / 360  trains Start...\n",
      "103 / 360  trains Start...\n",
      "104 / 360  trains Start...\n",
      "105 / 360  trains Start...\n",
      "106 / 360  trains Start...\n",
      "107 / 360  trains Start...\n",
      "108 / 360  trains Start...\n",
      "109 / 360  trains Start...\n",
      "110 / 360  trains Start...\n",
      "111 / 360  trains Start...\n",
      "112 / 360  trains Start...\n",
      "113 / 360  trains Start...\n",
      "114 / 360  trains Start...\n",
      "115 / 360  trains Start...\n",
      "116 / 360  trains Start...\n",
      "117 / 360  trains Start...\n",
      "118 / 360  trains Start...\n",
      "119 / 360  trains Start...\n",
      "120 / 360  trains Start...\n",
      "121 / 360  trains Start...\n",
      "122 / 360  trains Start...\n",
      "123 / 360  trains Start...\n",
      "124 / 360  trains Start...\n",
      "125 / 360  trains Start...\n",
      "126 / 360  trains Start...\n",
      "127 / 360  trains Start...\n",
      "128 / 360  trains Start...\n",
      "129 / 360  trains Start...\n",
      "130 / 360  trains Start...\n",
      "131 / 360  trains Start...\n",
      "132 / 360  trains Start...\n",
      "133 / 360  trains Start...\n",
      "134 / 360  trains Start...\n",
      "135 / 360  trains Start...\n",
      "136 / 360  trains Start...\n",
      "137 / 360  trains Start...\n",
      "138 / 360  trains Start...\n",
      "139 / 360  trains Start...\n",
      "140 / 360  trains Start...\n",
      "141 / 360  trains Start...\n",
      "142 / 360  trains Start...\n",
      "143 / 360  trains Start...\n",
      "144 / 360  trains Start...\n",
      "145 / 360  trains Start...\n",
      "146 / 360  trains Start...\n",
      "147 / 360  trains Start...\n",
      "148 / 360  trains Start...\n",
      "149 / 360  trains Start...\n",
      "150 / 360  trains Start...\n",
      "151 / 360  trains Start...\n",
      "152 / 360  trains Start...\n",
      "153 / 360  trains Start...\n",
      "154 / 360  trains Start...\n",
      "155 / 360  trains Start...\n",
      "156 / 360  trains Start...\n",
      "157 / 360  trains Start...\n",
      "158 / 360  trains Start...\n",
      "159 / 360  trains Start...\n",
      "160 / 360  trains Start...\n",
      "161 / 360  trains Start...\n",
      "162 / 360  trains Start...\n",
      "163 / 360  trains Start...\n",
      "164 / 360  trains Start...\n",
      "165 / 360  trains Start...\n",
      "166 / 360  trains Start...\n",
      "167 / 360  trains Start...\n",
      "168 / 360  trains Start...\n",
      "169 / 360  trains Start...\n",
      "170 / 360  trains Start...\n",
      "171 / 360  trains Start...\n",
      "172 / 360  trains Start...\n",
      "173 / 360  trains Start...\n",
      "174 / 360  trains Start...\n",
      "175 / 360  trains Start...\n",
      "176 / 360  trains Start...\n",
      "177 / 360  trains Start...\n",
      "178 / 360  trains Start...\n",
      "179 / 360  trains Start...\n",
      "180 / 360  trains Start...\n",
      "181 / 360  trains Start...\n",
      "182 / 360  trains Start...\n",
      "183 / 360  trains Start...\n",
      "184 / 360  trains Start...\n",
      "185 / 360  trains Start...\n",
      "186 / 360  trains Start...\n",
      "187 / 360  trains Start...\n",
      "188 / 360  trains Start...\n",
      "189 / 360  trains Start...\n",
      "190 / 360  trains Start...\n",
      "191 / 360  trains Start...\n",
      "192 / 360  trains Start...\n",
      "193 / 360  trains Start...\n",
      "194 / 360  trains Start...\n",
      "195 / 360  trains Start...\n",
      "196 / 360  trains Start...\n",
      "197 / 360  trains Start...\n",
      "198 / 360  trains Start...\n",
      "199 / 360  trains Start...\n",
      "200 / 360  trains Start...\n",
      "201 / 360  trains Start...\n",
      "202 / 360  trains Start...\n",
      "203 / 360  trains Start...\n",
      "204 / 360  trains Start...\n",
      "205 / 360  trains Start...\n",
      "206 / 360  trains Start...\n",
      "207 / 360  trains Start...\n",
      "208 / 360  trains Start...\n",
      "209 / 360  trains Start...\n",
      "210 / 360  trains Start...\n",
      "211 / 360  trains Start...\n",
      "212 / 360  trains Start...\n",
      "213 / 360  trains Start...\n",
      "214 / 360  trains Start...\n",
      "215 / 360  trains Start...\n",
      "216 / 360  trains Start...\n",
      "217 / 360  trains Start...\n",
      "218 / 360  trains Start...\n",
      "219 / 360  trains Start...\n",
      "220 / 360  trains Start...\n",
      "221 / 360  trains Start...\n",
      "222 / 360  trains Start...\n",
      "223 / 360  trains Start...\n",
      "224 / 360  trains Start...\n",
      "225 / 360  trains Start...\n",
      "226 / 360  trains Start...\n",
      "227 / 360  trains Start...\n",
      "228 / 360  trains Start...\n",
      "229 / 360  trains Start...\n",
      "230 / 360  trains Start...\n",
      "231 / 360  trains Start...\n",
      "232 / 360  trains Start...\n",
      "233 / 360  trains Start...\n",
      "234 / 360  trains Start...\n",
      "235 / 360  trains Start...\n",
      "236 / 360  trains Start...\n",
      "237 / 360  trains Start...\n",
      "238 / 360  trains Start...\n",
      "239 / 360  trains Start...\n",
      "240 / 360  trains Start...\n",
      "241 / 360  trains Start...\n",
      "242 / 360  trains Start...\n",
      "243 / 360  trains Start...\n",
      "244 / 360  trains Start...\n",
      "245 / 360  trains Start...\n",
      "246 / 360  trains Start...\n",
      "247 / 360  trains Start...\n",
      "248 / 360  trains Start...\n",
      "249 / 360  trains Start...\n",
      "250 / 360  trains Start...\n",
      "251 / 360  trains Start...\n",
      "252 / 360  trains Start...\n",
      "253 / 360  trains Start...\n",
      "254 / 360  trains Start...\n",
      "255 / 360  trains Start...\n",
      "256 / 360  trains Start...\n",
      "257 / 360  trains Start...\n",
      "258 / 360  trains Start...\n",
      "259 / 360  trains Start...\n",
      "260 / 360  trains Start...\n",
      "261 / 360  trains Start...\n",
      "262 / 360  trains Start...\n",
      "263 / 360  trains Start...\n",
      "264 / 360  trains Start...\n",
      "265 / 360  trains Start...\n",
      "266 / 360  trains Start...\n",
      "267 / 360  trains Start...\n",
      "268 / 360  trains Start...\n",
      "269 / 360  trains Start...\n",
      "270 / 360  trains Start...\n",
      "271 / 360  trains Start...\n",
      "272 / 360  trains Start...\n",
      "273 / 360  trains Start...\n",
      "274 / 360  trains Start...\n",
      "275 / 360  trains Start...\n",
      "276 / 360  trains Start...\n",
      "277 / 360  trains Start...\n",
      "278 / 360  trains Start...\n",
      "279 / 360  trains Start...\n",
      "280 / 360  trains Start...\n",
      "281 / 360  trains Start...\n",
      "282 / 360  trains Start...\n",
      "283 / 360  trains Start...\n",
      "284 / 360  trains Start...\n",
      "285 / 360  trains Start...\n",
      "286 / 360  trains Start...\n",
      "287 / 360  trains Start...\n",
      "288 / 360  trains Start...\n",
      "289 / 360  trains Start...\n",
      "290 / 360  trains Start...\n",
      "291 / 360  trains Start...\n",
      "292 / 360  trains Start...\n",
      "293 / 360  trains Start...\n",
      "294 / 360  trains Start...\n",
      "295 / 360  trains Start...\n",
      "296 / 360  trains Start...\n",
      "297 / 360  trains Start...\n",
      "298 / 360  trains Start...\n",
      "299 / 360  trains Start...\n",
      "300 / 360  trains Start...\n",
      "301 / 360  trains Start...\n",
      "302 / 360  trains Start...\n",
      "303 / 360  trains Start...\n",
      "304 / 360  trains Start...\n",
      "305 / 360  trains Start...\n",
      "306 / 360  trains Start...\n",
      "307 / 360  trains Start...\n",
      "308 / 360  trains Start...\n",
      "309 / 360  trains Start...\n",
      "310 / 360  trains Start...\n",
      "311 / 360  trains Start...\n",
      "312 / 360  trains Start...\n",
      "313 / 360  trains Start...\n",
      "314 / 360  trains Start...\n",
      "315 / 360  trains Start...\n",
      "316 / 360  trains Start...\n",
      "317 / 360  trains Start...\n",
      "318 / 360  trains Start...\n",
      "319 / 360  trains Start...\n",
      "320 / 360  trains Start...\n",
      "321 / 360  trains Start...\n",
      "322 / 360  trains Start...\n",
      "323 / 360  trains Start...\n",
      "324 / 360  trains Start...\n",
      "325 / 360  trains Start...\n",
      "326 / 360  trains Start...\n",
      "327 / 360  trains Start...\n",
      "328 / 360  trains Start...\n",
      "329 / 360  trains Start...\n",
      "330 / 360  trains Start...\n",
      "331 / 360  trains Start...\n",
      "332 / 360  trains Start...\n",
      "333 / 360  trains Start...\n",
      "334 / 360  trains Start...\n",
      "335 / 360  trains Start...\n",
      "336 / 360  trains Start...\n",
      "337 / 360  trains Start...\n",
      "338 / 360  trains Start...\n",
      "339 / 360  trains Start...\n",
      "340 / 360  trains Start...\n",
      "341 / 360  trains Start...\n",
      "342 / 360  trains Start...\n",
      "343 / 360  trains Start...\n",
      "344 / 360  trains Start...\n",
      "345 / 360  trains Start...\n",
      "346 / 360  trains Start...\n",
      "347 / 360  trains Start...\n",
      "348 / 360  trains Start...\n",
      "349 / 360  trains Start...\n",
      "350 / 360  trains Start...\n",
      "351 / 360  trains Start...\n",
      "352 / 360  trains Start...\n",
      "353 / 360  trains Start...\n",
      "354 / 360  trains Start...\n",
      "355 / 360  trains Start...\n",
      "356 / 360  trains Start...\n",
      "357 / 360  trains Start...\n",
      "358 / 360  trains Start...\n",
      "359 / 360  trains Start...\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    This cell is for Extracting Headword from train dataset\n",
    "''' \n",
    "print 'Start Extracting Headwords from train dataset'\n",
    "total = len(train)\n",
    "count = 0\n",
    "for article in train:\n",
    "    print count,'/',total,' trains Start...'\n",
    "    count += 1\n",
    "    qas = article['qa']\n",
    "    sentences = article['sentences']\n",
    "    token_sentences = copy.deepcopy(sentences)\n",
    "    for i in range(len(sentences)):\n",
    "        token_sentences[i] = nltk.word_tokenize(token_sentences[i])\n",
    "    ner_sentences = st.tag_sents(token_sentences)\n",
    "    parse_ner_sentences = parse_NER(ner_sentences)\n",
    "    #Get the NER of the answer in the corresponding sentence\n",
    "    for qa in qas:\n",
    "        rank = {}\n",
    "        ner_sentence = parse_ner_sentences[qa['answer_sentence']]\n",
    "        for word in nltk.word_tokenize(qa['answer']):\n",
    "            for w,t in ner_sentence:\n",
    "                if t != u'O' and w == word:\n",
    "                    rank[t] = rank.get(t,0) + 1\n",
    "        result = sorted(rank.items(), lambda x, y: cmp(x[1], y[1]), reverse=True)\n",
    "        if result != []:        \n",
    "            kind = result[0][0]\n",
    "            text = qa['question']\n",
    "            #According to the NER of answer, save headwords into different dictionary key: headword; value: doc frequency\n",
    "            if kind == u'NUMBER':\n",
    "                for word in getHeadWord(text):\n",
    "                    number_headword[word] = number_headword.get(word,0) + 1\n",
    "            elif kind == u'YEAR':\n",
    "                for word in getHeadWord(text):\n",
    "                    year_headword[word] = year_headword.get(word,0) + 1\n",
    "            elif kind == u'ORGANIZATION':\n",
    "                for word in getHeadWord(text):\n",
    "                    organization_headword[word] = organization_headword.get(word,0) + 1\n",
    "            elif kind == u'PERSON':\n",
    "                for word in getHeadWord(text):\n",
    "                    person_headword[word] = person_headword.get(word,0) + 1\n",
    "            elif kind == u'LOCATION':\n",
    "                for word in getHeadWord(text):\n",
    "                    location_headword[word] = location_headword.get(word,0) + 1\n",
    "\n",
    "#If two dictionary have the same headword, which one has the higher doc frequency will keep the result, while the other will delete this record.\n",
    "for (w,n) in year_headword.items():\n",
    "    if w in organization_headword:\n",
    "        if n >= organization_headword.get(w):\n",
    "            del organization_headword[w]\n",
    "        else:\n",
    "            del year_headword[w]\n",
    "for (w,n) in year_headword.items():\n",
    "    if w in person_headword:\n",
    "        if n >= person_headword.get(w):\n",
    "            del person_headword[w]\n",
    "        else:\n",
    "            del year_headword[w]\n",
    "for (w,n) in year_headword.items():\n",
    "    if w in number_headword:\n",
    "        if n >= number_headword.get(w):\n",
    "            del number_headword[w]\n",
    "        else:\n",
    "            del year_headword[w]\n",
    "for (w,n) in year_headword.items():\n",
    "    if w in location_headword:\n",
    "        if n >= location_headword.get(w):\n",
    "            del location_headword[w]\n",
    "        else:\n",
    "            del year_headword[w]\n",
    "            \n",
    "for (w,n) in organization_headword.items():\n",
    "    if w in person_headword:\n",
    "        if n >= person_headword.get(w):\n",
    "            del person_headword[w]\n",
    "        else:\n",
    "            del organization_headword[w]\n",
    "for (w,n) in organization_headword.items():\n",
    "    if w in number_headword:\n",
    "        if n >= number_headword.get(w):\n",
    "            del number_headword[w]\n",
    "        else:\n",
    "            del organization_headword[w]\n",
    "for (w,n) in organization_headword.items():\n",
    "    if w in location_headword:\n",
    "        if n >= location_headword.get(w):\n",
    "            del location_headword[w]\n",
    "        else:\n",
    "            del organization_headword[w]\n",
    "            \n",
    "for (w,n) in person_headword.items():\n",
    "    if w in number_headword:\n",
    "        if n >= number_headword.get(w):\n",
    "            del number_headword[w]\n",
    "        else:\n",
    "            del person_headword[w]\n",
    "for (w,n) in person_headword.items():\n",
    "    if w in location_headword:\n",
    "        if n >= location_headword.get(w):\n",
    "            del location_headword[w]\n",
    "        else:\n",
    "            del person_headword[w]\n",
    "\n",
    "for (w,n) in number_headword.items():\n",
    "    if w in location_headword:\n",
    "        if n >= location_headword.get(w):\n",
    "            del location_headword[w]\n",
    "        else:\n",
    "            del number_headword[w]\n",
    "            \n",
    "#Filtering the result to avoid overfitting\n",
    "for (w,n) in year_headword.items():\n",
    "    if n < threshold:\n",
    "        del year_headword[w]\n",
    "for (w,n) in organization_headword.items():\n",
    "    if n < threshold:\n",
    "        del organization_headword[w]\n",
    "for (w,n) in person_headword.items():\n",
    "    if n < threshold:\n",
    "        del person_headword[w]\n",
    "for (w,n) in number_headword.items():\n",
    "    if n < threshold:\n",
    "        del number_headword[w]\n",
    "for (w,n) in location_headword.items():\n",
    "    if n < threshold:\n",
    "        del location_headword[w]\n",
    "\n",
    "location_list = location_headword.keys()\n",
    "number_list = number_headword.keys()\n",
    "organization_list = organization_headword.keys()\n",
    "name_list = person_headword.keys()\n",
    "year_list = year_headword.keys()\n",
    "\n",
    "location_list = location_list + ['country','county','district','city']\n",
    "number_list = number_list + ['version','size','msa','far','much','ration','time','many','population','large','percent','average','day','decade','big','long']\n",
    "name_list = name_list + ['name','center','president','denominations','denomination','film','broadcaster','pitcher','commentator']\n",
    "year_list = year_list + ['years','year','era']\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "article  1  finished.\n",
      "article  2  finished.\n",
      "article  3  finished.\n",
      "article  4  finished.\n",
      "article  5  finished.\n",
      "article  6  finished.\n",
      "article  7  finished.\n",
      "article  8  finished.\n",
      "article  9  finished.\n",
      "article  10  finished.\n",
      "article  11  finished.\n",
      "article  12  finished.\n",
      "article  13  finished.\n",
      "article  14  finished.\n",
      "article  15  finished.\n",
      "article  16  finished.\n",
      "article  17  finished.\n",
      "article  18  finished.\n",
      "article  19  finished.\n",
      "article  20  finished.\n",
      "article  21  finished.\n",
      "article  22  finished.\n",
      "article  23  finished.\n",
      "article  24  finished.\n",
      "article  25  finished.\n",
      "article  26  finished.\n",
      "article  27  finished.\n",
      "article  28  finished.\n",
      "article  29  finished.\n",
      "article  30  finished.\n",
      "article  31  finished.\n",
      "article  32  finished.\n",
      "article  33  finished.\n",
      "article  34  finished.\n",
      "article  35  finished.\n",
      "article  36  finished.\n",
      "article  37  finished.\n",
      "article  38  finished.\n",
      "article  39  finished.\n",
      "article  40  finished.\n",
      "article  41  finished.\n",
      "article  42  finished.\n",
      "Output result...\n",
      "Results are stored in  result_enhance.csv\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    This cell is for \n",
    "        I.   Question Classification\n",
    "        II.  Named Entity Recognition processing\n",
    "        III. Answer Ranking\n",
    "        IV.  Output.\n",
    "''' \n",
    "ids = []\n",
    "answers = []\n",
    "count = 1\n",
    "for index in range(len(test)):\n",
    "    article = test[index]\n",
    "    qas = article['qa']\n",
    "    sentences = article['sentences']\n",
    "    token_sentences = copy.deepcopy(sentences)\n",
    "    #Tag the sentence by NER processing\n",
    "    for i in range(len(sentences)):\n",
    "        token_sentences[i] = parse_token(nltk.word_tokenize(token_sentences[i]))\n",
    "    ner_sentences = st.tag_sents(token_sentences)\n",
    "    parse_ner_sentences = parse_NER(ner_sentences)\n",
    "    #Predict the answer of each question\n",
    "    for i in range(len(qas)):\n",
    "        qa = qas[i]\n",
    "        anss = []\n",
    "        scs = []\n",
    "        id = qa['id']\n",
    "        #the amount of extract sentences\n",
    "        amount = 4\n",
    "        for ii in range(amount):\n",
    "            if result_sentences_weight[index][i][ii] != 0:\n",
    "                weight = float(result_sentences_weight[index][i][ii])/sum(result_sentences_weight[index][i])\n",
    "            else:\n",
    "                weight = 0\n",
    "            #Extract the sentence of answer\n",
    "            answer_sentence = sentences[result_sentences[index][i][ii]]\n",
    "            #Extract the id of sentence of answer\n",
    "            answer_sentence_id = result_sentences[index][i][ii]\n",
    "            #Extract question query\n",
    "            text_question = qa['question']\n",
    "            #Get the grammar(POS) result of question \n",
    "            result = get_continuous_chunks(text_question)\n",
    "\n",
    "            if result != []:\n",
    "                wtype,word = result[0]\n",
    "                if wtype == 'NUMBER':\n",
    "                    answer_list = extract_NER(parse_ner_sentences[answer_sentence_id],1)\n",
    "                    if answer_list == []:\n",
    "                        answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],3)\n",
    "                elif wtype == 'WHO':\n",
    "                    answer_list = extract_NER(parse_ner_sentences[answer_sentence_id],0)\n",
    "                    if answer_list == []:\n",
    "                        answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],2)\n",
    "                        answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],3)\n",
    "                elif wtype == 'WHERE':\n",
    "                    answer_list = extract_NER(parse_ner_sentences[answer_sentence_id],2)\n",
    "                    if answer_list == []:\n",
    "                        answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],0)\n",
    "                        answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],3)\n",
    "                elif wtype == 'WHEN':\n",
    "                    answer_list = extract_NER(parse_ner_sentences[answer_sentence_id],4)\n",
    "                else:\n",
    "                    #what, or other types\n",
    "                    answer_list = extract_NER(parse_ner_sentences[answer_sentence_id],3)\n",
    "                    if answer_list == []:\n",
    "                        answer_sentence = sentences[answer_sentence_id]\n",
    "                        wpos,wresult = get_continuous_chunks_sentence(answer_sentence,0)\n",
    "                        answer_list = screen_out_answer_WHAT(wresult)\n",
    "                        answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],2)\n",
    "                        answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],0)\n",
    "                        answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],1)\n",
    "                        answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],4)\n",
    "\n",
    "            if answer_list != []:           \n",
    "                #For question that have answer, ranking the answer by rule 1&3\n",
    "                query = copy.deepcopy(text_question)\n",
    "                query = nltk.word_tokenize(query)\n",
    "                #token the answer sentence and copy it for further usage\n",
    "                answer_sentence = nltk.word_tokenize(answer_sentence)\n",
    "                sentence = copy.deepcopy(answer_sentence)\n",
    "                for query_index in range(len(query)):\n",
    "                    query[query_index] = lemmatize(query[query_index].lower())\n",
    "                for sent_index in range(len(sentence)):\n",
    "                    sentence[sent_index] = lemmatize(sentence[sent_index].lower())\n",
    "                scores_1 = []\n",
    "                scores_3 = []\n",
    "                scores = []           \n",
    "                for entity in answer_list:\n",
    "                    entity = nltk.word_tokenize(entity) \n",
    "                    score1 = rank_rule_1(entity,query)\n",
    "                    scores_1.append(score1)\n",
    "                    #answer_sentence is the original version and sentence is preprocessed\n",
    "                    score3 = rank_rule_3(answer_sentence,sentence,entity,query)\n",
    "                    scores_3.append(score3)\n",
    "                    w1 = 0.2\n",
    "                    w3 = 1 - w1\n",
    "                    if score1 == 0:\n",
    "                        score3 = 0\n",
    "                    total = w1 * score1 + w3 * score3\n",
    "                    scores.append(total)\n",
    "                answer = answer_list[scores.index(max(scores))]\n",
    "                sc = max(scores)*weight\n",
    "\n",
    "            else:\n",
    "                #For question that doesn't have answer, return the total sentence\n",
    "                answer = answer_sentence\n",
    "                sc = 0\n",
    "            anss.append(answer)\n",
    "            scs.append(sc)\n",
    "        ans = anss[scs.index(max(scs))]       \n",
    "        ids.append(id)\n",
    "        answers.append(ans)\n",
    "    print 'article ',count,' finished.'\n",
    "    count += 1\n",
    "'''\n",
    "    IV. Output\n",
    "'''\n",
    "print 'Output result...'\n",
    "filename = \"result_enhance.csv\"\n",
    "for i in range(len(answers)):\n",
    "    answers[i] = answers[i].replace(',','-COMMA-')\n",
    "    answers[i] = answers[i].replace('\"','')\n",
    "output_result(filename)\n",
    "print 'Results are stored in ', filename"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
