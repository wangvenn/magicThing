{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "with open('QA_train.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "#for i in range(0,len(data)):\n",
    "sample = []\n",
    "punctuation = [',','.','(',')',':','``','\\'\\'',';','&']\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def seperate_sentence(sentence):\n",
    "    #print '----------------------------'\n",
    "    #print sentence\n",
    "    combined_word = ''\n",
    "    result = re.split('[,() \\'\"]+',sentence)\n",
    "    segment = []\n",
    "    for index,word in enumerate(result):\n",
    "        if word != '':\n",
    "            if index == len(result)-1:\n",
    "                segment.append(word[:-1])\n",
    "            else:\n",
    "                segment.append(word)\n",
    "    #print segment\n",
    "    return segment\n",
    "    \n",
    "        \n",
    "\n",
    "for i in range(0,1):\n",
    "    qa_list = data[i].get('qa')\n",
    "    sentences_list = data[i].get('sentences')\n",
    "    for sentence in sentences_list:\n",
    "        seperate_sentence(sentence)\n",
    "    '''for j in range(0,len(qa_list)):\n",
    "        qa = qa_list[j]\n",
    "        sentence_id = qa.get('answer_sentence')\n",
    "        #print \"#################################\"\n",
    "        #print \"Q:\",qa.get('question')\n",
    "        #print \"A:\",qa.get('answer')\n",
    "        #print \"S:\",sentences_list[sentence_id]\n",
    "        sentence = sentences_list[sentence_id]\n",
    "        print seperate_sentence(sentence)'''\n",
    "        \n",
    "        #sample.append((qa.get('question'),qa.get('answer'),sentences_list[sentence_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "import nltk\n",
    "import copy\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tag import StanfordNERTagger\n",
    "\n",
    "def create_process_tools():\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    v = DictVectorizer(sparse=False)\n",
    "    transformer = TfidfTransformer(smooth_idf=False,norm=None)\n",
    "    svd = TruncatedSVD(n_components=260)\n",
    "    return stopwords,lemmatizer,v,transformer,svd\n",
    "\n",
    "def input_data():\n",
    "\n",
    "    base_path = os.path.join('')\n",
    "    train_file = base_path + 'QA_train.json'\n",
    "    train_data = json.load(open(train_file))\n",
    "    test_file = base_path + 'QA_test.json'\n",
    "    test_data = json.load(open(test_file))\n",
    "    dev_file = base_path + 'QA_dev.json'\n",
    "    dev_data = json.load(open(dev_file))\n",
    "\n",
    "    return train_data,test_data,dev_data\n",
    "\n",
    "def output_result(filename):\n",
    "\n",
    "    predictions_file = open(filename, \"wb\")\n",
    "    open_file_object = csv.writer(predictions_file)\n",
    "    open_file_object.writerow([\"id\",\"answer\"])\n",
    "    # open_file_object.writerows(zip(ID, output))\n",
    "    predictions_file.close()\n",
    "\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def transform_doc(doc):\n",
    "    return svd.fit_transform(transformer.fit_transform(v.fit_transform(doc)))\n",
    "\n",
    "def transform_query(query_text):\n",
    "    query_text = nltk.word_tokenize(query_text)\n",
    "    query_text = [lemmatize(word.lower()) for word in query_text]\n",
    "    for word in query_text:\n",
    "        if word in stopwords or word in punctuation:\n",
    "            query_text.remove(word)\n",
    "    return svd.transform(transformer.transform(v.transform([get_BOW(query_text)])))[0]\n",
    "\n",
    "def process_sentence(sentence):\n",
    "    #token, lower, remove stopwords, get bag of words\n",
    "    sentence = nltk.word_tokenize(sentence)\n",
    "    sentence = [lemmatize(word.lower()) for word in sentence]\n",
    "    for word in sentence:\n",
    "        if word in stopwords or word in punctuation:\n",
    "            sentence.remove(word)\n",
    "    sentence = get_BOW(sentence)\n",
    "    return sentence\n",
    "\n",
    "def divide_data(data):\n",
    "    #achieve sentences, questions, answers and the corresponding indexs.\n",
    "    queries = []\n",
    "    indexs = []\n",
    "    answers = []\n",
    "    qas = data['qa']\n",
    "    sentences = data['sentences']\n",
    "    for qa in qas:\n",
    "        query = qa['question']\n",
    "        queries.append(query)\n",
    "        index = qa['answer_sentence']\n",
    "        indexs.append(index)\n",
    "        answer = qa['answer']\n",
    "        answers.append(answer)\n",
    "    for i in range(len(sentences)):\n",
    "        sentences[i] = process_sentence(sentences[i])\n",
    "    sentences = transform_doc(sentences)\n",
    "    return sentences,indexs,queries,answers\n",
    "\n",
    "def get_best_doc_num(query,sentences):\n",
    "    #get the most likely documents according to the query\n",
    "    min = 1\n",
    "    index = 0\n",
    "    for i in range(sentences.shape[0]):\n",
    "        dist = cos_distance(query,sentences[i])\n",
    "        if dist < min:\n",
    "            min = dist\n",
    "            index = i\n",
    "    return index\n",
    "\n",
    "def evaluate_article(predicts,indexs):\n",
    "    #output the accuracy of predicted documents\n",
    "    count = 0\n",
    "    for i in range(len(predicts)):\n",
    "        if predicts[i] != indexs[i]:\n",
    "            count += 1\n",
    "    result = 1 - float(count)/len(predicts)\n",
    "    return result\n",
    "\n",
    "def show_wrong_result(original_article,predicts,indexs):\n",
    "    sentences_list = original_article.get('sentences')\n",
    "    for i in range(len(predicts)):\n",
    "        if predicts[i] != indexs[i]:\n",
    "            print \"############################\"\n",
    "            print \"Question: \",original_article.get('qa')[i].get('question')\n",
    "            print \"Guess: \",sentences_list[predicts[i]]\n",
    "            print \"Right: \",sentences_list[indexs[i]]\n",
    "\n",
    "def sentence_retrieval(article):\n",
    "    #achieve the original sentences which are retrieved\n",
    "    original_article = copy.deepcopy(article)\n",
    "    sentences,indexs,queries,answers = divide_data(article)\n",
    "    original_query = copy.deepcopy(queries)\n",
    "\n",
    "    predicts = []\n",
    "    for i in range(len(queries)):\n",
    "        queries[i] = transform_query(queries[i])\n",
    "        predict = get_best_doc_num(queries[i],sentences)\n",
    "        predicts.append(predict)\n",
    "    result = evaluate_article(predicts,indexs)\n",
    "    #print '(TF-IDF) retrieval accuracy is: ',\n",
    "    #show_wrong_result(original_article,predicts,indexs)\n",
    "\n",
    "    sentences_retrieval = []\n",
    "    for num in predicts:\n",
    "        sent = original_article['sentences'][num]\n",
    "        sentences_retrieval.append(sent)\n",
    "    #return sentences_retrieval,original_query\n",
    "    return result\n",
    "\n",
    "def input_NER():\n",
    "    stanford_dir = os.path.join('stanford-ner-2016-10-31')\n",
    "    jarfile = os.path.join(stanford_dir,'stanford-ner.jar')\n",
    "    modelfile = os.path.join(stanford_dir,'classifiers\\english.muc.7class.distsim.crf.ser.gz')\n",
    "    return modelfile,jarfile\n",
    "\n",
    "'''input data'''\n",
    "train,test,dev = input_data()\n",
    "'''input data'''\n",
    "\n",
    "punctuation = [',','.','(',')',':','``','\\'\\'',';','&']\n",
    "\n",
    "'''get processing tools'''\n",
    "stopwords,lemmatizer,v,transformer,svd = create_process_tools()\n",
    "'''get processing tools'''\n",
    "\n",
    "'''retrieve sentences'''\n",
    "model,jar = input_NER()\n",
    "st = StanfordNERTagger(model,jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "n_components must be < n_features; got 260 >= 229",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-ead7f022687a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marticals\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mresult\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msentence_retrieval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[1;34m\"Average accurancy is \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-501ef537fd24>\u001b[0m in \u001b[0;36msentence_retrieval\u001b[1;34m(article)\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;31m#achieve the original sentences which are retrieved\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m     \u001b[0moriginal_article\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m     \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindexs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mqueries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manswers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdivide_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m     \u001b[0moriginal_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    126\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-501ef537fd24>\u001b[0m in \u001b[0;36mdivide_data\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     87\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m         \u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransform_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindexs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mqueries\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0manswers\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-501ef537fd24>\u001b[0m in \u001b[0;36mtransform_doc\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtransform_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0msvd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtransform_query\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Canopy\\User\\lib\\site-packages\\sklearn\\decomposition\\truncated_svd.pyc\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mn_features\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 raise ValueError(\"n_components must be < n_features;\"\n\u001b[1;32m--> 170\u001b[1;33m                                  \" got %d >= %d\" % (k, n_features))\n\u001b[0m\u001b[0;32m    171\u001b[0m             U, Sigma, VT = randomized_svd(X, self.n_components,\n\u001b[0;32m    172\u001b[0m                                           \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: n_components must be < n_features; got 260 >= 229"
     ]
    }
   ],
   "source": [
    "result = 0.0\n",
    "for articals in train:\n",
    "    result += sentence_retrieval(articals)\n",
    "print \"Average accurancy is \", result/len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def BM25(articals,k1,k2,b): \n",
    "    sentences,queries,sentence_id = parse_data(articals)      \n",
    "    IDF_dict,avgdl,sentences_token = create_IDF_dict(sentences)\n",
    "    total_queries = len(queries)\n",
    "    count = 0\n",
    "    \n",
    "    for k11 in k1:\n",
    "        for k22 in k2:\n",
    "            for bb in b:\n",
    "                for index in range(total_queries):\n",
    "                    query = queries[index]\n",
    "                    answer_id = sentence_id[index]\n",
    "                    guess_id = find_max_score_sentence(query,sentences_token,IDF_dict,k1,k2,b,avgdl)\n",
    "                    if answer_id == guess_id:\n",
    "                        count += 1   \n",
    "                accurancy = float(count)/total_queries\n",
    "                \n",
    "    #print \"Accurancy of BM25 with k1:\",k1,\" k2:\",k2,\" b:\",b\n",
    "    #print \"  is \", accurancy\n",
    "    return accurancy\n",
    "\n",
    "def find_max_score_sentence(query,sentences_token,IDF_dict,k1,k2,b,avgdl):\n",
    "    query_text = nltk.word_tokenize(query)\n",
    "    query_text = [lemmatize(word.lower()) for word in query_text]\n",
    "    for word in query_text:\n",
    "        if word in stopwords or word in punctuation:\n",
    "            query_text.remove(word)\n",
    "    query_dict = get_BOW(query_text)\n",
    "    max_score = 0\n",
    "    guess_sentence = 0\n",
    "    for index in range(len(sentences_token)):\n",
    "        \n",
    "        score = 0\n",
    "        sentence = sentences_token[index]         \n",
    "        sentence_dict = get_BOW(sentence)\n",
    "        for word in query_dict:\n",
    "            N = len(sentences_token)\n",
    "            n_qi = IDF_dict.get(word,0)\n",
    "            fi = sentence_dict.get(word,0)\n",
    "            qfi = query_dict.get(word,0)\n",
    "            dl = len(sentence)\n",
    "            K = k1*(1-b+b*(float(dl)/avgdl))\n",
    "            \n",
    "            W = math.log((N-n_qi+0.5)/(n_qi+0.5))\n",
    "            R = (fi*(k1+1))/(fi+K)*qfi*(k2+1)/(qfi+k2)\n",
    "            score += W*R\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            guess_sentence = index\n",
    "        #print index,score\n",
    "    return guess_sentence\n",
    "      \n",
    "def parse_data(articals):\n",
    "    sentences = articals.get(\"sentences\")\n",
    "    qa = articals.get(\"qa\")\n",
    "    queries = []\n",
    "    sentence_id = []\n",
    "    for index in range(len(qa)):\n",
    "        queries.append(qa[index].get(\"question\"))\n",
    "        sentence_id.append(qa[index].get(\"answer_sentence\"))\n",
    "    return sentences,queries,sentence_id\n",
    "\n",
    "def create_IDF_dict(sentences):\n",
    "    IDF_dict = {}\n",
    "    total_length = 0\n",
    "    \n",
    "    sentences_token = []\n",
    "    for sentence in sentences:\n",
    "        sentence = nltk.word_tokenize(sentence)\n",
    "        sentence = [lemmatize(word.lower()) for word in sentence]\n",
    "        for word in sentence:\n",
    "            if word in stopwords or word in punctuation:\n",
    "                sentence.remove(word)\n",
    "        total_length += len(sentence)\n",
    "        for word in set(sentence):\n",
    "            IDF_dict[word] = IDF_dict.get(word,0) + 1\n",
    "        sentences_token.append(sentence)\n",
    "    return IDF_dict, float(total_length)/len(sentences),sentences_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "360\n",
      "(k1: 1.2  k2: 100  b: 0.1 ) average_accurancy:  0.66457691951\n",
      "(k1: 1.2  k2: 100  b: 0.125 ) average_accurancy:  0.664785305326\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-c7a3e652d611>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0maccurancy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0maccurancy\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mBM25\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0maverage_accurancy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maccurancy\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtest_length\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"(k1:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" k2:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" b:\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\") average_accurancy: \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maverage_accurancy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-8cd37ac256ea>\u001b[0m in \u001b[0;36mBM25\u001b[1;34m(articals, k1, k2, b)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mquery\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mqueries\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0manswer_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentence_id\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[0mguess_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_max_score_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msentences_token\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mIDF_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mk2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mavgdl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0manswer_id\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mguess_id\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-22-8cd37ac256ea>\u001b[0m in \u001b[0;36mfind_max_score_sentence\u001b[1;34m(query, sentences_token, IDF_dict, k1, k2, b, avgdl)\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentences_token\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0msentence_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_BOW\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquery_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m             \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentences_token\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-33-501ef537fd24>\u001b[0m in \u001b[0;36mget_BOW\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mBOW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mBOW\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBOW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mBOW\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with open('QA_train.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "print len(data)\n",
    "test_length = len(data)\n",
    "accurancy = 0.0\n",
    "\n",
    "k1 = 1.2\n",
    "k2 = 100\n",
    "for b in [0.1,0.125,0.15,0.175]:\n",
    "    accurancy = 0.0\n",
    "    for i in range(0,test_length):\n",
    "        accurancy += BM25(data[i],k1,k2,b)\n",
    "    average_accurancy = accurancy/test_length\n",
    "    \n",
    "    \n",
    "    \n",
    "#(k1: 1.2  k2: 100  b: 0.1 )   average_accurancy:  0.66457691951\n",
    "#(k1: 1.2  k2: 100  b: 0.125 ) average_accurancy:  0.664785305326\n",
    "#(k1: 1.2  k2: 100  b: 0.2 )   average_accurancy:  0.664918 1.2,100,0.2\n",
    "#(k1: 1.2  k2: 100  b: 0.3 )   average_accurancy:  0.664284 1.2,100,0.3\n",
    "#(k1: 1.2  k2: 100  b: 0.4 )   average_accurancy:  0.658218 1.2,100,0.4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Phonautograms', u'O'), (u'of', u'O'), (u'singing', u'O'), (u'and', u'O'), (u'speech', u'O'), (u'made', u'O'), (u'by', u'O'), (u'Scott', u'PERSON'), (u'in', u'O'), (u'1860', u'DATE'), (u'were', u'O'), (u'played', u'O'), (u'back', u'O'), (u'as', u'O'), (u'sound', u'O'), (u'for', u'O'), (u'the', u'O'), (u'first', u'O'), (u'time', u'O'), (u'in', u'O'), (u'2008', u'DATE')]\n",
      "[(u'Along', u'O'), (u'with', u'O'), (u'a', u'O'), (u'tuning', u'O'), (u'fork', u'O'), (u'tone', u'O'), (u'and', u'O'), (u'unintelligible', u'O'), (u'snippets', u'O'), (u'recorded', u'O'), (u'as', u'O'), (u'early', u'O'), (u'as', u'O'), (u'1857', u'DATE'), (u'these', u'O'), (u'are', u'O'), (u'the', u'O'), (u'earliest', u'O'), (u'known', u'O'), (u'recordings', u'O'), (u'of', u'O'), (u'sound', u'O')]\n",
      "[(u'In', u'O'), (u'1877', u'DATE'), (u'Thomas', u'ORGANIZATION'), (u'Edison', u'ORGANIZATION'), (u'invented', u'O'), (u'the', u'O'), (u'phonograph', u'O')]\n",
      "[(u'Unlike', u'O'), (u'the', u'O'), (u'phonautograph', u'O'), (u'it', u'O'), (u'was', u'O'), (u'capable', u'O'), (u'of', u'O'), (u'both', u'O'), (u'recording', u'O'), (u'and', u'O'), (u'reproducing', u'O'), (u'sound', u'O')]\n",
      "[(u'Despite', u'O'), (u'the', u'O'), (u'similarity', u'O'), (u'of', u'O'), (u'name', u'O'), (u'there', u'O'), (u'is', u'O'), (u'no', u'O'), (u'documentary', u'O'), (u'evidence', u'O'), (u'that', u'O'), (u'Edison', u'ORGANIZATION'), (u's', u'O'), (u'phonograph', u'O'), (u'was', u'O'), (u'based', u'O'), (u'on', u'O'), (u'Scott', u'O'), (u's', u'O'), (u'phonautograph', u'O')]\n",
      "[(u'Edison', u'O'), (u'first', u'O'), (u'tried', u'O'), (u'recording', u'O'), (u'sound', u'O'), (u'on', u'O'), (u'a', u'O'), (u'wax-impregnated', u'O'), (u'paper', u'O'), (u'tape', u'O'), (u'with', u'O'), (u'the', u'O'), (u'idea', u'O'), (u'of', u'O'), (u'creating', u'O'), (u'a', u'O'), (u'telephone', u'O'), (u'repeater', u'O'), (u'analogous', u'O'), (u'to', u'O'), (u'the', u'O'), (u'telegraph', u'O'), (u'repeater', u'O'), (u'he', u'O'), (u'had', u'O'), (u'been', u'O'), (u'working', u'O'), (u'on', u'O')]\n",
      "[(u'Although', u'O'), (u'the', u'O'), (u'visible', u'O'), (u'results', u'O'), (u'made', u'O'), (u'him', u'O'), (u'confident', u'O'), (u'that', u'O'), (u'sound', u'O'), (u'could', u'O'), (u'be', u'O'), (u'physically', u'O'), (u'recorded', u'O'), (u'and', u'O'), (u'reproduced', u'O'), (u'his', u'O'), (u'notes', u'O'), (u'do', u'O'), (u'not', u'O'), (u'indicate', u'O'), (u'that', u'O'), (u'he', u'O'), (u'actually', u'O'), (u'reproduced', u'O'), (u'sound', u'O'), (u'before', u'O'), (u'his', u'O'), (u'first', u'O'), (u'experiment', u'O'), (u'in', u'O'), (u'which', u'O'), (u'he', u'O'), (u'used', u'O'), (u'tinfoil', u'O'), (u'as', u'O'), (u'a', u'O'), (u'recording', u'O'), (u'medium', u'O'), (u'several', u'DATE'), (u'months', u'DATE'), (u'later', u'DATE')]\n",
      "[(u'The', u'O'), (u'tinfoil', u'O'), (u'was', u'O'), (u'wrapped', u'O'), (u'around', u'O'), (u'a', u'O'), (u'grooved', u'O'), (u'metal', u'O'), (u'cylinder', u'O'), (u'and', u'O'), (u'a', u'O'), (u'sound-vibrated', u'O'), (u'stylus', u'O'), (u'indented', u'O'), (u'the', u'O'), (u'tinfoil', u'O'), (u'while', u'O'), (u'the', u'O'), (u'cylinder', u'O'), (u'was', u'O'), (u'rotated', u'O')]\n",
      "[(u'The', u'O'), (u'recording', u'O'), (u'could', u'O'), (u'be', u'O'), (u'played', u'O'), (u'back', u'O'), (u'immediately', u'O')]\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,1):\n",
    "    qa_list = data[i].get('qa')\n",
    "    sentences_list = data[i].get('sentences')\n",
    "    for sentence in sentences_list[11:20]:\n",
    "        print st.tag(seperate_sentence(sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
