{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get document frequency for terms in the same article/instance; and get each sentence's BOW; and get each question's BOW;\n",
    "#get each sentence's length; get each article's average length of sentences\n",
    "import csv\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import nltk\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tag import StanfordNERTagger\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "def input_data():\n",
    "    base_path = os.path.join('data/')\n",
    "    train_file = base_path + 'QA_train.json'\n",
    "    train_data = json.load(open(train_file))\n",
    "    test_file = base_path + 'QA_test.json'\n",
    "    test_data = json.load(open(test_file))\n",
    "    dev_file = base_path + 'QA_dev.json'\n",
    "    dev_data = json.load(open(dev_file))\n",
    "\n",
    "    return train_data,test_data,dev_data\n",
    "def transform_text(text):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = [lemmatize(word.lower()) for word in text]\n",
    "    result = []\n",
    "    for word in text:\n",
    "        if word not in stopwords and word not in punctuations:\n",
    "            result.append(word)\n",
    "    return result\n",
    "def get_Docfrequency_SentenceBOW(dataset):\n",
    "    #save dics, each dictionary contains document frequencies for all terms in the same article\n",
    "    question_list = []\n",
    "    #save lists, each list represent an article, saving sentences' bow\n",
    "    total_sentence_bow = []\n",
    "    #save lists, each list represent an article, saving questions' bow\n",
    "    total_question_bow = []\n",
    "    #save lists, each list represent all sentences' lengthes.\n",
    "    sent_lengthes = []\n",
    "    #save a list, each item represents the average length of sentences\n",
    "    avg_lengthes = []\n",
    "    #\n",
    "    answer_id = []\n",
    "    \n",
    "    for article in dataset:\n",
    "        #Docfrequency\n",
    "        article_dic = defaultdict(list)\n",
    "        keyterms = [] #save all distinct terms in questions\n",
    "        \n",
    "        #SentenceBOW\n",
    "        bow_list = []\n",
    "        \n",
    "        #QuestionBOW\n",
    "        que_list = []\n",
    "        \n",
    "        #SentenceLength\n",
    "        sent_len = []\n",
    "        \n",
    "        #TotalLength\n",
    "        total_len = 0\n",
    "        \n",
    "        #RightAnser\n",
    "        right_answer = []\n",
    "        \n",
    "        qas = article['qa']\n",
    "        sentences = article['sentences']\n",
    "        for qa in qas:\n",
    "            question = qa['question']\n",
    "            newquestion = transform_text(question)\n",
    "            #QuestionBOW\n",
    "            que_list.append(get_BOW(newquestion))\n",
    "            answer = qa['answer_sentence']\n",
    "            right_answer.append(answer)\n",
    "            \n",
    "            keyterms.extend(newquestion)\n",
    "        keyterms = set(keyterms)\n",
    "        \n",
    "        #save sentences' BOW in list sen_BOW\n",
    "        sen_words = []\n",
    "        for sent in sentences:\n",
    "            sent = transform_text(sent)\n",
    "            #Docfrequency\n",
    "            sen_words.append(sent)\n",
    "            \n",
    "            #SentenceBOW\n",
    "            bow_list.append(get_BOW(sent))\n",
    "            \n",
    "            #SentenceLength\n",
    "            sent_len.append(len(sent))\n",
    "            \n",
    "            #TotalLength\n",
    "            total_len += len(sent)\n",
    "            \n",
    "        \n",
    "        #calculate doc frequency    \n",
    "        for term in keyterms:\n",
    "            for i,bow in enumerate(sen_words):\n",
    "                if term in bow:\n",
    "                    article_dic[term].append(i)\n",
    "                    \n",
    "        #Docfrequency\n",
    "        question_list.append(article_dic)\n",
    "        #SentenceBOW\n",
    "        total_sentence_bow.append(bow_list)\n",
    "        #QuestionBOW\n",
    "        total_question_bow.append(que_list)\n",
    "        #SentenceLength\n",
    "        sent_lengthes.append(sent_len)\n",
    "        #AverageLength\n",
    "        avg_lengthes.append(float(total_len)/len(sentences))\n",
    "        #\n",
    "        answer_id.append(right_answer)\n",
    "        \n",
    "    return question_list, total_sentence_bow, total_question_bow, sent_lengthes, avg_lengthes, answer_id\n",
    "\n",
    "train,test,dev = input_data()\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "punctuations = [',','\\'\\'','?','\\'','.','%','(',')',';','``']\n",
    "question_list, total_sentence_bow, total_question_bow, sent_lengthes, avg_lengthes, answer_id_list= get_Docfrequency_SentenceBOW(train)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 9,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def BM25(articles_index,k1,k2,b):\n",
    "    total_queries = len(total_question_bow[articles_index])\n",
    "    count = 0\n",
    "    correct_id = []\n",
    "    for index in range(len(total_question_bow[articles_index])):\n",
    "        answer_id = answer_id_list[articles_index][index]\n",
    "        guess_id = find_max_score_sentence(articles_index,index,k1,k2,b)\n",
    "        if answer_id == guess_id:\n",
<<<<<<< HEAD
    "            correct_id.append(index)\n",
    "            count += 1   \n",
    "    accurancy = float(count)/total_queries  \n",
    "    return correct_id,accurancy\n",
=======
    "            count += 1\n",
    "            correct_id.append(index)\n",
    "    accuracy = float(count)/total_queries  \n",
    "    return correct_id, accuracy\n",
>>>>>>> origin/master
    "\n",
    "def find_max_score_sentence(articles_index,index,k1,k2,b):\n",
    "\n",
    "    query_dict = total_question_bow[articles_index][index]\n",
    "    max_score = 0\n",
    "    guess_sentence = 0\n",
    "    for index in range(len(total_sentence_bow[articles_index])):     \n",
    "        score = 0  \n",
    "        sentence_dict = total_sentence_bow[articles_index][index]\n",
    "        for word in query_dict:\n",
    "            document_fre_list = question_list[articles_index].get(word,None)\n",
    "            \n",
    "            \n",
    "            N = len(total_sentence_bow[articles_index])\n",
    "            n_qi = 0\n",
    "            if document_fre_list != None:\n",
    "                n_qi = len(document_fre_list)\n",
    "            else:\n",
    "                n_qi = 0\n",
    "            fi = sentence_dict.get(word,0)\n",
    "            qfi = query_dict.get(word,0)\n",
    "            dl = sent_lengthes[articles_index][index]\n",
    "            avgdl = avg_lengthes[articles_index]\n",
    "            \n",
    "            K = k1*(1-b+b*(float(dl)/avgdl)) \n",
    "            W = math.log((N-n_qi+0.5)/(n_qi+0.5))\n",
    "            R = (fi*(k1+1))/(fi+K)*qfi*(k2+1)/(qfi+k2)\n",
    "            score += W*R\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            guess_sentence = index\n",
    "    return guess_sentence"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
=======
   "execution_count": 10,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of BM25 with k1: 1.1  k2: 110  b: 0.18\n",
      "0.683486756477\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0.0\n",
    "\n",
    "k1_list = [1.1]\n",
    "k2_list = [110]\n",
    "b_list = [0.18]\n",
    "\n",
    "correct = []\n",
    "\n",
    "test_length = len(train)\n",
    "\n",
    "for k1 in k1_list:\n",
    "    for k2 in k2_list:\n",
    "        for b in b_list:\n",
    "            accuracy = 0.0\n",
    "            for i in range(0,test_length):\n",
    "                correct_id,i_accuracy = BM25(i,k1,k2,b)\n",
    "                accuracy += i_accuracy\n",
    "                correct.append(correct_id)\n",
    "            average_accuracy = accuracy/test_length\n",
    "            print \"Accuracy of BM25 with k1:\",k1,\" k2:\",k2,\" b:\",b\n",
    "            print average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Accurancy of BM25 with k1: 1.1  k2: 110  b: 0.18\n",
    "#0.683486756477"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n"
     ]
    }
   ],
=======
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
>>>>>>> origin/master
   "source": [
    "import re\n",
    "pattern = re.compile(r'[0-9]+s') \n",
    "match = pattern.search('mid-1950s') \n",
    " \n",
    "if match: \n",
    "    print 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import re\n",
    "import sys\n",
    "def token(text):\n",
    "    text = text[:-1]\n",
    "    result = []\n",
    "    combo = u''\n",
    "    flag_upper = False\n",
    "    x = re.split(r'(\"|\\?|;|,|\\(|\\s|\\))\\s*', text)\n",
    "    #print x\n",
    "    for word in x:\n",
    "        if word != u'' and word != u' ':\n",
    "            if word[0].isalpha():\n",
    "                if flag_upper == False and word[0].isupper():\n",
    "                    combo = word\n",
    "                    flag_upper = True\n",
    "                elif flag_upper == False and word[0].islower():\n",
    "                    result.append(word)\n",
    "                elif flag_upper == True and word[0].isupper():\n",
    "                    combo = combo + u' ' + word\n",
    "                elif flag_upper == True and word[0].islower():\n",
    "                    result.append(combo)\n",
    "                    combo = u''\n",
    "                    result.append(word)\n",
    "                    flag_upper = False\n",
    "            else:\n",
    "                if flag_upper == True:\n",
    "                    result.append(combo)\n",
    "                    combo = u''\n",
    "                    result.append(word)\n",
    "                    flag_upper = False\n",
    "                else:\n",
    "                    result.append(word)\n",
    "    if combo != u'':\n",
    "        result.append(combo) \n",
    "    return result\n",
    "def transfer_pos_question(pos):\n",
    "    new_pos = []\n",
    "    for (word,wtype) in pos:\n",
    "        if word.lower() == 'what':\n",
    "            new_pos.append((word,'WHAT'))\n",
    "        elif word.lower() == 'do' or word.lower() == 'does' or word.lower() == 'did':\n",
    "            new_pos.append((word,'DO'))\n",
    "        elif word.lower() == 'far' or word.lower() == 'much' or word.lower() == 'year' or word.lower() == 'era' or word.lower() == 'ration' or word.lower() == 'years' or word.lower() == 'time' or word.lower() == 'many':\n",
    "            new_pos.append((word,'TIME'))\n",
    "        elif word.lower() == 'is' or word.lower() == 'was' or word.lower() == 'are' or word.lower() == 'were':\n",
    "            new_pos.append((word,'BE'))\n",
    "        elif word.lower() == 'when':\n",
    "            new_pos.append((word,'WHEN'))\n",
    "        elif word.lower() == 'where':\n",
    "            new_pos.append((word,'WHERE'))\n",
    "        elif word.lower() == 'how':\n",
    "            new_pos.append((word,'HOW'))\n",
    "        elif word.lower() == 'who' or word.lower() == 'whom' or word.lower() == 'whose':\n",
    "            new_pos.append((word,'WHO'))\n",
    "        else:\n",
    "            new_pos.append((word,wtype))\n",
    "    return new_pos\n",
    "def transfer_pos_sentence(pos):\n",
    "    new_pos = []\n",
    "    \n",
    "    for (word,wtype) in pos:\n",
    "        time_pattern = re.compile(r'[0-9]{4}s?')\n",
    "        if word.lower() == 'and' or word.lower() == 'or':\n",
    "            new_pos.append((word,'POSICC'))\n",
    "        elif word.lower() == 'with':\n",
    "            new_pos.append((word,'WITH'))\n",
    "        elif word.lower() == 'a' or word.lower() == 'an':\n",
    "            new_pos.append((word,'A'))\n",
    "        elif word == '\"':\n",
    "            new_pos.append((word,'\"'))\n",
    "        elif word == 'minutes' or word == 'March':\n",
    "            new_pos.append((word,'TIME'))\n",
    "        elif time_pattern.search(word):\n",
    "            new_pos.append((word,'TIME'))\n",
    "        elif word == 'around':\n",
    "            new_pos.append((word,'AROUND'))\n",
    "        elif word == 'mm':\n",
    "            new_pos.append((word,'CD')) \n",
    "        else:\n",
    "            new_pos.append((word,wtype))\n",
    "    return new_pos\n",
    "def get_continuous_chunks(text,texttype):\n",
    "    #text = text.encode('ascii','replace')\n",
    "    t = copy.deepcopy(text)\n",
    "    #print token(t)\n",
    "    pos =  pos_tag(token(t))\n",
    "    if texttype==0:\n",
    "        #WHAT\n",
    "        pos = transfer_pos_sentence(pos)\n",
    "        grammar = r\"\"\"\n",
    "                    J:\n",
    "                        {<JJ.*><VBN>}\n",
    "                        {<JJ.*><POSICC><JJ.*>}   \n",
    "                        {<JJ.*>+}\n",
    "                        {<NN.*><POS>}\n",
    "                    N:\n",
    "                        {<CD>+<NN.*>}\n",
    "                        {<A>?<NN.*>?<J>?<NN.*>+}\n",
    "                        <\\\">{<A>?<J>?<NN.*>+}<\\\">\n",
    "                    COMBON:\n",
    "                        {(<N><,>)*<N><,>?<POSICC><N>}\n",
    "                    NWC:\n",
    "                        {<N><WITH><COMBON>}\n",
    "                    \"\"\"\n",
    "    elif texttype==1:\n",
    "        pos = transfer_pos_question(pos)\n",
    "        #print pos\n",
    "        grammar = r\"\"\"\n",
    "                    WHAT: \n",
    "                        {<WHAT>}\n",
    "                    WHEN:\n",
    "                        {<HOW><TIME>}\n",
    "                        {<WHAT><BE>?<DT>?<JJ>?<NN>*<JJ>?<TIME>}\n",
    "                        {<WHEN>}\n",
    "                    WHERE:\n",
    "                        {<WHERE>}\n",
    "                    HOW:\n",
    "                        {<HOW>}\n",
    "                    WHO:\n",
    "                        {<WHO>}\n",
    "                    \"\"\"\n",
    "    elif texttype==2:\n",
    "        #when\n",
    "        pos = transfer_pos_sentence(pos)\n",
    "        #print pos\n",
    "        grammar = r\"\"\"\n",
    "                    C:\n",
    "                        {<CD><TIME>}\n",
    "                        {<CD><\\.><CD>}\n",
    "                        {<RB><CD>+}   \n",
    "                        {<JJ><CD>+}\n",
    "                        {<AROUND><CD>+}\n",
    "                        {<RB><TIME>+}   \n",
    "                        {<JJ><TIME>+}\n",
    "                        {<AROUND><TIME>+}\n",
    "                        {<TIME>+}\n",
    "                        {<CD>+}      \n",
    "                    COMBOC:\n",
    "                        {<C><POSICC><C>}\n",
    "                    \"\"\"\n",
    "    elif texttype==3:\n",
    "        #when\n",
    "        pos = transfer_pos_sentence(pos)\n",
    "        #print pos\n",
    "        grammar = r\"\"\"\n",
    "                    H:\n",
    "                        {<RB><JJ><IN>?}\n",
    "                    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar) \n",
    "    result = []\n",
    "    poss = copy.deepcopy(pos)\n",
    "    tree = cp.parse(pos)\n",
    "    #record the position of pos\n",
    "    flag = 0\n",
    "    for subtree in tree.subtrees():\n",
    "        if texttype==0 and subtree.label() != 'S':\n",
    "            phrase = u''\n",
    "            for word,pos in subtree.leaves():\n",
    "                if word == ',':\n",
    "                    phrase = phrase + word\n",
    "                else:\n",
    "                    phrase = phrase + u' '\n",
    "                    phrase = phrase + word\n",
    "            result.append((subtree.label(),phrase[1:]))\n",
    "            #print subtree.label(),phrase\n",
    "        elif subtree.label() != 'S':\n",
    "            phrase = u' '.join([word for word,pos in subtree.leaves()])\n",
    "            result.append((subtree.label(),phrase))\n",
    "            \n",
    "            \n",
    "    return poss,result"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 67,
=======
   "execution_count": 12,
>>>>>>> origin/master
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "WHO\n",
      "Q:  From the 1990s to 2010s who was the primary consumer of vinyl records?\n",
      "S:  From the 1990s to the 2010s, records continued to be manufactured and sold on a much smaller scale, and were especially used by disc jockeys (DJ)s, released by artists in some genres, and listened to by a niche market of audiophiles.\n",
      "[(u'From', 'IN'), (u'the', 'DT'), (u'1990s', 'CD'), (u'to', 'TO'), (u'the', 'DT'), (u'2010s', 'CD'), (u',', ','), (u'records', 'NNS'), (u'continued', 'VBD'), (u'to', 'TO'), (u'be', 'VB'), (u'manufactured', 'VBN'), (u'and', 'CC'), (u'sold', 'VBN'), (u'on', 'IN'), (u'a', 'DT'), (u'much', 'RB'), (u'smaller', 'JJR'), (u'scale', 'NN'), (u',', ','), (u'and', 'CC'), (u'were', 'VBD'), (u'especially', 'RB'), (u'used', 'VBN'), (u'by', 'IN'), (u'disc', 'JJ'), (u'jockeys', 'NNS'), (u'(', '('), (u'DJ', 'NNP'), (u')', ')'), (u's', 'NN'), (u',', ','), (u'released', 'VBN'), (u'by', 'IN'), (u'artists', 'NNS'), (u'in', 'IN'), (u'some', 'DT'), (u'genres', 'NNS'), (u',', ','), (u'and', 'CC'), (u'listened', 'VBD'), (u'to', 'TO'), (u'by', 'IN'), (u'a', 'DT'), (u'niche', 'NN'), (u'market', 'NN'), (u'of', 'IN'), (u'audiophiles', 'NNS')]\n",
      "@@@A:  disc jockeys (DJ)s\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "WHO\n",
      "Q:  Who released 7\" microgrooved records in the 1950s?\n",
      "S:  Also in the late 1950s, Bell Records released a few budget-priced 7\" microgrooved records at 78 rpm.\n",
      "[(u'Also', 'RB'), (u'in', 'IN'), (u'the', 'DT'), (u'late', 'JJ'), (u'1950s', 'NNS'), (u',', ','), (u'Bell Records', 'NNS'), (u'released', 'VBD'), (u'a', 'DT'), (u'few', 'JJ'), (u'budget-priced', 'JJ'), (u'7', 'CD'), (u'\"', 'NNS'), (u'microgrooved', 'VBD'), (u'records', 'NNS'), (u'at', 'IN'), (u'78', 'CD'), (u'rpm', 'NN')]\n",
      "@@@A:  Bell Records\n",
      "---------------\n",
      "WHO\n",
      "Q:  Who founded Audiophile Records?\n",
      "S:  One of the first attempts at this was in the 1950s, when inventor Ewing Dunbar Nunn founded the label Audiophile Records, which released, in addition to standard 33 1/3 rpm LPs, 78 rpm-mastered albums that were microgroove and pressed on vinyl (as opposed to traditional 78s, with their shellac composition and wider 3-mil sized grooves).\n",
      "[(u'One', 'CD'), (u'of', 'IN'), (u'the', 'DT'), (u'first', 'JJ'), (u'attempts', 'NNS'), (u'at', 'IN'), (u'this', 'DT'), (u'was', 'VBD'), (u'in', 'IN'), (u'the', 'DT'), (u'1950s', 'NNS'), (u',', ','), (u'when', 'WRB'), (u'inventor', 'NN'), (u'Ewing Dunbar Nunn', 'NNP'), (u'founded', 'VBD'), (u'the', 'DT'), (u'label', 'NN'), (u'Audiophile Records', 'NNP'), (u',', ','), (u'which', 'WDT'), (u'released', 'VBD'), (u',', ','), (u'in', 'IN'), (u'addition', 'NN'), (u'to', 'TO'), (u'standard', 'VB'), (u'33', 'CD'), (u'1/3', 'CD'), (u'rpm', 'NN'), (u'LPs', 'NNP'), (u',', ','), (u'78', 'CD'), (u'rpm-mastered', 'JJ'), (u'albums', 'NNS'), (u'that', 'WDT'), (u'were', 'VBD'), (u'microgroove', 'RBS'), (u'and', 'CC'), (u'pressed', 'VBN'), (u'on', 'IN'), (u'vinyl', 'NN'), (u'(', '('), (u'as', 'IN'), (u'opposed', 'VBN'), (u'to', 'TO'), (u'traditional', 'JJ'), (u'78s', 'CD'), (u',', ','), (u'with', 'IN'), (u'their', 'PRP$'), (u'shellac', 'NN'), (u'composition', 'NN'), (u'and', 'CC'), (u'wider', 'VB'), (u'3-mil', 'JJ'), (u'sized', 'JJ'), (u'grooves', 'NNS'), (u')', ')')]\n",
      "@@@A:  Ewing Dunbar Nunn\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "WHO\n",
      "Q:  For whom was the 1990 release of 78 rpm intended?\n",
      "S:  In the 1990s Rhino Records issued a series of boxed sets of 78 rpm reissues of early rock and roll hits, intended for owners of vintage jukeboxes.\n",
      "[(u'In', 'IN'), (u'the', 'DT'), (u'1990s', 'CD'), (u'Rhino Records', 'NNP'), (u'issued', 'VBD'), (u'a', 'DT'), (u'series', 'NN'), (u'of', 'IN'), (u'boxed', 'JJ'), (u'sets', 'NNS'), (u'of', 'IN'), (u'78', 'CD'), (u'rpm', 'NN'), (u'reissues', 'NNS'), (u'of', 'IN'), (u'early', 'JJ'), (u'rock', 'NN'), (u'and', 'CC'), (u'roll', 'NN'), (u'hits', 'NNS'), (u',', ','), (u'intended', 'VBN'), (u'for', 'IN'), (u'owners', 'NNS'), (u'of', 'IN'), (u'vintage', 'NN'), (u'jukeboxes', 'NNS')]\n",
      "@@@A:  owners of vintage jukeboxes\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "WHO\n",
      "Q:  Who developed the 33 1/3 rpm LP?\n",
      "S:  The 33 1⁄3 rpm LP (for \"long-play\") format was developed by Columbia Records and marketed in June 1948.\n",
      "[(u'The', 'DT'), (u'33', 'CD'), (u'1\\u20443', 'CD'), (u'rpm', 'NN'), (u'LP', 'NNP'), (u'(', '('), (u'for', 'IN'), (u'\"', 'NNP'), (u'long-play', 'JJ'), (u'\"', 'NN'), (u')', ')'), (u'format', 'NN'), (u'was', 'VBD'), (u'developed', 'VBN'), (u'by', 'IN'), (u'Columbia Records', 'NNS'), (u'and', 'CC'), (u'marketed', 'VBN'), (u'in', 'IN'), (u'June', 'NNP'), (u'1948', 'CD')]\n",
      "@@@A:  Columbia Records\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "WHO\n",
      "Q:  Who developed the 33 1/3 rpm speed record?\n",
      "S:  Peter Goldmark, the man who developed the 33 1⁄3 rpm record, developed the Highway Hi-Fi 16 2⁄3 rpm record to be played in Chrysler automobiles, but poor performance of the system and weak implementation by Chrysler and Columbia led to the demise of the 16 2⁄3 rpm records.\n",
      "[(u'Peter Goldmark', 'NNP'), (u',', ','), (u'the', 'DT'), (u'man', 'NN'), (u'who', 'WP'), (u'developed', 'VBD'), (u'the', 'DT'), (u'33', 'CD'), (u'1\\u20443', 'CD'), (u'rpm', 'NN'), (u'record', 'NN'), (u',', ','), (u'developed', 'VBD'), (u'the', 'DT'), (u'Highway Hi-Fi', 'NNP'), (u'16', 'CD'), (u'2\\u20443', 'CD'), (u'rpm', 'NN'), (u'record', 'NN'), (u'to', 'TO'), (u'be', 'VB'), (u'played', 'VBN'), (u'in', 'IN'), (u'Chrysler', 'NNP'), (u'automobiles', 'NNS'), (u',', ','), (u'but', 'CC'), (u'poor', 'JJ'), (u'performance', 'NN'), (u'of', 'IN'), (u'the', 'DT'), (u'system', 'NN'), (u'and', 'CC'), (u'weak', 'JJ'), (u'implementation', 'NN'), (u'by', 'IN'), (u'Chrysler', 'NNP'), (u'and', 'CC'), (u'Columbia', 'NNP'), (u'led', 'VBD'), (u'to', 'TO'), (u'the', 'DT'), (u'demise', 'NN'), (u'of', 'IN'), (u'the', 'DT'), (u'16', 'CD'), (u'2\\u20443', 'CD'), (u'rpm', 'NN'), (u'records', 'NNS')]\n",
      "@@@A:  Peter Goldmark\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "WHO\n",
      "Q:  Who developed the 3-to-2 mix to create lifelike recordings?\n",
      "S:  The Mercury Living Presence recordings were remastered to CD in the 1990s by the original producer, Wilma Cozart Fine, using the same method of 3-to-2 mix directly to the master recorder.\n",
      "[(u'The Mercury Living Presence', 'NN'), (u'recordings', 'NNS'), (u'were', 'VBD'), (u'remastered', 'VBN'), (u'to', 'TO'), (u'CD', 'VB'), (u'in', 'IN'), (u'the', 'DT'), (u'1990s', 'CD'), (u'by', 'IN'), (u'the', 'DT'), (u'original', 'JJ'), (u'producer', 'NN'), (u',', ','), (u'Wilma Cozart Fine', 'NNP'), (u',', ','), (u'using', 'VBG'), (u'the', 'DT'), (u'same', 'JJ'), (u'method', 'NN'), (u'of', 'IN'), (u'3-to-2', 'JJ'), (u'mix', 'NN'), (u'directly', 'RB'), (u'to', 'TO'), (u'the', 'DT'), (u'master', 'NN'), (u'recorder', 'NN')]\n",
      "@@@A:  Mercury\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "WHO\n",
      "Q:  Whom found that they can make musical instruments sound more natural with an amplifier boost at the turnover point?\n",
      "S:  G. H. Miller in 1934 reported that when complementary boost at the turnover point was used in radio broadcasts of records, the reproduction was more realistic and many of the musical instruments stood out in their true form.\n",
      "[(u'G. H. Miller', 'NNP'), (u'in', 'IN'), (u'1934', 'CD'), (u'reported', 'VBD'), (u'that', 'IN'), (u'when', 'WRB'), (u'complementary', 'JJ'), (u'boost', 'NN'), (u'at', 'IN'), (u'the', 'DT'), (u'turnover', 'NN'), (u'point', 'NN'), (u'was', 'VBD'), (u'used', 'VBN'), (u'in', 'IN'), (u'radio', 'JJ'), (u'broadcasts', 'NNS'), (u'of', 'IN'), (u'records', 'NNS'), (u',', ','), (u'the', 'DT'), (u'reproduction', 'NN'), (u'was', 'VBD'), (u'more', 'RBR'), (u'realistic', 'JJ'), (u'and', 'CC'), (u'many', 'JJ'), (u'of', 'IN'), (u'the', 'DT'), (u'musical', 'JJ'), (u'instruments', 'NNS'), (u'stood', 'VBD'), (u'out', 'RP'), (u'in', 'IN'), (u'their', 'PRP$'), (u'true', 'JJ'), (u'form', 'NN')]\n",
      "@@@A:  G. H. Miller\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "WHO\n",
      "Q:  Whom found Wente-style condenser microphones helpful?\n",
      "S:  West in 1930 and later P. G. A. H. Voigt (1940) showed that the early Wente-style condenser microphones contributed to a 4 to 6 dB midrange brilliance or pre-emphasis in the recording chain.\n",
      "[(u'West', 'NNP'), (u'in', 'IN'), (u'1930', 'CD'), (u'and', 'CC'), (u'later', 'RB'), (u'P. G. A. H. Voigt', 'NNP'), (u'(', '('), (u'1940', 'CD'), (u')', ')'), (u'showed', 'VBD'), (u'that', 'IN'), (u'the', 'DT'), (u'early', 'JJ'), (u'Wente-style', 'JJ'), (u'condenser', 'NN'), (u'microphones', 'NNS'), (u'contributed', 'VBD'), (u'to', 'TO'), (u'a', 'DT'), (u'4', 'CD'), (u'to', 'TO'), (u'6', 'CD'), (u'dB', 'NNS'), (u'midrange', 'JJ'), (u'brilliance', 'NN'), (u'or', 'CC'), (u'pre-emphasis', 'NN'), (u'in', 'IN'), (u'the', 'DT'), (u'recording', 'NN'), (u'chain', 'NN')]\n",
      "@@@A:  P. G. A. H. Voigt\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "WHO\n",
      "Q:  Whom wrote the publication outlining the New Orthophonic curve?\n",
      "S:  Ultimately, the New Orthophonic curve was disclosed in a publication by R.C. Moyer of RCA Victor in 1953.\n",
      "[(u'Ultimately', 'RB'), (u',', ','), (u'the', 'DT'), (u'New Orthophonic', 'NNP'), (u'curve', 'NN'), (u'was', 'VBD'), (u'disclosed', 'VBN'), (u'in', 'IN'), (u'a', 'DT'), (u'publication', 'NN'), (u'by', 'IN'), (u'R.C. Moyer', 'NNP'), (u'of', 'IN'), (u'RCA Victor', 'NNP'), (u'in', 'IN'), (u'1953', 'CD')]\n",
      "@@@A:  R.C. Moyer\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "WHO\n",
      "Q:  Who was responsible for 2/3 of all recordings in the 1930s?\n",
      "S:  Radio transcription producers such as World Broadcasting System and Associated Music Publishers (AMP) were the dominant licensees of the Western Electric wide range system and towards the end of the 1930s were responsible for two-thirds of the total radio transcription business.\n",
      "[(u'Radio', 'NN'), (u'transcription', 'NN'), (u'producers', 'NNS'), (u'such', 'JJ'), (u'as', 'IN'), (u'World Broadcasting System', 'NNP'), (u'and', 'CC'), (u'Associated Music Publishers', 'NNP'), (u'(', '('), (u'AMP', 'NNP'), (u')', ')'), (u'were', 'VBD'), (u'the', 'DT'), (u'dominant', 'JJ'), (u'licensees', 'NNS'), (u'of', 'IN'), (u'the', 'DT'), (u'Western Electric', 'NNP'), (u'wide', 'JJ'), (u'range', 'NN'), (u'system', 'NN'), (u'and', 'CC'), (u'towards', 'VBZ'), (u'the', 'DT'), (u'end', 'NN'), (u'of', 'IN'), (u'the', 'DT'), (u'1930s', 'CD'), (u'were', 'VBD'), (u'responsible', 'JJ'), (u'for', 'IN'), (u'two-thirds', 'NNS'), (u'of', 'IN'), (u'the', 'DT'), (u'total', 'JJ'), (u'radio', 'NN'), (u'transcription', 'NN'), (u'business', 'NN')]\n",
      "@@@A:  World Broadcasting System and Associated Music Publishers\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "WHO\n",
      "Q:  Who would engrave the master disc of a vinyl record?\n",
      "S:  A record cutter would engrave the grooves into the master disc.\n",
      "[(u'A', 'DT'), (u'record', 'NN'), (u'cutter', 'NN'), (u'would', 'MD'), (u'engrave', 'VB'), (u'the', 'DT'), (u'grooves', 'NNS'), (u'into', 'IN'), (u'the', 'DT'), (u'master', 'NN'), (u'disc', 'NN')]\n",
      "@@@A:  record cutter\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "---------------\n",
      "WHO\n",
      "Q:  Who sold 34,000 vinyl records in 1994?\n",
      "S:  The sales record was previously held by Pearl Jam's, Vitalogy, which sold 34,000 copies in one week in 1994.\n",
      "[(u'The', 'DT'), (u'sales', 'NNS'), (u'record', 'NN'), (u'was', 'VBD'), (u'previously', 'RB'), (u'held', 'VBN'), (u'by', 'IN'), (u\"Pearl Jam's\", 'NNP'), (u',', ','), (u'Vitalogy', 'NNP'), (u',', ','), (u'which', 'WDT'), (u'sold', 'VBD'), (u'34', 'CD'), (u',', ','), (u'000', 'CD'), (u'copies', 'NNS'), (u'in', 'IN'), (u'one', 'CD'), (u'week', 'NN'), (u'in', 'IN'), (u'1994', 'CD')]\n",
      "@@@A:  Pearl Jam\n",
      "0.589928057554\n"
=======
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource u'taggers/averaged_perceptron_tagger/averaged_perceptro\n  n_tagger.pickle' not found.  Please use the NLTK Downloader to\n  obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/Venn/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-0fafefb0cbf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mtext_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0manswer_sentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mqa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'answer_sentence'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mpos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_continuous_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_question\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m#print pos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-ea04ec26623d>\u001b[0m in \u001b[0;36mget_continuous_chunks\u001b[0;34m(text, texttype)\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m#print token(t)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtexttype\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransfer_pos_sentence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Venn/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/tag/__init__.pyc\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Venn/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/tag/__init__.pyc\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0map_russian_model_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'eng'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Venn/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/tag/perceptron.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load)\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mAP_MODEL_LOC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'file:'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'taggers/averaged_perceptron_tagger/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mPICKLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAP_MODEL_LOC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Venn/anaconda3/envs/python2/lib/python2.7/site-packages/nltk/data.pyc\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'taggers/averaged_perceptron_tagger/averaged_perceptro\n  n_tagger.pickle' not found.  Please use the NLTK Downloader to\n  obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/Users/Venn/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
>>>>>>> origin/master
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "def input_NER():\n",
    "    stanford_dir = os.path.join('stanford-ner-2016-10-31')\n",
    "    jarfile = os.path.join(stanford_dir,'stanford-ner.jar')\n",
    "    modelfile = os.path.join(stanford_dir,'classifiers/english.muc.7class.distsim.crf.ser.gz')\n",
    "    return modelfile,jarfile\n",
    "model,jar = input_NER()\n",
    "st = StanfordNERTagger(model,jar)\n",
    "\n",
    "total_count = 0.0\n",
    "total_question = 0.0\n",
    "#for index,article in enumerate(train[:1]):\n",
    "article = train[0]\n",
    "index = 0\n",
    "qas = article['qa']\n",
    "sentences = article['sentences']\n",
    "count = 0\n",
    "for i in correct[index]:\n",
    "    qa = qas[i]\n",
    "    #print '---------------'\n",
    "    flag = False\n",
    "    text_question = qa['question']\n",
    "    answer_sentence = sentences[qa['answer_sentence']]\n",
    "    pos,result = get_continuous_chunks(text_question,1)\n",
    "    #print pos\n",
    "    if result != []:\n",
    "        wtype,word = result[0]\n",
    "        total_question += 1\n",
    "        if wtype == 'WHAT':\n",
    "            answer_list = []      \n",
    "            print '---------------'\n",
    "            #print result\n",
    "            wpos,wresult = get_continuous_chunks(answer_sentence,0)\n",
    "            for wwtype,wword in wresult:\n",
    "                if qa['answer'] == wword:\n",
    "                    flag = True\n",
    "                    total_count += 1\n",
    "                #print answer_list\n",
    "        elif wtype == 'WHEN':\n",
    "            answer_list = []\n",
    "            print '---------------'\n",
    "            wpos,wresult = get_continuous_chunks(answer_sentence,2)\n",
    "            for wwtype,wword in wresult:\n",
    "                if qa['answer'] == wword:\n",
    "                    flag = True\n",
    "                    total_count += 1\n",
    "            '''if flag == False:\n",
    "                print 'WHEN:'\n",
    "                print 'Q: ',text_question\n",
    "                print pos\n",
    "                print 'S: ',answer_sentence\n",
    "                print wpos\n",
    "                print '@@@A: ',qa['answer']\n",
    "                for t,w in wresult:\n",
    "                    if t != u'J':\n",
    "                        answer_list.append(w)\n",
    "                print answer_list'''\n",
    "        elif wtype == 'HOW':\n",
    "            '''print '---------------'\n",
    "            print wtype\n",
    "            print 'Q: ',text_question\n",
    "            print 'S: ',answer_sentence\n",
    "            print pos_tag(token(answer_sentence))\n",
    "            print '@@@A: ',qa['answer']'''\n",
    "            wpos,wresult = get_continuous_chunks(answer_sentence,3)\n",
    "            for wwtype,wword in wresult:\n",
    "                if qa['answer'] == wword:\n",
    "                    flag = True\n",
    "                    total_count += 1\n",
    "        elif wtype == 'WHO':\n",
    "            print '---------------'\n",
    "            print wtype\n",
    "            print 'Q: ',text_question\n",
    "            print 'S: ',answer_sentence\n",
    "            print pos_tag(token(answer_sentence))\n",
    "            print '@@@A: ',qa['answer']\n",
    "print total_count/total_question   "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'There', u'O'), (u'is', u'O'), (u'a', u'O'), (u'dedicated', u'O'), (u'museum', u'O'), (u'in', u'O'), (u'Montreal', u'LOCATION'), (u'for', u'O'), (u'Berline', u'LOCATION')]\n"
     ]
    }
   ],
   "source": [
    "text = 'There is a dedicated museum in Montreal for Berliner'\n",
    "def input_NER():\n",
    "    stanford_dir = os.path.join('stanford-ner-2016-10-31')\n",
    "    jarfile = os.path.join(stanford_dir,'stanford-ner.jar')\n",
    "    modelfile = os.path.join(stanford_dir,'classifiers/english.muc.7class.distsim.crf.ser.gz')\n",
    "    return modelfile,jarfile\n",
    "model,jar = input_NER()\n",
    "st = StanfordNERTagger(model,jar)\n",
    "#print token(text)\n",
    "print st.tag(token(text))"
   ]
  },
  {
   "cell_type": "code",
=======
>>>>>>> origin/master
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
