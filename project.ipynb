{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get document frequency for terms in the same article/instance; and get each sentence's BOW; and get each question's BOW;\n",
    "#get each sentence's length; get each article's average length of sentences\n",
    "import csv\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import nltk\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tag import StanfordNERTagger\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "def input_data():\n",
    "    base_path = os.path.join('data/')\n",
    "    train_file = base_path + 'QA_train.json'\n",
    "    train_data = json.load(open(train_file))\n",
    "    test_file = base_path + 'QA_test.json'\n",
    "    test_data = json.load(open(test_file))\n",
    "    dev_file = base_path + 'QA_dev.json'\n",
    "    dev_data = json.load(open(dev_file))\n",
    "\n",
    "    return train_data,test_data,dev_data\n",
    "def transform_text(text):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = [lemmatize(word.lower()) for word in text]\n",
    "    result = []\n",
    "    for word in text:\n",
    "        if word not in stopwords and word not in punctuations:\n",
    "            result.append(word)\n",
    "    return result\n",
    "def get_Docfrequency_SentenceBOW(dataset):\n",
    "    #save dics, each dictionary contains document frequencies for all terms in the same article\n",
    "    question_list = []\n",
    "    #save lists, each list represent an article, saving sentences' bow\n",
    "    total_sentence_bow = []\n",
    "    #save lists, each list represent an article, saving questions' bow\n",
    "    total_question_bow = []\n",
    "    #save lists, each list represent all sentences' lengthes.\n",
    "    sent_lengthes = []\n",
    "    #save a list, each item represents the average length of sentences\n",
    "    avg_lengthes = []\n",
    "    #\n",
    "    answer_id = []\n",
    "    \n",
    "    for article in dataset:\n",
    "        #Docfrequency\n",
    "        article_dic = defaultdict(list)\n",
    "        keyterms = [] #save all distinct terms in questions\n",
    "        \n",
    "        #SentenceBOW\n",
    "        bow_list = []\n",
    "        \n",
    "        #QuestionBOW\n",
    "        que_list = []\n",
    "        \n",
    "        #SentenceLength\n",
    "        sent_len = []\n",
    "        \n",
    "        #TotalLength\n",
    "        total_len = 0\n",
    "        \n",
    "        #RightAnser\n",
    "        right_answer = []\n",
    "        \n",
    "        qas = article['qa']\n",
    "        sentences = article['sentences']\n",
    "        for qa in qas:\n",
    "            question = qa['question']\n",
    "            newquestion = transform_text(question)\n",
    "            #QuestionBOW\n",
    "            que_list.append(get_BOW(newquestion))\n",
    "            answer = qa['answer_sentence']\n",
    "            right_answer.append(answer)\n",
    "            \n",
    "            keyterms.extend(newquestion)\n",
    "        keyterms = set(keyterms)\n",
    "        \n",
    "        #save sentences' BOW in list sen_BOW\n",
    "        sen_words = []\n",
    "        for sent in sentences:\n",
    "            sent = transform_text(sent)\n",
    "            #Docfrequency\n",
    "            sen_words.append(sent)\n",
    "            \n",
    "            #SentenceBOW\n",
    "            bow_list.append(get_BOW(sent))\n",
    "            \n",
    "            #SentenceLength\n",
    "            sent_len.append(len(sent))\n",
    "            \n",
    "            #TotalLength\n",
    "            total_len += len(sent)\n",
    "            \n",
    "        \n",
    "        #calculate doc frequency    \n",
    "        for term in keyterms:\n",
    "            for i,bow in enumerate(sen_words):\n",
    "                if term in bow:\n",
    "                    article_dic[term].append(i)\n",
    "                    \n",
    "        #Docfrequency\n",
    "        question_list.append(article_dic)\n",
    "        #SentenceBOW\n",
    "        total_sentence_bow.append(bow_list)\n",
    "        #QuestionBOW\n",
    "        total_question_bow.append(que_list)\n",
    "        #SentenceLength\n",
    "        sent_lengthes.append(sent_len)\n",
    "        #AverageLength\n",
    "        avg_lengthes.append(float(total_len)/len(sentences))\n",
    "        #\n",
    "        answer_id.append(right_answer)\n",
    "        \n",
    "    return question_list, total_sentence_bow, total_question_bow, sent_lengthes, avg_lengthes, answer_id\n",
    "\n",
    "train,test,dev = input_data()\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "punctuations = [',','\\'\\'','?','\\'','.','%','(',')',';','``']\n",
    "question_list, total_sentence_bow, total_question_bow, sent_lengthes, avg_lengthes, answer_id_list= get_Docfrequency_SentenceBOW(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def BM25(articles_index,k1,k2,b):\n",
    "    total_queries = len(total_question_bow[articles_index])\n",
    "    count = 0\n",
    "    correct_id = []\n",
    "    for index in range(len(total_question_bow[articles_index])):\n",
    "        answer_id = answer_id_list[articles_index][index]\n",
    "        guess_id = find_max_score_sentence(articles_index,index,k1,k2,b)\n",
    "        if answer_id == guess_id:\n",
    "            count += 1\n",
    "            correct_id.append(index)\n",
    "    accuracy = float(count)/total_queries  \n",
    "    return correct_id, accuracy\n",
    "\n",
    "def find_max_score_sentence(articles_index,index,k1,k2,b):\n",
    "\n",
    "    query_dict = total_question_bow[articles_index][index]\n",
    "    max_score = 0\n",
    "    guess_sentence = 0\n",
    "    for index in range(len(total_sentence_bow[articles_index])):     \n",
    "        score = 0  \n",
    "        sentence_dict = total_sentence_bow[articles_index][index]\n",
    "        for word in query_dict:\n",
    "            document_fre_list = question_list[articles_index].get(word,None)\n",
    "            \n",
    "            \n",
    "            N = len(total_sentence_bow[articles_index])\n",
    "            n_qi = 0\n",
    "            if document_fre_list != None:\n",
    "                n_qi = len(document_fre_list)\n",
    "            else:\n",
    "                n_qi = 0\n",
    "            fi = sentence_dict.get(word,0)\n",
    "            qfi = query_dict.get(word,0)\n",
    "            dl = sent_lengthes[articles_index][index]\n",
    "            avgdl = avg_lengthes[articles_index]\n",
    "            \n",
    "            K = k1*(1-b+b*(float(dl)/avgdl)) \n",
    "            W = math.log((N-n_qi+0.5)/(n_qi+0.5))\n",
    "            R = (fi*(k1+1))/(fi+K)*qfi*(k2+1)/(qfi+k2)\n",
    "            score += W*R\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            guess_sentence = index\n",
    "    return guess_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of BM25 with k1: 1.1  k2: 110  b: 0.18\n",
      "0.683486756477\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0.0\n",
    "\n",
    "k1_list = [1.1]\n",
    "k2_list = [110]\n",
    "b_list = [0.18]\n",
    "\n",
    "correct = []\n",
    "\n",
    "test_length = len(train)\n",
    "\n",
    "for k1 in k1_list:\n",
    "    for k2 in k2_list:\n",
    "        for b in b_list:\n",
    "            accuracy = 0.0\n",
    "            for i in range(0,test_length):\n",
    "                correct_id,i_accuracy = BM25(i,k1,k2,b)\n",
    "                accuracy += i_accuracy\n",
    "                correct.append(correct_id)\n",
    "            average_accuracy = accuracy/test_length\n",
    "            print \"Accuracy of BM25 with k1:\",k1,\" k2:\",k2,\" b:\",b\n",
    "            print average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Accurancy of BM25 with k1: 1.1  k2: 110  b: 0.18\n",
    "#0.683486756477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import re\n",
    "import sys\n",
    "def token(text):\n",
    "    result = []\n",
    "    combo = u''\n",
    "    flag_upper = False\n",
    "    x = re.split(r'(\"|\\.|\\?|;|,|\\(|\\s|\\))\\s*', text)\n",
    "    #print x\n",
    "    for word in x:\n",
    "        if word != u'' and word != u' ':\n",
    "            if word[0].isalpha():\n",
    "                if flag_upper == False and word[0].isupper():\n",
    "                    combo = word\n",
    "                    flag_upper = True\n",
    "                elif flag_upper == False and word[0].islower():\n",
    "                    result.append(word)\n",
    "                elif flag_upper == True and word[0].isupper():\n",
    "                    combo = combo + u' ' + word\n",
    "                elif flag_upper == True and word[0].islower():\n",
    "                    result.append(combo)\n",
    "                    result.append(word)\n",
    "                    flag_upper = False\n",
    "            else:\n",
    "                if flag_upper == True:\n",
    "                    result.append(combo)\n",
    "                    result.append(word)\n",
    "                    flag_upper = False\n",
    "                else:\n",
    "                    result.append(word)\n",
    "    return result\n",
    "def transfer_pos_question(pos):\n",
    "    new_pos = []\n",
    "    for (word,wtype) in pos:\n",
    "        if word.lower() == 'what':\n",
    "            new_pos.append((word,'WHAT'))\n",
    "        elif word.lower() == 'do' or word.lower() == 'does' or word.lower() == 'did':\n",
    "            new_pos.append((word,'DO'))\n",
    "        elif word.lower() == 'year' or word.lower() == 'era' or word.lower() == 'ration' or word.lower() == 'years' or word.lower() == 'time':\n",
    "            new_pos.append((word,'TIME'))\n",
    "        elif word.lower() == 'is' or word.lower() == 'was' or word.lower() == 'are' or word.lower() == 'were':\n",
    "            new_pos.append((word,'BE'))\n",
    "        else:\n",
    "            new_pos.append((word,wtype))\n",
    "    return new_pos\n",
    "def transfer_pos_sentence(pos):\n",
    "    new_pos = []\n",
    "    for (word,wtype) in pos:\n",
    "        if word.lower() == 'and' or word.lower() == 'or':\n",
    "            new_pos.append((word,'POSICC'))\n",
    "        elif word.lower() == 'with':\n",
    "            new_pos.append((word,'WITH'))\n",
    "        elif word.lower() == 'a' or word.lower() == 'an':\n",
    "            new_pos.append((word,'A'))\n",
    "        elif word == '\"':\n",
    "            new_pos.append((word,'\"'))\n",
    "        else:\n",
    "            new_pos.append((word,wtype))\n",
    "    return new_pos\n",
    "def get_continuous_chunks(text,texttype):\n",
    "    #text = text.encode('ascii','replace')\n",
    "    t = copy.deepcopy(text)\n",
    "    #print token(t)\n",
    "    pos =  pos_tag(token(t))\n",
    "    if texttype==0:\n",
    "        pos = transfer_pos_sentence(pos)\n",
    "        #print pos\n",
    "        #<\\(>{<[^\\(\\)]*>+}<\\)>\n",
    "        grammar = r\"\"\"\n",
    "                    J:\n",
    "                        {<JJ.*><VBN>}\n",
    "                        {<JJ.*><POSICC><JJ.*>}   \n",
    "                        {<JJ.*>+}\n",
    "                        {<NN.*><POS>}\n",
    "                    N:\n",
    "                        {<A>?<J>?<NN.*>+}\n",
    "                        <\\(>{(<CD><,>)*<CD>}<\\)>\n",
    "                        <\\\">{<A>?<J>?<NN.*>+}<\\\">\n",
    "                    COMBON:\n",
    "                        {(<N><,>)*<N><,>?<POSICC><N>}\n",
    "                    NWC:\n",
    "                        {<N><WITH><COMBON>}\n",
    "                    \"\"\"\n",
    "    elif texttype==1:\n",
    "        pos = transfer_pos_question(pos)\n",
    "        #print pos\n",
    "        grammar = r\"\"\"\n",
    "                    WHATDO:\n",
    "                        {<WHAT><DO>}\n",
    "                    TIME:\n",
    "                        {<WHAT><BE>?<DT>?<NN>?<JJ>?<TIME>}\n",
    "                    WHAT: \n",
    "                        {<WHAT>}\n",
    "                    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar) \n",
    "    result = []\n",
    "    poss = copy.deepcopy(pos)\n",
    "    tree = cp.parse(pos)\n",
    "    #record the position of pos\n",
    "    flag = 0\n",
    "    for subtree in tree.subtrees():\n",
    "        if texttype==0 and subtree.label() != 'S':\n",
    "            phrase = u''\n",
    "            for word,pos in subtree.leaves():\n",
    "                if word == ',':\n",
    "                    phrase = phrase + word\n",
    "                else:\n",
    "                    phrase = phrase + u' '\n",
    "                    phrase = phrase + word\n",
    "            result.append((subtree.label(),phrase[1:]))\n",
    "            #print subtree.label(),phrase\n",
    "        elif texttype==1 and subtree.label() != 'S':\n",
    "            phrase = u' '.join([word for word,pos in subtree.leaves()])\n",
    "            result.append((subtree.label(),phrase))\n",
    "            #print subtree.label(),phrase\n",
    "            \n",
    "    return poss,result\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "def input_NER():\n",
    "    stanford_dir = os.path.join('stanford-ner-2016-10-31')\n",
    "    jarfile = os.path.join(stanford_dir,'stanford-ner.jar')\n",
    "    modelfile = os.path.join(stanford_dir,'classifiers/english.muc.7class.distsim.crf.ser.gz')\n",
    "    return modelfile,jarfile\n",
    "model,jar = input_NER()\n",
    "st = StanfordNERTagger(model,jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_count = 0.0\n",
    "total_question = 0.0\n",
    "#for index,article in enumerate(train[:1]):\n",
    "article = train[0]\n",
    "index = 0\n",
    "qas = article['qa']\n",
    "sentences = article['sentences']\n",
    "token_sentences = copy.deepcopy(sentences)\n",
    "for i in range(len(sentences)):\n",
    "    token_sentences[i] = nltk.word_tokenize(token_sentences[i])\n",
    "ner_sentences = st.tag_sents(token_sentences)\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_rule_1(entity,query):\n",
    "    #lower scores for content words also appear in the query\n",
    "    query = nltk.word_tokenize(query)\n",
    "    for index in range(len(query)):\n",
    "        query[index] = lemmatize(query[index].lower())\n",
    "    count = 0\n",
    "    length = len(entity)\n",
    "    for word in entity:\n",
    "        word = lemmatize(word.lower())\n",
    "        if word in query:\n",
    "            count += 1\n",
    "    score = 1 - float(count)/length\n",
    "    return score\n",
    "\n",
    "def rank_rule_2(ner_entity,query):\n",
    "    #answers which match the question type should be ranked higher\n",
    "    question_word = get_question_word(query)\n",
    "    question_type = get_question_type(question_word,query)\n",
    "    if ner_entity == question_type:\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What is the niche market of phonograph record fans known as?\n",
      "audiophiles\n",
      "[u'records', u'smaller scale', u'disc jockeys', u'DJ', u's', u'artists', u'genres', u'a niche market', u'audiophiles']\n",
      "From the 1990s to the 2010s, records continued to be manufactured and sold on a much smaller scale, and were especially used by disc jockeys (DJ)s, released by artists in some genres, and listened to by a niche market of audiophiles.\n",
      "[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What was the original intent of the phonautograph?\n",
      "visual analysis\n",
      "[u'phonautograph', u'L\\xe9on Scott', u'diaphragm and stylus', u'diaphragm', u'stylus', u'sound waves', u'tracings', u'sheets', u'paper', u'visual analysis', u'intent']\n",
      "The phonautograph, patented by Léon Scott in 1857, used a vibrating diaphragm and stylus to graphically record sound waves as tracings on sheets of paper, purely for visual analysis and without any intent of playing them back.\n",
      "[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What is the name of lateral cut disc records?\n",
      "gramophone\n",
      "[u'Lateral-cut disc records', u'United States', u'Emile Berliner', u'system', u'gramophone', u\"Edison's wax cylinder\", u'phonograph', u\"Columbia's\", u'graphophone']\n",
      "Lateral-cut disc records were developed in the United States by Emile Berliner, who named his system the \"gramophone\", distinguishing it from Edison's wax cylinder \"phonograph\" and Columbia's wax cylinder \"graphophone\".\n",
      "[0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What disc format was the least fragile prior to 1919?\n",
      "Amberol cylinder\n",
      "[u'an attempt', u'disc advantage', u'Edison', u'Amberol cylinder', u'a maximum playing time', u'minutes', u'rpm', u'turn', u'Blue Amberol Records', u'a playing surface', u'celluloid', u'a plastic']\n",
      "In an attempt to head off the disc advantage, Edison introduced the Amberol cylinder in 1909, with a maximum playing time of 4½ minutes (at 160 rpm), which in turn were superseded by Blue Amberol Records, which had a playing surface made of celluloid, a plastic, which was far less fragile.\n",
      "[1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What was the playing surface of the blue amerbol cylinder discs made of?\n",
      "celluloid\n",
      "[u'an attempt', u'disc advantage', u'Edison', u'Amberol cylinder', u'a maximum playing time', u'minutes', u'rpm', u'turn', u'Blue Amberol Records', u'a playing surface', u'celluloid', u'a plastic']\n",
      "In an attempt to head off the disc advantage, Edison introduced the Amberol cylinder in 1909, with a maximum playing time of 4½ minutes (at 160 rpm), which in turn were superseded by Blue Amberol Records, which had a playing surface made of celluloid, a plastic, which was far less fragile.\n",
      "[1.0, 0.5, 1.0, 0.5, 0.75, 1.0, 1.0, 1.0, 0.6666666666666667, 0.33333333333333337, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What instruments did not record well?\n",
      "Lower-pitched orchestral instruments\n",
      "[u'Lower-pitched orchestral instruments', u'cellos and double basses', u'cellos', u'double basses', u'louder wind instruments', u'tubas']\n",
      "Lower-pitched orchestral instruments such as cellos and double basses were often doubled (or replaced) by louder wind instruments, such as tubas.\n",
      "[0.6666666666666667, 1.0, 1.0, 1.0, 0.6666666666666667, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What was an early took used to amplify sounds?\n",
      "vacuum tubes\n",
      "[u'first half', u'engineers', u'Western Electric', u'independent inventors', u'Orlando Marsh', u'technology', u'sound', u'a microphone', u'vacuum tubes', u'amplified signal', u'an electromagnetic recording head']\n",
      "During the first half of the 1920s, engineers at Western Electric, as well as independent inventors such as Orlando Marsh, developed technology for capturing sound with a microphone, amplifying it with vacuum tubes, then using the amplified signal to drive an electromagnetic recording head.\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 0.75]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What materials were discs made of in 1889-1894?\n",
      "hard rubber\n",
      "[u'earliest disc records', u'1889\\u20131894', u'various materials', u'hard rubber']\n",
      "The earliest disc records (1889–1894) were made of various materials including hard rubber.\n",
      "[0.6666666666666667, 1.0, 0.5, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What was the standard material for discs around 1895?\n",
      "a shellac-based compound\n",
      "[u'a shellac-based compound', u'standard']\n",
      "Around 1895, a shellac-based compound was introduced and became standard.\n",
      "[1.0, 0.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What company was known for laminate construction of discs?\n",
      "Columbia Records\n",
      "[u'makers', u'Columbia Records', u'a laminated construction', u'a core disc', u'coarser material or fiber', u'coarser material', u'fiber']\n",
      "Some makers, notably Columbia Records, used a laminated construction with a core disc of coarser material or fiber.\n",
      "[1.0, 1.0, 0.33333333333333337, 0.6666666666666667, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What material give the vinyl records their known black color?\n",
      "carbon\n",
      "[u'Exact formulas', u'compound', u'manufacturer', u'course', u'time', u'one-third shellac', u'two-thirds', u'mineral filler', u'rock', u'limestone', u'an admixture', u'cotton fibers', u'tensile strength', u'carbon', u'color', u'dirty', u'gray or brown color', u'gray', u'brown color', u'most record companies', u'small amount', u'a lubricant', u'mold release', u'manufacture']\n",
      "Exact formulas for this compound varied by manufacturer and over the course of time, but it was typically composed of about one-third shellac and about two-thirds mineral filler, which meant finely pulverized rock, usually slate and limestone, with an admixture of cotton fibers to add tensile strength, carbon black for color (without this, it tended to be a \"dirty\" gray or brown color that most record companies considered unattractive), and a very small amount of a lubricant to facilitate mold release during manufacture.\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.75, 1.0, 0.5, 0.6666666666666667, 1.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What was a major downfall of the success of Durium records?\n",
      "Great Depression\n",
      "[u'victim', u'Great Depression and production', u'Great Depression', u'production', u'US']\n",
      "Although inexpensive and commercially successful at first, they soon fell victim to the Great Depression and production in the US ended in 1932.\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What was a common problem found in early flexible records?\n",
      "surface noise\n",
      "[u'UK', u'Nicole records', u'celluloid or a similar substance', u'celluloid', u'a similar substance', u'a cardboard core disc', u'a few years', u'high level', u'surface noise']\n",
      "In the UK, Nicole records, made of celluloid or a similar substance coated onto a cardboard core disc, were produced for a few years beginning in 1904, but they suffered from an exceptionally high level of surface noise.\n",
      "[1.0, 0.5, 0.8, 1.0, 0.6666666666666667, 0.75, 0.6666666666666667, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What material was used by RCA Victor for special purpose records?\n",
      "vinyl-based Victrolac compound\n",
      "[u'RCA Victor', u'vinyl-based Victrolac compound', u'a material', u'unusual-format and special-purpose records']\n",
      "In 1931, RCA Victor introduced their vinyl-based Victrolac compound as a material for some unusual-format and special-purpose records.\n",
      "[0.0, 1.0, 0.5, 0.75]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What was Metrolite and Sav-o-flex materials primarily used for?\n",
      "children's records\n",
      "[u'Decca Records', u'vinyl Deccalite', u'other record companies', u'vinyl concoctions', u'Metrolite, Merco Plastic and Sav-o-flex', u'Metrolite', u'Merco Plastic', u'Sav-o-flex', u\"children's records and special thin vinyl DJ pressings\", u\"children's records\", u'special thin vinyl DJ pressings', u'shipment', u'stations']\n",
      "Later, Decca Records introduced vinyl Deccalite 78s, while other record companies came up with vinyl concoctions such as Metrolite, Merco Plastic and Sav-o-flex, but these were mainly used to produce \"unbreakable\" children's records and special thin vinyl DJ pressings for shipment to radio stations.\n",
      "[1.0, 1.0, 1.0, 1.0, 0.5, 0.0, 1.0, 0.0, 0.8888888888888888, 1.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What was the most popular sized disc by 1910?\n",
      "10-inch\n",
      "[u'cm', u'record', u'popular standard', u'minutes', u'music or other entertainment', u'music', u'other entertainment', u'a side']\n",
      "By 1910 the 10-inch (25.4 cm) record was by far the most popular standard, holding about three minutes of music or other entertainment on a side.\n",
      "[1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What was the normal size disc for popular music?\n",
      "10-inch\n",
      "[u'rpm record', u'minutes', u'side', u'10-inch size', u'standard size', u'popular music', u'popular recordings', u'minutes', u'length']\n",
      "Because a 10-inch 78 rpm record could hold about three minutes of sound per side and the 10-inch size was the standard size for popular music, almost all popular recordings were limited to around three minutes in length.\n",
      "[1.0, 1.0, 1.0, 0.5, 0.5, 0.0, 0.5, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What were early record album covers made of?\n",
      "empty sleeves with a paperboard or leather cover\n",
      "[u'collections', u'empty sleeves with a paperboard or leather cover', u'empty sleeves', u'a paperboard or leather cover', u'a paperboard', u'leather cover', u'a photograph album', u'record', u'customers', u'records', u'term', u'record album', u'covers']\n",
      "By about 1910,[note 1] bound collections of empty sleeves with a paperboard or leather cover, similar to a photograph album, were sold as record albums that customers could use to store their records (the term \"record album\" was printed on some covers).\n",
      "[1.0, 0.875, 1.0, 0.8, 1.0, 0.5, 0.6666666666666667, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "What is typically found on the front cover of an album?\n",
      "artwork\n",
      "[u'record companies', u'collections', u'rpm records', u'performer', u'type', u'music', u'albums', u'artwork', u'front cover and liner notes', u'front cover', u'liner notes', u'back or inside cover', u'back', u'inside cover']\n",
      "In the 1930s, record companies began issuing collections of 78 rpm records by one performer or of one type of music in specially assembled albums, typically with artwork on the front cover and liner notes on the back or inside cover.\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.6, 0.0, 1.0, 0.75, 1.0, 0.5]\n"
     ]
    }
   ],
   "source": [
    "for i in correct[index]:\n",
    "    answer_list = []\n",
    "    qa = qas[i]\n",
    "    #print '---------------'\n",
    "    flag = False\n",
    "    text_question = qa['question']\n",
    "    answer_sentence = sentences[qa['answer_sentence']]\n",
    "    ner_answer_sentence = ner_sentences[qa['answer_sentence']]\n",
    "    pos,result = get_continuous_chunks(text_question,1)\n",
    "    if result != []:\n",
    "        wtype,word = result[0]\n",
    "        if wtype == 'WHAT':\n",
    "            total_question += 1\n",
    "#             print '---------------'\n",
    "            #print result\n",
    "            wpos,wresult = get_continuous_chunks(answer_sentence,0)\n",
    "            for wwtype,wword in wresult:\n",
    "                if qa['answer'] == wword:\n",
    "                    flag = True\n",
    "                    total_count += 1\n",
    "            if flag == True:\n",
    "                #print 'Q: ',text_question\n",
    "                #print 'S: ',answer_sentence\n",
    "#                 print 'A: ',qa['answer']\n",
    "                for t,w in wresult:\n",
    "                    if t != u'J':\n",
    "                        answer_list.append(w)\n",
    "    if answer_list != []:\n",
    "        query = text_question\n",
    "        scores_1 = []\n",
    "        for entity in answer_list:\n",
    "            entity = nltk.word_tokenize(entity) \n",
    "            score = rank_rule_1(entity,query)\n",
    "            scores_1.append(score)\n",
    "        print '^'*100\n",
    "        print text_question\n",
    "        print qa['answer']\n",
    "        print answer_list\n",
    "        print answer_sentence\n",
    "        print scores_1\n",
    "        \n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
