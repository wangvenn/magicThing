{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get document frequency for terms in the same article/instance; and get each sentence's BOW; and get each question's BOW;\n",
    "#get each sentence's length; get each article's average length of sentences\n",
    "import csv\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import nltk\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tag import StanfordNERTagger\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "def input_data():\n",
    "    base_path = os.path.join('data/')\n",
    "    train_file = base_path + 'QA_train.json'\n",
    "    train_data = json.load(open(train_file))\n",
    "    test_file = base_path + 'QA_test.json'\n",
    "    test_data = json.load(open(test_file))\n",
    "    dev_file = base_path + 'QA_dev.json'\n",
    "    dev_data = json.load(open(dev_file))\n",
    "\n",
    "    return train_data,test_data,dev_data\n",
    "def transform_text(text):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = [lemmatize(word.lower()) for word in text]\n",
    "    result = []\n",
    "    for word in text:\n",
    "        if word not in stopwords and word not in punctuations:\n",
    "            result.append(word)\n",
    "    return result\n",
    "def get_Docfrequency_SentenceBOW(dataset):\n",
    "    #save dics, each dictionary contains document frequencies for all terms in the same article\n",
    "    question_list = []\n",
    "    #save lists, each list represent an article, saving sentences' bow\n",
    "    total_sentence_bow = []\n",
    "    #save lists, each list represent an article, saving questions' bow\n",
    "    total_question_bow = []\n",
    "    #save lists, each list represent all sentences' lengthes.\n",
    "    sent_lengthes = []\n",
    "    #save a list, each item represents the average length of sentences\n",
    "    avg_lengthes = []\n",
    "    #\n",
    "    answer_id = []\n",
    "    \n",
    "    for article in dataset:\n",
    "        #Docfrequency\n",
    "        article_dic = defaultdict(list)\n",
    "        keyterms = [] #save all distinct terms in questions\n",
    "        \n",
    "        #SentenceBOW\n",
    "        bow_list = []\n",
    "        \n",
    "        #QuestionBOW\n",
    "        que_list = []\n",
    "        \n",
    "        #SentenceLength\n",
    "        sent_len = []\n",
    "        \n",
    "        #TotalLength\n",
    "        total_len = 0\n",
    "        \n",
    "        #RightAnser\n",
    "        right_answer = []\n",
    "        \n",
    "        qas = article['qa']\n",
    "        sentences = article['sentences']\n",
    "        for qa in qas:\n",
    "            question = qa['question']\n",
    "            newquestion = transform_text(question)\n",
    "            #QuestionBOW\n",
    "            que_list.append(get_BOW(newquestion))\n",
    "            answer = qa['answer_sentence']\n",
    "            right_answer.append(answer)\n",
    "            \n",
    "            keyterms.extend(newquestion)\n",
    "        keyterms = set(keyterms)\n",
    "        \n",
    "        #save sentences' BOW in list sen_BOW\n",
    "        sen_words = []\n",
    "        for sent in sentences:\n",
    "            sent = transform_text(sent)\n",
    "            #Docfrequency\n",
    "            sen_words.append(sent)\n",
    "            \n",
    "            #SentenceBOW\n",
    "            bow_list.append(get_BOW(sent))\n",
    "            \n",
    "            #SentenceLength\n",
    "            sent_len.append(len(sent))\n",
    "            \n",
    "            #TotalLength\n",
    "            total_len += len(sent)\n",
    "            \n",
    "        \n",
    "        #calculate doc frequency    \n",
    "        for term in keyterms:\n",
    "            for i,bow in enumerate(sen_words):\n",
    "                if term in bow:\n",
    "                    article_dic[term].append(i)\n",
    "                    \n",
    "        #Docfrequency\n",
    "        question_list.append(article_dic)\n",
    "        #SentenceBOW\n",
    "        total_sentence_bow.append(bow_list)\n",
    "        #QuestionBOW\n",
    "        total_question_bow.append(que_list)\n",
    "        #SentenceLength\n",
    "        sent_lengthes.append(sent_len)\n",
    "        #AverageLength\n",
    "        avg_lengthes.append(float(total_len)/len(sentences))\n",
    "        #\n",
    "        answer_id.append(right_answer)\n",
    "        \n",
    "    return question_list, total_sentence_bow, total_question_bow, sent_lengthes, avg_lengthes, answer_id\n",
    "\n",
    "train,test,dev = input_data()\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "punctuations = [',','\\'\\'','?','\\'','.','%','(',')',';','``']\n",
    "question_list, total_sentence_bow, total_question_bow, sent_lengthes, avg_lengthes, answer_id_list= get_Docfrequency_SentenceBOW(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def BM25(articles_index,k1,k2,b):\n",
    "    total_queries = len(total_question_bow[articles_index])\n",
    "    count = 0\n",
    "    correct_id = []\n",
    "    for index in range(len(total_question_bow[articles_index])):\n",
    "        answer_id = answer_id_list[articles_index][index]\n",
    "        guess_id = find_max_score_sentence(articles_index,index,k1,k2,b)\n",
    "        if answer_id == guess_id:\n",
    "            correct_id.append(index)\n",
    "            count += 1   \n",
    "    accurancy = float(count)/total_queries  \n",
    "    return correct_id,accurancy\n",
    "\n",
    "def find_max_score_sentence(articles_index,index,k1,k2,b):\n",
    "\n",
    "    query_dict = total_question_bow[articles_index][index]\n",
    "    max_score = 0\n",
    "    guess_sentence = 0\n",
    "    for index in range(len(total_sentence_bow[articles_index])):     \n",
    "        score = 0  \n",
    "        sentence_dict = total_sentence_bow[articles_index][index]\n",
    "        for word in query_dict:\n",
    "            document_fre_list = question_list[articles_index].get(word,None)\n",
    "            \n",
    "            \n",
    "            N = len(total_sentence_bow[articles_index])\n",
    "            n_qi = 0\n",
    "            if document_fre_list != None:\n",
    "                n_qi = len(document_fre_list)\n",
    "            else:\n",
    "                n_qi = 0\n",
    "            fi = sentence_dict.get(word,0)\n",
    "            qfi = query_dict.get(word,0)\n",
    "            dl = sent_lengthes[articles_index][index]\n",
    "            avgdl = avg_lengthes[articles_index]\n",
    "            \n",
    "            K = k1*(1-b+b*(float(dl)/avgdl)) \n",
    "            W = math.log((N-n_qi+0.5)/(n_qi+0.5))\n",
    "            R = (fi*(k1+1))/(fi+K)*qfi*(k2+1)/(qfi+k2)\n",
    "            score += W*R\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            guess_sentence = index\n",
    "    return guess_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of BM25 with k1: 1.1  k2: 0  b: 0.18\n",
      "0.683639289744\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0.0\n",
    "\n",
    "k1_list = [1.1]\n",
    "k2_list = [0]\n",
    "b_list = [0.18]\n",
    "\n",
    "correct = []\n",
    "\n",
    "test_length = len(train)\n",
    "for k1 in k1_list:\n",
    "    for k2 in k2_list:\n",
    "        for b in b_list:\n",
    "            accuracy = 0.0\n",
    "            for i in range(0,test_length):\n",
    "                correct_id,i_accuracy = BM25(i,k1,k2,b)\n",
    "                accuracy += i_accuracy\n",
    "                correct.append(correct_id)\n",
    "            average_accuracy = accuracy/test_length\n",
    "            print \"Accuracy of BM25 with k1:\",k1,\" k2:\",k2,\" b:\",b\n",
    "            print average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accuracy of BM25 with k1: 1.1  k2: 0  b: 0.18\n",
    "# 0.683639289744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "ssss\n",
      "[u'Ccording', u' ', u'to', u' ', u'the', u' ', u'Federal', u' ', u'Bureau', u' ', u'of', u' ', u\"Investigation's\", u' ', u'Uniform', u' ', u'Crime', u' ', u'Reports', u', ', u'in', u' ', u'2010', u' ', u'the', u' ', u'Raleigh', u' ', u'Police', u' ', u'Department', u' ', u'and', u' ', u'other', u' ', u'agencies', u' ', u'in', u' ', u'the', u' ', u'city', u' ', u'reported', u' ', u'1,740', u' ', u'incidents', u' ', u'of', u' ', u'violent', u' ', u'crime', u' ', u'and', u' ', u'12,995', u' ', u'incidents', u' ', u'of', u' ', u'property', u' ', u'crime', u' ', u'\\u2013', u' ', u'far', u' ', u'below', u' ', u'both', u' ', u'the', u' ', u'national', u' ', u'average', u' ', u'and', u' ', u'the', u' ', u'North', u' ', u'Carolina', u' ', u'average.']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(r'[0-9]+s') \n",
    "match = pattern.search('mid-1950s') \n",
    "\n",
    "text = u'Ccording to the Federal Bureau of Investigation\\'s Uniform Crime Reports, in 2010 the Raleigh Police Department and other agencies in the city reported 1,740 incidents of violent crime and 12,995 incidents of property crime – far below both the national average and the North Carolina average.'\n",
    "x = re.split(r'(\"|\\?|;|\\\\u2013|, |\\(|\\s|\\))\\s*', text)\n",
    "\n",
    "print text[0].isupper()\n",
    "\n",
    "for word in x:\n",
    "    if word == u'\\u2013':\n",
    "        print 'ssss'\n",
    "print x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import re\n",
    "import sys\n",
    "\n",
    "location_list = ['country','county']\n",
    "number_list = ['msa','far','much','year','era','ration','years','time','many','population','district','large','percent','average','day','decade','big','long']\n",
    "time_list = ['january','february','march','april','may','june','july','august','september','october','november','december','hours','minutes','miles','inches','foot','years','seasons']\n",
    "name_list = ['university','name','center','president','denominations','denomination','film','broadcaster','pitcher','commentator']\n",
    "\n",
    "def token(text):\n",
    "    text = text[:-1]\n",
    "    result = []\n",
    "    combo = u''\n",
    "    flag_upper = False\n",
    "    x = re.split(r'(\"|\\?|;|, |\\(|\\s|\\))\\s*', text)\n",
    "    #print x\n",
    "    for i,word in enumerate(x):\n",
    "        if word != u'' and word != u' ':\n",
    "            if word == u', ':\n",
    "                word = u','\n",
    "            if word == u'\\u2013':\n",
    "                word = u'-'\n",
    "            if word.lower() in ['in','on','after','at']:\n",
    "                word = word.lower()\n",
    "            if word[0].isalpha():\n",
    "                if flag_upper == False and word[0].isupper():\n",
    "                    combo = word\n",
    "                    flag_upper = True\n",
    "                elif flag_upper == False and word[0].islower():\n",
    "                    result.append(word)\n",
    "                elif flag_upper == True and word[0].isupper():\n",
    "                    combo = combo + u' ' + word\n",
    "                elif flag_upper == True and word[0].islower():\n",
    "                    result.append(combo)\n",
    "                    combo = u''\n",
    "                    result.append(word)\n",
    "                    flag_upper = False\n",
    "            else:\n",
    "                if flag_upper == True:\n",
    "                    result.append(combo)\n",
    "                    combo = u''\n",
    "                    result.append(word)\n",
    "                    flag_upper = False\n",
    "                else:\n",
    "                    result.append(word)\n",
    "    if combo != u'':\n",
    "        result.append(combo) \n",
    "    return result\n",
    "def token_question(text):\n",
    "    text = text[:-1]\n",
    "    result = []\n",
    "    combo = u''\n",
    "    flag_upper = False\n",
    "    x = re.split(r'(\"|\\?|;|, |\\(|\\s|\\))\\s*', text)\n",
    "    for i,word in enumerate(x):\n",
    "        if i == 0:\n",
    "            result.append(word)\n",
    "        else:\n",
    "            if word == u', ':\n",
    "                word = u','\n",
    "            if word != u'' and word != u' ':\n",
    "                if word[0].isalpha():\n",
    "                    if flag_upper == False and word[0].isupper():\n",
    "                        combo = word\n",
    "                        flag_upper = True\n",
    "                    elif flag_upper == False and word[0].islower():\n",
    "                        result.append(word)\n",
    "                    elif flag_upper == True and word[0].isupper():\n",
    "                        combo = combo + u' ' + word\n",
    "                    elif flag_upper == True and word[0].islower():\n",
    "                        result.append(combo)\n",
    "                        combo = u''\n",
    "                        result.append(word)\n",
    "                        flag_upper = False\n",
    "                else:\n",
    "                    if flag_upper == True:\n",
    "                        result.append(combo)\n",
    "                        combo = u''\n",
    "                        result.append(word)\n",
    "                        flag_upper = False\n",
    "                    else:\n",
    "                        result.append(word)\n",
    "    if combo != u'':\n",
    "        result.append(combo) \n",
    "    return result\n",
    "def transfer_pos_question(pos):\n",
    "    new_pos = []\n",
    "    for (word,wtype) in pos:\n",
    "        if word.lower() == 'what' or word.lower() == 'what\\'s':\n",
    "            new_pos.append((word,'WHAT'))\n",
    "        elif word.lower() == 'do' or word.lower() == 'does' or word.lower() == 'did':\n",
    "            new_pos.append((word,'DO'))\n",
    "        elif word.lower() in number_list:\n",
    "            new_pos.append((word,'TIME'))\n",
    "        elif word.lower() == 'is' or word.lower() == 'was' or word.lower() == 'are' or word.lower() == 'were' or word.lower() == 'be':\n",
    "            new_pos.append((word,'BE'))\n",
    "        elif word.lower() in location_list:\n",
    "            new_pos.append((word,'LOC'))\n",
    "        elif word.lower() == 'when':\n",
    "            new_pos.append((word,'WHEN'))\n",
    "        elif word.lower() == 'where':\n",
    "            new_pos.append((word,'WHERE'))\n",
    "        elif word.lower() == 'can':\n",
    "            new_pos.append((word,'CAN'))\n",
    "        elif word.lower() == 'how':\n",
    "            new_pos.append((word,'HOW'))\n",
    "        elif word.lower() == 'who' or word.lower() == 'whom' or word.lower() == 'whose'  or word.lower() == 'whos':\n",
    "            new_pos.append((word,'WHO'))\n",
    "        elif word.lower() == 'which':\n",
    "            new_pos.append((word,'WHICH'))\n",
    "        elif word.lower() in name_list:\n",
    "            new_pos.append((word,'NAME'))\n",
    "        elif word.lower() == 'define':\n",
    "            new_pos.append((word,'DEFINE'))\n",
    "        elif word.lower() == 'should':\n",
    "            new_pos.append((word,'SHOULD'))\n",
    "        elif word.lower() == 'why' or word.lower() == 'wy':\n",
    "            new_pos.append((word,'WHY'))\n",
    "        else:\n",
    "            new_pos.append((word,wtype))\n",
    "    return new_pos\n",
    "def transfer_pos_sentence(pos):\n",
    "    new_pos = []\n",
    "    \n",
    "    for (word,wtype) in pos:\n",
    "        if word.lower() == 'and' or word.lower() == 'or':\n",
    "            new_pos.append((word,'POSICC'))\n",
    "        elif word.lower() == 'but' or word.lower() == 'without':\n",
    "            new_pos.append((word,'NEGCC'))\n",
    "        elif word.lower() == 'with':\n",
    "            new_pos.append((word,'WITH'))\n",
    "        elif word.lower() == 'a' or word.lower() == 'an':\n",
    "            new_pos.append((word,'A'))\n",
    "        elif word == '\"':\n",
    "            new_pos.append((word,'\"'))\n",
    "        elif word.lower() in time_list or '$' in word:\n",
    "            new_pos.append((word,'TIME'))\n",
    "        elif word == 'around':\n",
    "            new_pos.append((word,'AROUND'))\n",
    "        elif word == 'mm':\n",
    "            new_pos.append((word,'CD'))   \n",
    "        elif word == 'by':\n",
    "            new_pos.append((word,'BY'))\n",
    "        elif word[0].isupper():\n",
    "            new_pos.append((word,'UPPER'))\n",
    "        else:\n",
    "            new_pos.append((word,wtype))\n",
    "    return new_pos\n",
    "def get_continuous_chunks(text,texttype):\n",
    "    #text = text.encode('ascii','replace')\n",
    "    t = copy.deepcopy(text)\n",
    "    #print token(t)\n",
    "    if texttype==0:\n",
    "        pos = pos_tag(token(t))\n",
    "        pos = transfer_pos_sentence(pos)\n",
    "        grammar = r\"\"\"\n",
    "                    BASICN:\n",
    "                        {<UPPER>}\n",
    "                        {<NN.*>}\n",
    "                    J:\n",
    "                        {<JJ.*><VBN>}\n",
    "                        {<JJ.*><POSICC><JJ.*>}   \n",
    "                        {<JJ.*>+}\n",
    "                        {<NN.*><POS>}\n",
    "                    N:\n",
    "                        {<DT>?<BASICN>?<J>?<BASICN>+}\n",
    "                        <\\\">{<A>?<J>?<BASICN>+}<\\\">\n",
    "                    COMBON:\n",
    "                        {(<N><,>)*<N><,>?<POSICC><N>}\n",
    "                    NWC:\n",
    "                        {<N><WITH><COMBON>}\n",
    "                    \"\"\"\n",
    "    elif texttype==1:\n",
    "        pos = pos_tag(token_question(t))\n",
    "        pos = transfer_pos_question(pos)\n",
    "        #print pos\n",
    "        grammar = r\"\"\"\n",
    "                    WHAT: \n",
    "                        {<WHAT>}\n",
    "                        {<WHICH>}\n",
    "                        {<DEFINE>}\n",
    "                    WHO:\n",
    "                        {<WHO>}\n",
    "                        {<WHAT><BE>?<DT>?<JJ|RB>*<NAME>}\n",
    "                        {<WHAT><JJ|RB>*<NN>+<NAME>}\n",
    "                    WHEN:\n",
    "                        {<WHICH><TIME>}\n",
    "                        {<HOW><TIME>}\n",
    "                        {<WHAT><BE>?<DT>?<JJ>?<NN>*<JJ>?<TIME>}\n",
    "                        {<WHEN>}\n",
    "                    WHERE:\n",
    "                        {<WHERE>}\n",
    "                        {<WHAT><LOC>}\n",
    "                    HOW:\n",
    "                        {<CAN>}\n",
    "                        {<DO>}\n",
    "                        {<SHOULD>}\n",
    "                        {<WHY>}\n",
    "                        {<HOW>}\n",
    "                    \n",
    "                    \"\"\"\n",
    "    elif texttype==2:\n",
    "        #when\n",
    "        pos = pos_tag(token(t))\n",
    "        pos = transfer_pos_sentence(pos)\n",
    "        #print pos\n",
    "        grammar = r\"\"\"\n",
    "                    C:\n",
    "                        {<TIME><CD>?<,><CD>}\n",
    "                        {<TIME><CD>?<,><TIME>}\n",
    "                        {<CD><TIME>}\n",
    "                        {<CD><\\.><CD>}\n",
    "                        {<AROUND><CD>+}\n",
    "                        {<RB><TIME>+}   \n",
    "                        {<JJ><TIME>+}\n",
    "                        {<AROUND><TIME>+}\n",
    "                        {<TIME>+<CD>+}\n",
    "                        {<DT>?<TIME>+}\n",
    "                        {<CD>+}      \n",
    "                    COMBOC:\n",
    "                        {<C><POSICC><C>}\n",
    "                    \"\"\"\n",
    "    elif texttype==3:\n",
    "        #when\n",
    "        pos = pos_tag(token(t))\n",
    "        pos = transfer_pos_sentence(pos)\n",
    "        #print pos\n",
    "        grammar = r\"\"\"\n",
    "                    H:\n",
    "                        {<RB><JJ><IN>?}\n",
    "                        {<IN>?<JJ.*>*<NN.*>+}\n",
    "                    HH:\n",
    "                        {<H>+}\n",
    "                    SEN:\n",
    "                        {<.*>+}\n",
    "                        }<VBD|BY|,|WITH|NEGCC|TO>+{\n",
    "                    \"\"\"\n",
    "    elif texttype==4:\n",
    "        #who\n",
    "        pos = pos_tag(token(t))\n",
    "        pos = transfer_pos_sentence(pos)\n",
    "        #print pos\n",
    "        grammar = r\"\"\"\n",
    "                    UP:\n",
    "                        {<DT>?<UPPER>}\n",
    "                    SEN:\n",
    "                        {<.*>+}\n",
    "                        }<VBD|BY|,|WITH>+{\n",
    "                    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar) \n",
    "    result = []\n",
    "    poss = copy.deepcopy(pos)\n",
    "    tree = cp.parse(pos)\n",
    "    #record the position of pos\n",
    "    flag = 0\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() != 'S':\n",
    "            phrase = u''\n",
    "            for word,pos in subtree.leaves():\n",
    "                if word == ',':\n",
    "                    phrase = phrase + word\n",
    "                else:\n",
    "                    phrase = phrase + u' '\n",
    "                    phrase = phrase + word\n",
    "            result.append((subtree.label(),phrase[1:]))         \n",
    "    return poss,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "def input_NER():\n",
    "    stanford_dir = os.path.join('stanford-ner-2016-10-31')\n",
    "    jarfile = os.path.join(stanford_dir,'stanford-ner.jar')\n",
    "    modelfile = os.path.join(stanford_dir,'classifiers/english.muc.7class.distsim.crf.ser.gz')\n",
    "    return modelfile,jarfile\n",
    "model,jar = input_NER()\n",
    "st = StanfordNERTagger(model,jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_rule_1(entity,query):\n",
    "    #lower scores for content words also appear in the query\n",
    "    count = 0\n",
    "    length = len(entity)\n",
    "    for word in entity:\n",
    "        word = lemmatize(word.lower())\n",
    "        if word not in stopwords:\n",
    "            if word in query:\n",
    "                count += 1\n",
    "    score = 1 - float(count)/length\n",
    "    return score\n",
    "\n",
    "def rank_rule_2(ner_entity,query):\n",
    "    #answers which match the question type should be ranked higher\n",
    "    question_word = get_question_word(query)\n",
    "    question_type = get_question_type(question_word,query)\n",
    "    if ner_entity == question_type:\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_NER_entity(ner_sentence,type):\n",
    "    result = []\n",
    "    temp = ''\n",
    "    for index in range(len(ner_sentence)):\n",
    "        if ner_sentence[index][1] == type:\n",
    "            if temp != '':\n",
    "                temp += ' '\n",
    "            temp += ner_sentence[index][0]\n",
    "            if index + 1 < len(ner_sentence):\n",
    "                if ner_sentence[index+1][1] != type:\n",
    "                    result.append(temp)\n",
    "                    temp = ''\n",
    "            else:\n",
    "                result.append(temp)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_answer_list(flag,wresult,answer_list):\n",
    "    #if flag == True:\n",
    "    for t,w in wresult:\n",
    "        if t != u'J':\n",
    "            answer_list.append(w)\n",
    "    return answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.728184553661\n"
     ]
    }
   ],
   "source": [
    "total_count = 0.0\n",
    "total_question = 0.0\n",
    "#for index,article in enumerate(train[:1]):\n",
    "for index in range(10):\n",
    "    article = train[index]\n",
    "    qas = article['qa']\n",
    "    sentences = article['sentences']\n",
    "    token_sentences = copy.deepcopy(sentences)\n",
    "    for i in range(len(sentences)):\n",
    "        token_sentences[i] = nltk.word_tokenize(token_sentences[i])\n",
    "    ner_sentences = st.tag_sents(token_sentences)\n",
    "    count = 0\n",
    "    for i in correct[index]:\n",
    "        answer_list = []\n",
    "        qa = qas[i]\n",
    "        #print '---------------'\n",
    "        flag = False\n",
    "        text_question = qa['question']\n",
    "        answer = qa['answer']\n",
    "        answer_sentence = sentences[qa['answer_sentence']]\n",
    "        pos,result = get_continuous_chunks(text_question,1)\n",
    "        #print pos\n",
    "        if result != []:\n",
    "            wtype,word = result[0]\n",
    "            total_question += 1\n",
    "            if wtype == 'WHAT':          \n",
    "                wpos,wresult = get_continuous_chunks(answer_sentence,0)\n",
    "                for wwtype,wword in wresult:\n",
    "                    if qa['answer'] == wword:\n",
    "                        flag = True\n",
    "                        total_count += 1\n",
    "                answer_list = get_answer_list(flag,wresult,[])\n",
    "\n",
    "            elif wtype == 'WHEN':\n",
    "                wpos,wresult = get_continuous_chunks(answer_sentence,2)\n",
    "                for wwtype,wword in wresult:\n",
    "                    if qa['answer'] == wword:\n",
    "                        flag = True\n",
    "                        total_count += 1\n",
    "                answer_list = get_answer_list(flag,wresult,[])\n",
    "\n",
    "            elif wtype == 'HOW':               \n",
    "                wpos,wresult = get_continuous_chunks(answer_sentence,3)           \n",
    "                for wwtype,wword in wresult:\n",
    "                    if qa['answer'] == wword:\n",
    "                        flag = True\n",
    "                        total_count += 1\n",
    "                answer_list = get_answer_list(flag,wresult,[])\n",
    "            elif wtype == 'WHO':\n",
    "                ner_answer_sentence = ner_sentences[qa['answer_sentence']]\n",
    "                answer_list = get_NER_entity(ner_answer_sentence,'PERSON')\n",
    "                if answer_list == []:\n",
    "                    #or do some other things\n",
    "                    answer_list = get_NER_entity(ner_answer_sentence,'ORGANIZATION')\n",
    "                if answer_list == []:\n",
    "                    wpos,wresult = get_continuous_chunks(answer_sentence,4)\n",
    "                    answer_list = get_answer_list(flag,wresult,[])\n",
    "\n",
    "                if qa['answer'] in answer_list:\n",
    "                    flag = True\n",
    "                    total_count += 1\n",
    "\n",
    "            elif wtype == 'WHERE':\n",
    "                ner_answer_sentence = ner_sentences[qa['answer_sentence']]\n",
    "                answer_list = get_NER_entity(ner_answer_sentence,'LOCATION')\n",
    "                if answer_list == []:\n",
    "                    wpos,wresult = get_continuous_chunks(answer_sentence,4)\n",
    "                    answer_list = get_answer_list(flag,wresult,[])\n",
    "                if qa['answer'] in answer_list:\n",
    "                    flag = True\n",
    "                    total_count += 1\n",
    "    #             print '^'*100\n",
    "    #             print 'list: ',answer_list\n",
    "    #             print 'A: ',qa['answer']\n",
    "            '''if flag == False:\n",
    "                print '^'*100\n",
    "                print text_question\n",
    "                #print pos,result\n",
    "                print wtype\n",
    "                print answer_sentence\n",
    "                print transfer_pos_sentence(pos_tag(token(answer_sentence)))\n",
    "                print answer\n",
    "                print answer_list'''\n",
    "        else:\n",
    "            wpos,wresult = get_continuous_chunks(answer_sentence,0)\n",
    "            for wwtype,wword in wresult:\n",
    "                if qa['answer'] == wword:\n",
    "                    flag = True\n",
    "                    total_count += 1\n",
    "            answer_list = get_answer_list(flag,wresult,[])\n",
    "            #print '^'*100\n",
    "            #print text_question\n",
    "            #print answer\n",
    "            #print pos\n",
    "        '''if answer_list != []:\n",
    "            query = copy.deepcopy(text_question)\n",
    "            query = nltk.word_tokenize(query)\n",
    "            for index in range(len(query)):\n",
    "                query[index] = lemmatize(query[index].lower())\n",
    "            scores_1 = []\n",
    "            for entity in answer_list:\n",
    "                entity = nltk.word_tokenize(entity) \n",
    "                #score = rank_rule_1(entity,query)\n",
    "                #scores_1.append(score)\n",
    "            print '^'*100\n",
    "            print 'Q: ',text_question\n",
    "            print 'A: ',qa['answer']\n",
    "            print answer_list\n",
    "            print answer_sentence\n",
    "            print 'socres: ',scores_1'''\n",
    "print total_count/total_question   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
