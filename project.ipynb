{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get document frequency for terms in the same article/instance; and get each sentence's BOW; and get each question's BOW;\n",
    "#get each sentence's length; get each article's average length of sentences\n",
    "import csv\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import nltk\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tag import StanfordNERTagger\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "def input_data():\n",
    "    base_path = os.path.join('data/')\n",
    "    train_file = base_path + 'QA_train.json'\n",
    "    train_data = json.load(open(train_file))\n",
    "    test_file = base_path + 'QA_test.json'\n",
    "    test_data = json.load(open(test_file))\n",
    "    dev_file = base_path + 'QA_dev.json'\n",
    "    dev_data = json.load(open(dev_file))\n",
    "\n",
    "    return train_data,test_data,dev_data\n",
    "def transform_text(text):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = [lemmatize(word.lower()) for word in text]\n",
    "    result = []\n",
    "    for word in text:\n",
    "        if word not in stopwords and word not in punctuations:\n",
    "            result.append(word)\n",
    "    return result\n",
    "def get_Docfrequency_SentenceBOW(dataset):\n",
    "    #save dics, each dictionary contains document frequencies for all terms in the same article\n",
    "    question_list = []\n",
    "    #save lists, each list represent an article, saving sentences' bow\n",
    "    total_sentence_bow = []\n",
    "    #save lists, each list represent an article, saving questions' bow\n",
    "    total_question_bow = []\n",
    "    #save lists, each list represent all sentences' lengthes.\n",
    "    sent_lengthes = []\n",
    "    #save a list, each item represents the average length of sentences\n",
    "    avg_lengthes = []\n",
    "    #\n",
    "    answer_id = []\n",
    "    \n",
    "    for article in dataset:\n",
    "        #Docfrequency\n",
    "        article_dic = defaultdict(list)\n",
    "        keyterms = [] #save all distinct terms in questions\n",
    "        \n",
    "        #SentenceBOW\n",
    "        bow_list = []\n",
    "        \n",
    "        #QuestionBOW\n",
    "        que_list = []\n",
    "        \n",
    "        #SentenceLength\n",
    "        sent_len = []\n",
    "        \n",
    "        #TotalLength\n",
    "        total_len = 0\n",
    "        \n",
    "        #RightAnser\n",
    "        right_answer = []\n",
    "        \n",
    "        qas = article['qa']\n",
    "        sentences = article['sentences']\n",
    "        for qa in qas:\n",
    "            question = qa['question']\n",
    "            newquestion = transform_text(question)\n",
    "            #QuestionBOW\n",
    "            que_list.append(get_BOW(newquestion))\n",
    "            answer = qa['answer_sentence']\n",
    "            right_answer.append(answer)\n",
    "            \n",
    "            keyterms.extend(newquestion)\n",
    "        keyterms = set(keyterms)\n",
    "        \n",
    "        #save sentences' BOW in list sen_BOW\n",
    "        sen_words = []\n",
    "        for sent in sentences:\n",
    "            sent = transform_text(sent)\n",
    "            #Docfrequency\n",
    "            sen_words.append(sent)\n",
    "            \n",
    "            #SentenceBOW\n",
    "            bow_list.append(get_BOW(sent))\n",
    "            \n",
    "            #SentenceLength\n",
    "            sent_len.append(len(sent))\n",
    "            \n",
    "            #TotalLength\n",
    "            total_len += len(sent)\n",
    "            \n",
    "        \n",
    "        #calculate doc frequency    \n",
    "        for term in keyterms:\n",
    "            for i,bow in enumerate(sen_words):\n",
    "                if term in bow:\n",
    "                    article_dic[term].append(i)\n",
    "                    \n",
    "        #Docfrequency\n",
    "        question_list.append(article_dic)\n",
    "        #SentenceBOW\n",
    "        total_sentence_bow.append(bow_list)\n",
    "        #QuestionBOW\n",
    "        total_question_bow.append(que_list)\n",
    "        #SentenceLength\n",
    "        sent_lengthes.append(sent_len)\n",
    "        #AverageLength\n",
    "        avg_lengthes.append(float(total_len)/len(sentences))\n",
    "        #\n",
    "        answer_id.append(right_answer)\n",
    "        \n",
    "    return question_list, total_sentence_bow, total_question_bow, sent_lengthes, avg_lengthes, answer_id\n",
    "\n",
    "train,test,dev = input_data()\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "punctuations = [',','\\'\\'','?','\\'','.','%','(',')',';','``']\n",
    "question_list, total_sentence_bow, total_question_bow, sent_lengthes, avg_lengthes, answer_id_list= get_Docfrequency_SentenceBOW(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def BM25(articles_index,k1,k2,b):\n",
    "    total_queries = len(total_question_bow[articles_index])\n",
    "    count = 0\n",
    "    correct_id = []\n",
    "    for index in range(len(total_question_bow[articles_index])):\n",
    "        answer_id = answer_id_list[articles_index][index]\n",
    "        guess_id = find_max_score_sentence(articles_index,index,k1,k2,b)\n",
    "        if answer_id == guess_id:\n",
    "            correct_id.append(index)\n",
    "            count += 1   \n",
    "    accurancy = float(count)/total_queries  \n",
    "    return correct_id,accurancy\n",
    "\n",
    "def find_max_score_sentence(articles_index,index,k1,k2,b):\n",
    "\n",
    "    query_dict = total_question_bow[articles_index][index]\n",
    "    max_score = 0\n",
    "    guess_sentence = 0\n",
    "    for index in range(len(total_sentence_bow[articles_index])):     \n",
    "        score = 0  \n",
    "        sentence_dict = total_sentence_bow[articles_index][index]\n",
    "        for word in query_dict:\n",
    "            document_fre_list = question_list[articles_index].get(word,None)\n",
    "            \n",
    "            \n",
    "            N = len(total_sentence_bow[articles_index])\n",
    "            n_qi = 0\n",
    "            if document_fre_list != None:\n",
    "                n_qi = len(document_fre_list)\n",
    "            else:\n",
    "                n_qi = 0\n",
    "            fi = sentence_dict.get(word,0)\n",
    "            qfi = query_dict.get(word,0)\n",
    "            dl = sent_lengthes[articles_index][index]\n",
    "            avgdl = avg_lengthes[articles_index]\n",
    "            \n",
    "            K = k1*(1-b+b*(float(dl)/avgdl)) \n",
    "            W = math.log((N-n_qi+0.5)/(n_qi+0.5))\n",
    "            R = (fi*(k1+1))/(fi+K)*qfi*(k2+1)/(qfi+k2)\n",
    "            score += W*R\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            guess_sentence = index\n",
    "    return guess_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of BM25 with k1: 1.1  k2: 0  b: 0.18\n",
      "0.683639289744\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0.0\n",
    "\n",
    "k1_list = [1.1]\n",
    "k2_list = [0]\n",
    "b_list = [0.18]\n",
    "\n",
    "correct = []\n",
    "\n",
    "test_length = len(train)\n",
    "for k1 in k1_list:\n",
    "    for k2 in k2_list:\n",
    "        for b in b_list:\n",
    "            accuracy = 0.0\n",
    "            for i in range(0,test_length):\n",
    "                correct_id,i_accuracy = BM25(i,k1,k2,b)\n",
    "                accuracy += i_accuracy\n",
    "                correct.append(correct_id)\n",
    "            average_accuracy = accuracy/test_length\n",
    "            print \"Accuracy of BM25 with k1:\",k1,\" k2:\",k2,\" b:\",b\n",
    "            print average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Accurancy of BM25 with k1: 1.1  k2: 110  b: 0.18\n",
    "#0.683486756477"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(r'[0-9]+s') \n",
    "match = pattern.search('mid-1950s') \n",
    " \n",
    "if match: \n",
    "    print 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import re\n",
    "import sys\n",
    "def token(text):\n",
    "    text = text[:-1]\n",
    "    result = []\n",
    "    combo = u''\n",
    "    flag_upper = False\n",
    "    x = re.split(r'(\"|\\?|;|,|\\(|\\s|\\))\\s*', text)\n",
    "    #print x\n",
    "    for word in x:\n",
    "        if word != u'' and word != u' ':\n",
    "            if word[0].isalpha():\n",
    "                if flag_upper == False and word[0].isupper():\n",
    "                    combo = word\n",
    "                    flag_upper = True\n",
    "                elif flag_upper == False and word[0].islower():\n",
    "                    result.append(word)\n",
    "                elif flag_upper == True and word[0].isupper():\n",
    "                    combo = combo + u' ' + word\n",
    "                elif flag_upper == True and word[0].islower():\n",
    "                    result.append(combo)\n",
    "                    combo = u''\n",
    "                    result.append(word)\n",
    "                    flag_upper = False\n",
    "            else:\n",
    "                if flag_upper == True:\n",
    "                    result.append(combo)\n",
    "                    combo = u''\n",
    "                    result.append(word)\n",
    "                    flag_upper = False\n",
    "                else:\n",
    "                    result.append(word)\n",
    "    if combo != u'':\n",
    "        result.append(combo) \n",
    "    return result\n",
    "def transfer_pos_question(pos):\n",
    "    new_pos = []\n",
    "    for (word,wtype) in pos:\n",
    "        if word.lower() == 'what':\n",
    "            new_pos.append((word,'WHAT'))\n",
    "        elif word.lower() == 'do' or word.lower() == 'does' or word.lower() == 'did':\n",
    "            new_pos.append((word,'DO'))\n",
    "        elif word.lower() == 'far' or word.lower() == 'much' or word.lower() == 'year' or word.lower() == 'era' or word.lower() == 'ration' or word.lower() == 'years' or word.lower() == 'time' or word.lower() == 'many':\n",
    "            new_pos.append((word,'TIME'))\n",
    "        elif word.lower() == 'is' or word.lower() == 'was' or word.lower() == 'are' or word.lower() == 'were':\n",
    "            new_pos.append((word,'BE'))\n",
    "        elif word.lower() == 'when':\n",
    "            new_pos.append((word,'WHEN'))\n",
    "        elif word.lower() == 'where':\n",
    "            new_pos.append((word,'WHERE'))\n",
    "        elif word.lower() == 'how':\n",
    "            new_pos.append((word,'HOW'))\n",
    "        elif word.lower() == 'who' or word.lower() == 'whom' or word.lower() == 'whose':\n",
    "            new_pos.append((word,'WHO'))\n",
    "        else:\n",
    "            new_pos.append((word,wtype))\n",
    "    return new_pos\n",
    "def transfer_pos_sentence(pos):\n",
    "    new_pos = []\n",
    "    \n",
    "    for (word,wtype) in pos:\n",
    "        time_pattern = re.compile(r'[0-9]{4}s?')\n",
    "        if word.lower() == 'and' or word.lower() == 'or':\n",
    "            new_pos.append((word,'POSICC'))\n",
    "        elif word.lower() == 'with':\n",
    "            new_pos.append((word,'WITH'))\n",
    "        elif word.lower() == 'a' or word.lower() == 'an':\n",
    "            new_pos.append((word,'A'))\n",
    "        elif word == '\"':\n",
    "            new_pos.append((word,'\"'))\n",
    "        elif word == 'minutes' or word == 'March':\n",
    "            new_pos.append((word,'TIME'))\n",
    "        elif time_pattern.search(word):\n",
    "            new_pos.append((word,'TIME'))\n",
    "        elif word == 'around':\n",
    "            new_pos.append((word,'AROUND'))\n",
    "        elif word == 'mm':\n",
    "            new_pos.append((word,'CD')) \n",
    "        else:\n",
    "            new_pos.append((word,wtype))\n",
    "    return new_pos\n",
    "def get_continuous_chunks(text,texttype):\n",
    "    #text = text.encode('ascii','replace')\n",
    "    t = copy.deepcopy(text)\n",
    "    #print token(t)\n",
    "    pos =  pos_tag(token(t))\n",
    "    if texttype==0:\n",
    "        #WHAT\n",
    "        pos = transfer_pos_sentence(pos)\n",
    "        grammar = r\"\"\"\n",
    "                    J:\n",
    "                        {<JJ.*><VBN>}\n",
    "                        {<JJ.*><POSICC><JJ.*>}   \n",
    "                        {<JJ.*>+}\n",
    "                        {<NN.*><POS>}\n",
    "                    N:\n",
    "                        {<CD>+<NN.*>}\n",
    "                        {<A>?<NN.*>?<J>?<NN.*>+}\n",
    "                        <\\\">{<A>?<J>?<NN.*>+}<\\\">\n",
    "                    COMBON:\n",
    "                        {(<N><,>)*<N><,>?<POSICC><N>}\n",
    "                    NWC:\n",
    "                        {<N><WITH><COMBON>}\n",
    "                    \"\"\"\n",
    "    elif texttype==1:\n",
    "        pos = transfer_pos_question(pos)\n",
    "        #print pos\n",
    "        grammar = r\"\"\"\n",
    "                    WHAT: \n",
    "                        {<WHAT>}\n",
    "                    WHEN:\n",
    "                        {<HOW><TIME>}\n",
    "                        {<WHAT><BE>?<DT>?<JJ>?<NN>*<JJ>?<TIME>}\n",
    "                        {<WHEN>}\n",
    "                    WHERE:\n",
    "                        {<WHERE>}\n",
    "                    HOW:\n",
    "                        {<HOW>}\n",
    "                    WHO:\n",
    "                        {<WHO>}\n",
    "                    \"\"\"\n",
    "    elif texttype==2:\n",
    "        #when\n",
    "        pos = transfer_pos_sentence(pos)\n",
    "        #print pos\n",
    "        grammar = r\"\"\"\n",
    "                    C:\n",
    "                        {<CD><TIME>}\n",
    "                        {<CD><\\.><CD>}\n",
    "                        {<RB><CD>+}   \n",
    "                        {<JJ><CD>+}\n",
    "                        {<AROUND><CD>+}\n",
    "                        {<RB><TIME>+}   \n",
    "                        {<JJ><TIME>+}\n",
    "                        {<AROUND><TIME>+}\n",
    "                        {<TIME>+}\n",
    "                        {<CD>+}      \n",
    "                    COMBOC:\n",
    "                        {<C><POSICC><C>}\n",
    "                    \"\"\"\n",
    "    elif texttype==3:\n",
    "        #when\n",
    "        pos = transfer_pos_sentence(pos)\n",
    "        #print pos\n",
    "        grammar = r\"\"\"\n",
    "                    H:\n",
    "                        {<RB><JJ><IN>?}\n",
    "                    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar) \n",
    "    result = []\n",
    "    poss = copy.deepcopy(pos)\n",
    "    tree = cp.parse(pos)\n",
    "    #record the position of pos\n",
    "    flag = 0\n",
    "    for subtree in tree.subtrees():\n",
    "        if texttype==0 and subtree.label() != 'S':\n",
    "            phrase = u''\n",
    "            for word,pos in subtree.leaves():\n",
    "                if word == ',':\n",
    "                    phrase = phrase + word\n",
    "                else:\n",
    "                    phrase = phrase + u' '\n",
    "                    phrase = phrase + word\n",
    "            result.append((subtree.label(),phrase[1:]))\n",
    "            #print subtree.label(),phrase\n",
    "        elif subtree.label() != 'S':\n",
    "            phrase = u' '.join([word for word,pos in subtree.leaves()])\n",
    "            result.append((subtree.label(),phrase))\n",
    "            \n",
    "            \n",
    "    return poss,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "def input_NER():\n",
    "    stanford_dir = os.path.join('stanford-ner-2016-10-31')\n",
    "    jarfile = os.path.join(stanford_dir,'stanford-ner.jar')\n",
    "    modelfile = os.path.join(stanford_dir,'classifiers/english.muc.7class.distsim.crf.ser.gz')\n",
    "    return modelfile,jarfile\n",
    "model,jar = input_NER()\n",
    "st = StanfordNERTagger(model,jar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_count = 0.0\n",
    "total_question = 0.0\n",
    "#for index,article in enumerate(train[:1]):\n",
    "article = train[0]\n",
    "index = 0\n",
    "qas = article['qa']\n",
    "sentences = article['sentences']\n",
    "token_sentences = copy.deepcopy(sentences)\n",
    "for i in range(len(sentences)):\n",
    "    token_sentences[i] = nltk.word_tokenize(token_sentences[i])\n",
    "ner_sentences = st.tag_sents(token_sentences)\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_rule_1(entity,query):\n",
    "    #lower scores for content words also appear in the query\n",
    "    count = 0\n",
    "    length = len(entity)\n",
    "    for word in entity:\n",
    "        word = lemmatize(word.lower())\n",
    "        if word not in stopwords:\n",
    "            if word in query:\n",
    "                count += 1\n",
    "    score = 1 - float(count)/length\n",
    "    return score\n",
    "\n",
    "def rank_rule_2(ner_entity,query):\n",
    "    #answers which match the question type should be ranked higher\n",
    "    question_word = get_question_word(query)\n",
    "    question_type = get_question_type(question_word,query)\n",
    "    if ner_entity == question_type:\n",
    "        score = 1\n",
    "    else:\n",
    "        score = 0\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_answer_list(flag,wresult,answer_list):\n",
    "    if flag == True:\n",
    "        for t,w in wresult:\n",
    "            if t != u'J':\n",
    "                answer_list.append(w)\n",
    "    return answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  Approximately how many phonograph records were sold in 2014?\n",
      "A:  9.2 million\n",
      "[u'9.2 million', u'2014', u'260%', u'2009']\n",
      "The phonograph record has made a niche resurgence in the early 21st century – 9.2 million records were sold in the U.S. in 2014, a 260% increase since 2009.\n",
      "socres:  [1.0, 0.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What is the niche market of phonograph record fans known as?\n",
      "A:  audiophiles\n",
      "[u'records', u'smaller scale', u'disc jockeys', u'DJ', u's', u'artists', u'genres', u'a niche market', u'audiophiles']\n",
      "From the 1990s to the 2010s, records continued to be manufactured and sold on a much smaller scale, and were especially used by disc jockeys (DJ)s, released by artists in some genres, and listened to by a niche market of audiophiles.\n",
      "socres:  [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.33333333333333337, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What year were the earliest known recordings of sound?\n",
      "A:  1857\n",
      "[u'1857']\n",
      "Along with a tuning fork tone and unintelligible snippets recorded as early as 1857, these are the earliest known recordings of sound.\n",
      "socres:  [1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  In what year was phonautograms patented?\n",
      "A:  1857\n",
      "[u'1857']\n",
      "The phonautograph, patented by Léon Scott in 1857, used a vibrating diaphragm and stylus to graphically record sound waves as tracings on sheets of paper, purely for visual analysis and without any intent of playing them back.\n",
      "socres:  [1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  At what era was the recorded sound market introduced?\n",
      "A:  1880s\n",
      "[u'1880s']\n",
      "The wax phonograph cylinder created the recorded sound market at the end of the 1880s and dominated it through the early years of the 20th century.\n",
      "socres:  [1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What is the name of lateral cut disc records?\n",
      "A:  gramophone\n",
      "[u'Lateral-cut disc records', u'United States', u'Emile Berliner', u'system', u'gramophone', u\"Edison's wax cylinder\", u'phonograph', u\"Columbia's\", u'graphophone']\n",
      "Lateral-cut disc records were developed in the United States by Emile Berliner, who named his system the \"gramophone\", distinguishing it from Edison's wax cylinder \"phonograph\" and Columbia's wax cylinder \"graphophone\".\n",
      "socres:  [0.33333333333333337, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What year did the patent for lateral cut discs expire?\n",
      "A:  1919\n",
      "[u'1919']\n",
      "By 1919 the basic patents for the manufacture of lateral-cut disc records had expired, opening the field for countless companies to produce them.\n",
      "socres:  [1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What disc format was the least fragile prior to 1919?\n",
      "A:  Amberol cylinder\n",
      "[u'an attempt', u'disc advantage', u'Edison', u'Amberol cylinder', u'a maximum playing time', u'160 rpm', u'turn', u'Blue Amberol Records', u'a playing surface', u'celluloid', u'a plastic']\n",
      "In an attempt to head off the disc advantage, Edison introduced the Amberol cylinder in 1909, with a maximum playing time of 4½ minutes (at 160 rpm), which in turn were superseded by Blue Amberol Records, which had a playing surface made of celluloid, a plastic, which was far less fragile.\n",
      "socres:  [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  How was the frequency response in early recordings?\n",
      "A:  very irregular\n",
      "[u'very irregular', u'instantly recognizable']\n",
      "Sensitivity and frequency range were poor, and frequency response was very irregular, giving acoustic recordings an instantly recognizable tonal quality.\n",
      "socres:  [1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What was an early took used to amplify sounds?\n",
      "A:  vacuum tubes\n",
      "[u'first half', u'engineers', u'Western Electric', u'independent inventors', u'Orlando Marsh', u'technology', u'sound', u'a microphone', u'vacuum tubes', u'amplified signal', u'an electromagnetic recording head']\n",
      "During the first half of the 1920s, engineers at Western Electric, as well as independent inventors such as Orlando Marsh, developed technology for capturing sound with a microphone, amplifying it with vacuum tubes, then using the amplified signal to drive an electromagnetic recording head.\n",
      "socres:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.5, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What options were available that effected the cost of the Victor Orthophonic Victorla?\n",
      "A:  cabinetry\n",
      "[u'acoustical Orthophonics', u'price', u'US$95', u'US$300', u'cabinetry', u'comparison', u'cheapest Electrola cost US$650', u'price', u'a new Ford automobile', u'an era', u'clerical jobs', u'$20', u'a week']\n",
      "The acoustical Orthophonics ranged in price from US$95 to US$300, depending on cabinetry; by comparison, the cheapest Electrola cost US$650, the price of a new Ford automobile in an era when clerical jobs paid about $20 a week.\n",
      "socres:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.8333333333333334, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What was the standard material for discs around 1895?\n",
      "A:  a shellac-based compound\n",
      "[u'a shellac-based compound']\n",
      "Around 1895, a shellac-based compound was introduced and became standard.\n",
      "socres:  [1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What material give the vinyl records their known black color?\n",
      "A:  carbon\n",
      "[u'Exact formulas', u'compound', u'manufacturer', u'course', u'time', u'one-third shellac', u'two-thirds mineral filler', u'rock', u'limestone', u'an admixture', u'cotton fibers', u'tensile strength', u'carbon', u'color', u'dirty', u'gray or brown color', u'gray', u'brown color', u'most record companies', u'small amount', u'a lubricant', u'mold release', u'manufacture']\n",
      "Exact formulas for this compound varied by manufacturer and over the course of time, but it was typically composed of about one-third shellac and about two-thirds mineral filler, which meant finely pulverized rock, usually slate and limestone, with an admixture of cotton fibers to add tensile strength, carbon black for color (without this, it tended to be a \"dirty\" gray or brown color that most record companies considered unattractive), and a very small amount of a lubricant to facilitate mold release during manufacture.\n",
      "socres:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.75, 1.0, 0.5, 0.6666666666666667, 1.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What was a major downfall of the success of Durium records?\n",
      "A:  Great Depression\n",
      "[u'victim', u'Great Depression and production', u'Great Depression', u'production', u'US']\n",
      "Although inexpensive and commercially successful at first, they soon fell victim to the Great Depression and production in the US ended in 1932.\n",
      "socres:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What was a common problem found in early flexible records?\n",
      "A:  surface noise\n",
      "[u'UK', u'Nicole records', u'celluloid or a similar substance', u'celluloid', u'a similar substance', u'a cardboard core disc', u'a few years', u'high level', u'surface noise']\n",
      "In the UK, Nicole records, made of celluloid or a similar substance coated onto a cardboard core disc, were produced for a few years beginning in 1904, but they suffered from an exceptionally high level of surface noise.\n",
      "socres:  [1.0, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What was the thin translucent plastic most successfully used to make discs called?\n",
      "A:  Durium\n",
      "[u'US', u'Hit', u'Week records', u'a patented translucent plastic', u'Durium', u'a heavy brown paper base']\n",
      "In the US, Hit of the Week records, made of a patented translucent plastic called Durium coated on a heavy brown paper base, were introduced in early 1930.\n",
      "socres:  [1.0, 1.0, 1.0, 0.5, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What material was used by RCA Victor for special purpose records?\n",
      "A:  vinyl-based Victrolac compound\n",
      "[u'RCA Victor', u'vinyl-based Victrolac compound', u'a material', u'unusual-format and special-purpose records']\n",
      "In 1931, RCA Victor introduced their vinyl-based Victrolac compound as a material for some unusual-format and special-purpose records.\n",
      "socres:  [0.0, 1.0, 0.5, 0.75]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What year was the Victrolac compound released?\n",
      "A:  1931\n",
      "[u'1931']\n",
      "In 1931, RCA Victor introduced their vinyl-based Victrolac compound as a material for some unusual-format and special-purpose records.\n",
      "socres:  [1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What were advantages of vinyl in the 1930's?\n",
      "A:  light weight, relative unbreakability and low surface noise\n",
      "[u'end', u\"vinyl's advantages\", u'light weight, relative unbreakability and low surface noise', u'light weight', u'relative unbreakability', u'low surface noise', u'material', u'choice', u'prerecorded radio programming and other critical applications', u'prerecorded radio programming', u'other critical applications']\n",
      "By the end of the 1930s vinyl's advantages of light weight, relative unbreakability and low surface noise had made it the material of choice for prerecorded radio programming and other critical applications.\n",
      "socres:  [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What was a downfall of ordinary 78 rpm vinyls in household house?\n",
      "A:  higher cost\n",
      "[u'78 rpm', u'records', u'higher cost', u'raw material', u'vulnerability', u'heavy pickups', u'mass-produced steel needles', u'home record players', u'general substitution', u'time']\n",
      "When it came to ordinary 78 rpm records, however, the much higher cost of the raw material, as well as its vulnerability to the heavy pickups and crudely mass-produced steel needles still commonly used in home record players, made its general substitution for shellac impractical at that time.\n",
      "socres:  [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What was the most popular sized disc by 1910?\n",
      "A:  10-inch\n",
      "[u'25.4 cm', u'record', u'popular standard', u'music or other entertainment', u'music', u'other entertainment', u'a side']\n",
      "By 1910 the 10-inch (25.4 cm) record was by far the most popular standard, holding about three minutes of music or other entertainment on a side.\n",
      "socres:  [1.0, 1.0, 0.5, 1.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  How many recordings did Kind Oliver's Creole Jazz band require?\n",
      "A:  13\n",
      "[u'13', u'1923', u'one', u'2:09 and four', u'2:09', u'four', u'2:52\\u20132:59']\n",
      "For example, when King Oliver's Creole Jazz Band, including Louis Armstrong on his first recordings, recorded 13 sides at Gennett Records in Richmond, Indiana, in 1923, one side was 2:09 and four sides were 2:52–2:59.\n",
      "socres:  [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What were early record album covers made of?\n",
      "A:  empty sleeves with a paperboard or leather cover\n",
      "[u'collections', u'empty sleeves with a paperboard or leather cover', u'empty sleeves', u'a paperboard or leather cover', u'a paperboard', u'leather cover', u'a photograph album', u'record', u'customers', u'records', u'term', u'record album', u'covers']\n",
      "By about 1910,[note 1] bound collections of empty sleeves with a paperboard or leather cover, similar to a photograph album, were sold as record albums that customers could use to store their records (the term \"record album\" was printed on some covers).\n",
      "socres:  [1.0, 0.875, 1.0, 0.8, 1.0, 0.5, 0.6666666666666667, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What are reasons for recent releases of 78 rpm speed vinyls?\n",
      "A:  collectable or nostalgia purposes\n",
      "[u'collectable or nostalgia purposes', u'benefit', u'higher-quality audio playback', u'78 rpm', u'speed', u'newer vinyl records', u'lightweight stylus pickups', u'a small number', u'78 rpm', u'records', u'major labels', u'production']\n",
      "For collectable or nostalgia purposes, or for the benefit of higher-quality audio playback provided by the 78 rpm speed with newer vinyl records and their lightweight stylus pickups, a small number of 78 rpm records have been released since the major labels ceased production.\n",
      "socres:  [1.0, 1.0, 1.0, 0.0, 0.0, 0.6666666666666667, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What is a benefit of newer releases of 78 rpm speed vinyls?\n",
      "A:  higher-quality audio playback\n",
      "[u'collectable or nostalgia purposes', u'benefit', u'higher-quality audio playback', u'78 rpm', u'speed', u'newer vinyl records', u'lightweight stylus pickups', u'a small number', u'78 rpm', u'records', u'major labels', u'production']\n",
      "For collectable or nostalgia purposes, or for the benefit of higher-quality audio playback provided by the 78 rpm speed with newer vinyl records and their lightweight stylus pickups, a small number of 78 rpm records have been released since the major labels ceased production.\n",
      "socres:  [1.0, 0.0, 1.0, 0.0, 0.0, 0.33333333333333337, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What materials were 45 rpm records made of?\n",
      "A:  vinyl or polystyrene\n",
      "[u'45 rpm', u'records', u'vinyl or polystyrene', u'vinyl', u'polystyrene']\n",
      "Early 45 rpm records were made from either vinyl or polystyrene.\n",
      "socres:  [0.0, 0.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What was the size of a RCA Victor 45 rpm?\n",
      "A:  7 inches\n",
      "[u\"Columbia's system\", u'February', u'RCA Victor', u'cooperation', u'parent', u'Radio Corporation', u'America', u'45 rpm', u'7 inches', u'diameter', u'a large center hole']\n",
      "Unwilling to accept and license Columbia's system, in February 1949 RCA Victor, in cooperation of its parent, the Radio Corporation of America, released the first 45 rpm single, 7 inches in diameter with a large center hole.\n",
      "socres:  [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What currently utilized item was World Records CLV similar to?\n",
      "A:  Philips Laser Disc\n",
      "[u'action', u'reverse', u'modern compact disc', u'CLV version', u'predecessor', u'Philips Laser Disc']\n",
      "This action is similar (although in reverse) to that on the modern compact disc and the CLV version of its predecessor, the Philips Laser Disc.\n",
      "socres:  [1.0, 1.0, 1.0, 0.5, 1.0, 1.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What gear ration creates 78.26 rpm?\n",
      "A:  46:1\n",
      "[u'3600', u'46:1', u'78.26']\n",
      "This motor ran at 3600 rpm, such that a 46:1 gear ratio would produce 78.26 rpm.\n",
      "socres:  [1.0, 1.0, 0.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  In what year was 78.26 rpm chosen as standard?\n",
      "A:  1925\n",
      "[u'1925', u'78.26']\n",
      "In 1925, 78.26 rpm was chosen as the standard because of the introduction of the electrically powered synchronous turntable motor.\n",
      "socres:  [1.0, 0.0]\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "Q:  What was the last country to phase out the 78 rpm?\n",
      "A:  United Kingdom\n",
      "[u'United Kingdom', u'78 rpm', u'United States', u'45 rpm']\n",
      "In the United Kingdom, the 78 rpm single lasted longer than in the United States and the 45 rpm took longer to become popular.\n",
      "socres:  [1.0, 0.0, 1.0, 0.5]\n",
      "0.54752851711\n"
     ]
    }
   ],
   "source": [
    "for i in correct[index]:\n",
    "    answer_list = []\n",
    "    qa = qas[i]\n",
    "    #print '---------------'\n",
    "    flag = False\n",
    "    text_question = qa['question']\n",
    "    answer_sentence = sentences[qa['answer_sentence']]\n",
    "    pos,result = get_continuous_chunks(text_question,1)\n",
    "    #print pos\n",
    "    if result != []:\n",
    "        \n",
    "        wtype,word = result[0]\n",
    "        total_question += 1\n",
    "        \n",
    "        if wtype == 'WHAT':          \n",
    "            wpos,wresult = get_continuous_chunks(answer_sentence,0)\n",
    "            for wwtype,wword in wresult:\n",
    "                if qa['answer'] == wword:\n",
    "                    flag = True\n",
    "                    total_count += 1\n",
    "            answer_list = get_answer_list(flag,wresult,[])\n",
    "            \n",
    "        elif wtype == 'WHEN':\n",
    "            wpos,wresult = get_continuous_chunks(answer_sentence,2)\n",
    "            for wwtype,wword in wresult:\n",
    "                if qa['answer'] == wword:\n",
    "                    flag = True\n",
    "                    total_count += 1\n",
    "            answer_list = get_answer_list(flag,wresult,[])\n",
    "            \n",
    "            '''if flag == False:\n",
    "                print 'WHEN:'\n",
    "                print 'Q: ',text_question\n",
    "                print pos\n",
    "                print 'S: ',answer_sentence\n",
    "                print wpos\n",
    "                print '@@@A: ',qa['answer']\n",
    "                for t,w in wresult:\n",
    "                    if t != u'J':\n",
    "                        answer_list.append(w)\n",
    "                print answer_list'''\n",
    "            \n",
    "        elif wtype == 'HOW':\n",
    "            '''print '---------------'\n",
    "            print wtype\n",
    "            print 'Q: ',text_question\n",
    "            print 'S: ',answer_sentence\n",
    "            print pos_tag(token(answer_sentence))\n",
    "            print '@@@A: ',qa['answer']'''\n",
    "            wpos,wresult = get_continuous_chunks(answer_sentence,3)\n",
    "            for wwtype,wword in wresult:\n",
    "                if qa['answer'] == wword:\n",
    "                    flag = True\n",
    "                    total_count += 1\n",
    "            answer_list = get_answer_list(flag,wresult,[])\n",
    "            \n",
    "#         elif wtype == 'WHO':\n",
    "#             print '---------------'\n",
    "#             print wtype\n",
    "#             print 'Q: ',text_question\n",
    "#             print 'S: ',answer_sentence\n",
    "#             print pos_tag(token(answer_sentence))\n",
    "#             print '@@@A: ',qa['answer']\n",
    "\n",
    "    if answer_list != []:\n",
    "        query = copy.deepcopy(text_question)\n",
    "        query = nltk.word_tokenize(query)\n",
    "        for index in range(len(query)):\n",
    "            query[index] = lemmatize(query[index].lower())\n",
    "        scores_1 = []\n",
    "        for entity in answer_list:\n",
    "            entity = nltk.word_tokenize(entity) \n",
    "            score = rank_rule_1(entity,query)\n",
    "            scores_1.append(score)\n",
    "        print '^'*100\n",
    "        print 'Q: ',text_question\n",
    "        print 'A: ',qa['answer']\n",
    "        print answer_list\n",
    "        print answer_sentence\n",
    "        print 'socres: ',scores_1\n",
    "        \n",
    "print total_count/total_question   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = 'There is a dedicated museum in Montreal for Berliner'\n",
    "def input_NER():\n",
    "    stanford_dir = os.path.join('stanford-ner-2016-10-31')\n",
    "    jarfile = os.path.join(stanford_dir,'stanford-ner.jar')\n",
    "    modelfile = os.path.join(stanford_dir,'classifiers/english.muc.7class.distsim.crf.ser.gz')\n",
    "    return modelfile,jarfile\n",
    "model,jar = input_NER()\n",
    "st = StanfordNERTagger(model,jar)\n",
    "#print token(text)\n",
    "print st.tag(token(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
