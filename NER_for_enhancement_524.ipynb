{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get document frequency for terms in the same article/instance; and get each sentence's BOW; and get each question's BOW;\n",
    "#get each sentence's length; get each article's average length of sentences\n",
    "import csv\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import nltk\n",
    "import sys \n",
    "import math\n",
    "import operator\n",
    "import re\n",
    "import string\n",
    "from math import log\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "def input_data():\n",
    "    base_path = os.path.join('data/')\n",
    "    train_file = base_path + 'QA_train.json'\n",
    "    train_data = json.load(open(train_file))\n",
    "    test_file = base_path + 'QA_test.json'\n",
    "    test_data = json.load(open(test_file))\n",
    "    dev_file = base_path + 'QA_dev.json'\n",
    "    dev_data = json.load(open(dev_file))\n",
    "\n",
    "    return train_data,test_data,dev_data\n",
    "def transform_text(text):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = [lemmatize(word.lower()) for word in text]\n",
    "    result = []\n",
    "    for word in text:\n",
    "        if word not in stopwords and word not in punctuations:\n",
    "            result.append(word)\n",
    "    return result\n",
    "def get_Docfrequency_SentenceBOW(dataset):\n",
    "    #save dics, each dictionary contains document frequencies for all terms in the same article\n",
    "    question_list = []\n",
    "    #save lists, each list represent an article, saving sentences' bow\n",
    "    total_sentence_bow = []\n",
    "    #save lists, each list represent an article, saving questions' bow\n",
    "    total_question_bow = []\n",
    "    #save lists, each list represent all sentences' lengthes.\n",
    "    sent_lengthes = []\n",
    "    #save a list, each item represents the average length of sentences\n",
    "    avg_lengthes = []\n",
    "    #\n",
    "    answer_id = []\n",
    "    \n",
    "    for article in dataset:\n",
    "        #Docfrequency\n",
    "        article_dic = defaultdict(list)\n",
    "        keyterms = [] #save all distinct terms in questions\n",
    "        \n",
    "        #SentenceBOW\n",
    "        bow_list = []\n",
    "        \n",
    "        #QuestionBOW\n",
    "        que_list = []\n",
    "        \n",
    "        #SentenceLength\n",
    "        sent_len = []\n",
    "        \n",
    "        #TotalLength\n",
    "        total_len = 0\n",
    "        \n",
    "        #RightAnser\n",
    "        right_answer = []\n",
    "        \n",
    "        qas = article['qa']\n",
    "        sentences = article['sentences']\n",
    "        for qa in qas:\n",
    "            question = qa['question']\n",
    "            newquestion = transform_text(question)\n",
    "            #QuestionBOW\n",
    "            que_list.append(get_BOW(newquestion))\n",
    "            answer = qa['answer_sentence']\n",
    "            right_answer.append(answer)\n",
    "            \n",
    "            keyterms.extend(newquestion)\n",
    "        keyterms = set(keyterms)\n",
    "        \n",
    "        #save sentences' BOW in list sen_BOW\n",
    "        sen_words = []\n",
    "        for sent in sentences:\n",
    "            sent = transform_text(sent)\n",
    "            #Docfrequency\n",
    "            sen_words.append(sent)\n",
    "            \n",
    "            #SentenceBOW\n",
    "            bow_list.append(get_BOW(sent))\n",
    "            \n",
    "            #SentenceLength\n",
    "            sent_len.append(len(sent))\n",
    "            \n",
    "            #TotalLength\n",
    "            total_len += len(sent)\n",
    "            \n",
    "        \n",
    "        #calculate doc frequency    \n",
    "        for term in keyterms:\n",
    "            for i,bow in enumerate(sen_words):\n",
    "                if term in bow:\n",
    "                    article_dic[term].append(i)\n",
    "                    \n",
    "        #Docfrequency\n",
    "        question_list.append(article_dic)\n",
    "        #SentenceBOW\n",
    "        total_sentence_bow.append(bow_list)\n",
    "        #QuestionBOW\n",
    "        total_question_bow.append(que_list)\n",
    "        #SentenceLength\n",
    "        sent_lengthes.append(sent_len)\n",
    "        #AverageLength\n",
    "        avg_lengthes.append(float(total_len)/len(sentences))\n",
    "        #\n",
    "        answer_id.append(right_answer)\n",
    "        \n",
    "    return question_list, total_sentence_bow, total_question_bow, sent_lengthes, avg_lengthes, answer_id\n",
    "\n",
    "train,test,dev = input_data()\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "punctuations = [',','\\'\\'','?','\\'','.','%','(',')',';','``']\n",
    "question_list, total_sentence_bow, total_question_bow, sent_lengthes, avg_lengthes, answer_id_list= get_Docfrequency_SentenceBOW(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "NER\n",
    "'''\n",
    "time_word = [\n",
    "    'one','two','three','four','five','six','seven','eight','nine',\n",
    "    'January','February','March','April','May','June','July','August','September','October','November','December',\n",
    "    'million','billion',\n",
    "    'minutes','hours','years','times',\n",
    "    'mm','miles','inches','foot','feet','nm','cm','km','m',\n",
    "    'late','early','around','over',\n",
    "    'persons','seasons','square',\n",
    "    'spring','summer','fall','autumn','winter'\n",
    "]\n",
    "\n",
    "location_word = [\n",
    "    'southwest','southeast','northwest','northeast'\n",
    "]\n",
    "\n",
    "conjunction_word = ['and','of']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def input_NER():\n",
    "    stanford_dir = os.path.join('stanford-ner-2016-10-31')\n",
    "    jarfile = os.path.join(stanford_dir,'stanford-ner.jar')\n",
    "    modelfile = os.path.join(stanford_dir,'classifiers/english.all.3class.distsim.crf.ser.gz')\n",
    "    return modelfile,jarfile\n",
    "model,jar = input_NER()\n",
    "st = StanfordNERTagger(model,jar)\n",
    "\n",
    "def analyse_NER(ner_sentences):\n",
    "    result_sentences = []\n",
    "    for ner_sentence in ner_sentences:\n",
    "        result_sentence = []\n",
    "        perv_type = u'O'\n",
    "        word = u''\n",
    "        conjunction = u''\n",
    "        conjunc_flag = False\n",
    "        for index,(entity,etype) in enumerate(ner_sentence):\n",
    "            if perv_type == u'O' and etype != u'O':              \n",
    "                perv_type = etype\n",
    "                word = entity + u' '\n",
    "            elif word != u'':\n",
    "                if etype == u'O':\n",
    "                    if entity not in conjunction_word:\n",
    "                        result_sentence.append((word[:-1],perv_type))\n",
    "                        word = u''\n",
    "                        perv_type = u'O'\n",
    "                        if conjunction != u'':\n",
    "                            conjunction = u''\n",
    "                            conjunc_flag = False\n",
    "                    else:\n",
    "                        if conjunction != u'':\n",
    "                            conjunction = u''\n",
    "                            conjunc_flag = False\n",
    "                        else:\n",
    "                            conjunction = entity\n",
    "                            conjunc_flag = True\n",
    "                elif etype != perv_type:\n",
    "                    result_sentence.append((word[:-1],perv_type))\n",
    "                    word = entity + u' '\n",
    "                    perv_type = etype\n",
    "                    conjunction = u''\n",
    "                    conjunc_flag = False\n",
    "                elif etype == perv_type:\n",
    "                    if conjunc_flag:\n",
    "                        if conjunction == u',':\n",
    "                            word = word[:-1] + conjunction + u' ' + entity + u' '\n",
    "                        else:\n",
    "                            word = word + conjunction + u' ' + entity + u' '\n",
    "                        conjunction = u''\n",
    "                        conjunc_flag = False\n",
    "                    else:\n",
    "                        if entity in ['%'] or word == u'$ ':\n",
    "                            word = word[:-1] + entity + u' '\n",
    "                        else:\n",
    "                            word = word + entity + u' '\n",
    "        if word != u'':\n",
    "            result_sentence.append((word[:-1],perv_type))\n",
    "        result_sentences.append(result_sentence)      \n",
    "    return result_sentences\n",
    "\n",
    "def parse_NER(ner_sentences):\n",
    "    pattern_number = re.compile(r'([0-9]+|\\%|\\$)')\n",
    "    year_number = re.compile(r'([0-9]{4}s?)')\n",
    "    result_sentences = []\n",
    "    for ner_sentence in ner_sentences:\n",
    "        result_sentence = []\n",
    "        for index,(entity,etype) in enumerate(ner_sentence):\n",
    "            if entity != u'':\n",
    "                entity.replace(u'\\u2013',u'-')\n",
    "                entity.replace(u'\\u2014',u'-')\n",
    "                entity.replace(u'\\u2212',u'-')\n",
    "                entity.replace(u'\\u2044',u'%')\n",
    "                if etype == u'O':\n",
    "                    if year_number.search(entity):\n",
    "                        result_sentence.append((entity,u'YEAR'))\n",
    "                    elif pattern_number.search(entity) or entity in time_word:\n",
    "                        result_sentence.append((entity,u'NUMBER'))\n",
    "                    elif u'-' in entity:\n",
    "                        word_seperate = entity.split(u'-')\n",
    "                        for word in word_seperate:\n",
    "                            if word in time_word:\n",
    "                                result_sentence.append((entity,u'NUMBER'))\n",
    "                                break\n",
    "                    elif entity in location_word:\n",
    "                        result_sentence.append((entity,u'LOCATION'))\n",
    "                    elif index == 0 and entity.lower() not in stopwords:\n",
    "                        result_sentence.append((entity,u'ORGANIZATION'))\n",
    "                    elif index != 0 and entity[0].isupper():\n",
    "                        result_sentence.append((entity,u'ORGANIZATION'))\n",
    "                    else:\n",
    "                        result_sentence.append((entity,etype))\n",
    "                elif entity in ['(',')']:\n",
    "                    result_sentence.append((entity,u'O'))\n",
    "                else:\n",
    "                    result_sentence.append((entity,etype))\n",
    "        result_sentences.append(result_sentence)\n",
    "        \n",
    "    return result_sentences\n",
    "\n",
    "def extract_NER(parse_ner_sentence,mode):\n",
    "    result = []\n",
    "    if mode == 0:\n",
    "        #PERSON\n",
    "        for entity,etype in parse_ner_sentence:\n",
    "            if etype == u'PERSON':\n",
    "                result.append(entity)\n",
    "    elif mode == 1:\n",
    "        #NUMBER\n",
    "        for entity,etype in parse_ner_sentence:\n",
    "            if etype == u'NUMBER':\n",
    "                result.append(entity)\n",
    "    elif mode == 2:\n",
    "        #LOCATION\n",
    "        for entity,etype in parse_ner_sentence:\n",
    "            if etype == u'LOCATION':\n",
    "                result.append(entity)\n",
    "    elif mode == 3:\n",
    "        #ORGANIZATION\n",
    "        for entity,etype in parse_ner_sentence:\n",
    "            if etype == u'ORGANIZATION':\n",
    "                result.append(entity)\n",
    "    elif mode == 4:\n",
    "        #YEAR\n",
    "        for entity,etype in parse_ner_sentence:\n",
    "            if etype == u'YEAR':\n",
    "                result.append(entity)\n",
    "    return result\n",
    "\n",
    "def parse_token(token_sentence):\n",
    "    result = []\n",
    "    for index,word in enumerate(token_sentence):\n",
    "        if index != 0 and index != (len(token_sentence)-1) and word == u'.':\n",
    "            last_word = result[-1]\n",
    "            last_word = last_word + u'.'\n",
    "            result = result[:-1]\n",
    "            result.append(last_word)\n",
    "        else:\n",
    "            result.append(word)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only consider queries with 'What'/'what'\n",
    "def getHeadWord(text):\n",
    "    text = text.encode('ascii','replace')\n",
    "    pos =  pos_tag(word_tokenize(text))\n",
    "    if pos[0]==('Which', 'JJ'):\n",
    "        pos[0] = ('Which', 'WHICH')\n",
    "    for i,item in enumerate(pos):   \n",
    "        #print ('item',item)\n",
    "        word = item[0].lower()\n",
    "        ppos = item[1]\n",
    "        if word=='what' and (ppos=='WP' or ppos=='WDT'):\n",
    "            pos[i] = ('what', 'WHAT')   \n",
    "        elif word=='which' and ppos=='JJ':\n",
    "            pos[i] = ('which', 'WHICH')  \n",
    "    #print pos\n",
    "    grammar = r\"\"\"\n",
    "                V: {<V.*>}          # Verb\n",
    "                HEAD:\n",
    "                    {<IN>?<WHAT><V>?<DT>?<JJ.*|CD>*<V>?<IN>?<NN.*>+}\n",
    "                    {<IN>?<WHAT><V>?<DT>?<JJ.*|CD>*<V>?<IN>?<VBG.*>+}\n",
    "                    }<IN>?<WHAT><V>?<DT>?<JJ.*|CD>*<V>?<IN>?{  \n",
    "                \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar) \n",
    "    result = []\n",
    "    tree = cp.parse(pos)\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label()=='HEAD':\n",
    "            phrase = u' '.join([word for word,pos in subtree.leaves()])\n",
    "            phrase_list = phrase.split()\n",
    "            if len(phrase_list)>1:\n",
    "                ph = phrase_list[-1]\n",
    "            else:\n",
    "                ph = phrase\n",
    "            result.append(ph)\n",
    "            #print subtree.label()+':'+phrase+'->'+ph       \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n"
     ]
    }
   ],
   "source": [
    "headwords = []\n",
    "year_pattern = re.compile(r'(^[0-9]{4}s?$)')\n",
    "person_pattern = re.compile(r'(^(the )?([A-Z]+[a-zA-Z\\']+ ?((&|and|of) )?)+$)')\n",
    "number_pattern = re.compile(r'([0-9]+)')\n",
    "the_pattern = re.compile(r'(^the)')\n",
    "\n",
    "year_headword = {}\n",
    "organization_headword = {}\n",
    "person_headword = {}\n",
    "number_headword = {}\n",
    "location_headword = {}\n",
    "\n",
    "count = 0\n",
    "for article in train:\n",
    "    print count\n",
    "    count += 1\n",
    "    qas = article['qa']\n",
    "    sentences = article['sentences']\n",
    "    token_sentences = copy.deepcopy(sentences)\n",
    "    for i in range(len(sentences)):\n",
    "        token_sentences[i] = nltk.word_tokenize(token_sentences[i])\n",
    "    ner_sentences = st.tag_sents(token_sentences)\n",
    "    parse_ner_sentences = parse_NER(ner_sentences)\n",
    "    #print parse_ner_sentences\n",
    "    for qa in qas:\n",
    "        rank = {}\n",
    "        ner_sentence = parse_ner_sentences[qa['answer_sentence']]\n",
    "        for word in nltk.word_tokenize(qa['answer']):\n",
    "            for w,t in ner_sentence:\n",
    "                if t != u'O' and w == word:\n",
    "                    rank[t] = rank.get(t,0) + 1\n",
    "        result = sorted(rank.items(), lambda x, y: cmp(x[1], y[1]), reverse=True)\n",
    "        if result != []:        \n",
    "            kind = result[0][0]\n",
    "            text = qa['question']\n",
    "            if kind == u'NUMBER':\n",
    "                for word in getHeadWord(text):\n",
    "                    number_headword[word] = number_headword.get(word,0) + 1\n",
    "            elif kind == u'YEAR':\n",
    "                for word in getHeadWord(text):\n",
    "                    year_headword[word] = year_headword.get(word,0) + 1\n",
    "            elif kind == u'ORGANIZATION':\n",
    "                for word in getHeadWord(text):\n",
    "                    organization_headword[word] = organization_headword.get(word,0) + 1\n",
    "            elif kind == u'PERSON':\n",
    "                for word in getHeadWord(text):\n",
    "                    person_headword[word] = person_headword.get(word,0) + 1\n",
    "            elif kind == u'LOCATION':\n",
    "                for word in getHeadWord(text):\n",
    "                    location_headword[word] = location_headword.get(word,0) + 1\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "year_headword_edit = copy.deepcopy(year_headword)\n",
    "organization_headword_edit = copy.deepcopy(organization_headword)\n",
    "person_headword_edit = copy.deepcopy(person_headword)\n",
    "number_headword_edit = copy.deepcopy(number_headword)\n",
    "location_headword_edit = copy.deepcopy(location_headword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "year_headword = copy.deepcopy(year_headword_edit)\n",
    "organization_headword = copy.deepcopy(organization_headword_edit)\n",
    "person_headword = copy.deepcopy(person_headword_edit)\n",
    "number_headword = copy.deepcopy(number_headword_edit)\n",
    "location_headword = copy.deepcopy(location_headword_edit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year:  [u'years', u'year', u'decade', u'span']\n",
      "Organ:  [u'dynasty', u'school', u'companies', u'force', u'Neptune', u'studio', u'items', u'study', u'unit', u'army', u'music', u'type', u'award', u'word', u'work', u'era', u'example', u'feature', u'dialect', u'law', u'types', u'Beyonc', u'order', u'office', u'band', u'schools', u'mean', u'series', u'network', u'foundation', u'University', u'standard', u'project', u'Madonna', u'treaty', u'religions', u'result', u'subject', u'capacity', u'terms', u'Nasser', u'people', u'magazine', u'facility', u'war', u'concept', u'chain', u'battle', u'division', u'team', u'Chopin', u'groups', u'change', u'club', u'species', u'group', u'policy', u'term', u'name', u'album', u'theory', u'language', u'place', u'origin', u'form', u'channel', u'show', u'title', u'college', u'sport', u'label', u'nationality', u'color', u'period', u'case', u'newspaper', u'technology', u'document', u'model', u'world', u'competition', u'act', u'tribe', u'parties', u'film', u'Empire', u'building', u'represent', u'program', u'song', u'version', u'method', u'movement', u'Gaddafi', u'family', u'company', u'phrase', u'court', u'goal', u'plant', u'sect', u'committee', u'style', u'systems', u'event', u'issue', u'pub', u'reason', u'service', u'Oklahoma', u'station', u'store', u'park', u'translation', u'kind', u'plan', u'factor', u'Whitehead', u'culture', u'movie', u'denomination', u'league', u'church', u'ethnicity', u'conflict', u'purpose', u'nickname', u'person', u'organization', u'things', u'abbreviation', u'source', u'format', u'game', u'examples', u'Burke', u'religion', u'civilization', u'agreement', u'genre', u'Switzerland', u'line', u'role', u'department', u'structure', u'languages', u'Seattle', u'body', u'names', u'use', u'sort', u'process', u'stand', u'philosophy', u'holiday', u'material', u'university', u'cause', u'publication', u'festival', u'system', u'institution', u'agency', u'device', u'text', u'entity', u'view', u'party', u'practice', u'center', u'position', u'government', u'empire', u'acronym', u'Kerry', u'museum', u'piece', u'book', u'branch', u'category', u'Act', u'rule']\n",
      "Person:  [u'composer', u'figure', u'president', u'architect', u'singer', u'writer', u'artist', u'author', u'ruler', u'philosopher', u'leader', u'Darwin', u'actor']\n",
      "Number:  [u'month', u'increase', u'level', u'rate', u'cost', u'amount', u'Nigeria', u'share', u'date', u'Greece', u'day', u'Tucson', u'temperature', u'length', u'number', u'rank', u'size', u'percentage', u'season', u'population', u'price', u'average', u'GDP', u'ratio', u'score', u'point', u'value', u'century', u'centuries', u'speed', u'density', u'range', u'fraction', u'Detroit', u'limit', u'percent', u'portion', u'income', u'age', u'time']\n",
      "Location:  [u'venue', u'neighborhood', u'street', u'capital', u'airport', u'town', u'states', u'district', u'city', u'part', u'province', u'areas', u'countries', u'river', u'state', u'cities', u'region', u'territory', u'location', u'country', u'site', u'nation', u'continent', u'kingdom', u'island', u'road', u'area', u'county']\n"
     ]
    }
   ],
   "source": [
    "threshold = 8\n",
    "\n",
    "for (w,n) in year_headword.items():\n",
    "    if w in organization_headword:\n",
    "        if n >= organization_headword.get(w):\n",
    "            del organization_headword[w]\n",
    "        else:\n",
    "            del year_headword[w]\n",
    "for (w,n) in year_headword.items():\n",
    "    if w in person_headword:\n",
    "        if n >= person_headword.get(w):\n",
    "            del person_headword[w]\n",
    "        else:\n",
    "            del year_headword[w]\n",
    "for (w,n) in year_headword.items():\n",
    "    if w in number_headword:\n",
    "        if n >= number_headword.get(w):\n",
    "            del number_headword[w]\n",
    "        else:\n",
    "            del year_headword[w]\n",
    "for (w,n) in year_headword.items():\n",
    "    if w in location_headword:\n",
    "        if n >= location_headword.get(w):\n",
    "            del location_headword[w]\n",
    "        else:\n",
    "            del year_headword[w]\n",
    "            \n",
    "for (w,n) in organization_headword.items():\n",
    "    if w in person_headword:\n",
    "        if n >= person_headword.get(w):\n",
    "            del person_headword[w]\n",
    "        else:\n",
    "            del organization_headword[w]\n",
    "for (w,n) in organization_headword.items():\n",
    "    if w in number_headword:\n",
    "        if n >= number_headword.get(w):\n",
    "            del number_headword[w]\n",
    "        else:\n",
    "            del organization_headword[w]\n",
    "for (w,n) in organization_headword.items():\n",
    "    if w in location_headword:\n",
    "        if n >= location_headword.get(w):\n",
    "            del location_headword[w]\n",
    "        else:\n",
    "            del organization_headword[w]\n",
    "            \n",
    "for (w,n) in person_headword.items():\n",
    "    if w in number_headword:\n",
    "        if n >= number_headword.get(w):\n",
    "            del number_headword[w]\n",
    "        else:\n",
    "            del person_headword[w]\n",
    "for (w,n) in person_headword.items():\n",
    "    if w in location_headword:\n",
    "        if n >= location_headword.get(w):\n",
    "            del location_headword[w]\n",
    "        else:\n",
    "            del person_headword[w]\n",
    "\n",
    "for (w,n) in number_headword.items():\n",
    "    if w in location_headword:\n",
    "        if n >= location_headword.get(w):\n",
    "            del location_headword[w]\n",
    "        else:\n",
    "            del number_headword[w]\n",
    "            \n",
    "for (w,n) in year_headword.items():\n",
    "    if n < threshold:\n",
    "        del year_headword[w]\n",
    "for (w,n) in organization_headword.items():\n",
    "    if n < threshold:\n",
    "        del organization_headword[w]\n",
    "for (w,n) in person_headword.items():\n",
    "    if n < threshold:\n",
    "        del person_headword[w]\n",
    "for (w,n) in number_headword.items():\n",
    "    if n < threshold:\n",
    "        del number_headword[w]\n",
    "for (w,n) in location_headword.items():\n",
    "    if n < threshold:\n",
    "        del location_headword[w]\n",
    "\n",
    "\n",
    "location_list = location_headword.keys()\n",
    "number_list = number_headword.keys()\n",
    "organization_list = organization_headword.keys()\n",
    "name_list = person_headword.keys()\n",
    "year_list = year_headword.keys()\n",
    "\n",
    "print 'Year: ',year_headword.keys()\n",
    "print 'Organ: ',organization_headword.keys()\n",
    "print 'Person: ',person_headword.keys()\n",
    "print 'Number: ',number_headword.keys()\n",
    "print 'Location: ',location_headword.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def BM25_n(articles_index,k1,k2,b,n):\n",
    "    total_queries = len(total_question_bow[articles_index])\n",
    "    count = 0\n",
    "    correct_id = []\n",
    "    correct_id_weight = []\n",
    "    for index in range(len(total_question_bow[articles_index])):\n",
    "        poss_results = find_max_n_sentences(articles_index,index,k1,k2,b,n)\n",
    "        guess_ids = []\n",
    "        weights = []\n",
    "        for sent in poss_results:\n",
    "            guess_ids.append(sent[0])\n",
    "            weights.append(sent[1])\n",
    "        correct_id.append(guess_ids)\n",
    "        correct_id_weight.append(weights)\n",
    "    return correct_id,correct_id_weight\n",
    "\n",
    "def find_max_n_sentences(articles_index,index,k1,k2,b,n):\n",
    "    query_dict = total_question_bow[articles_index][index]\n",
    "    scores = {}\n",
    "    for index in range(len(total_sentence_bow[articles_index])):     \n",
    "        score = 0  \n",
    "        sentence_dict = total_sentence_bow[articles_index][index]\n",
    "        for word in query_dict:\n",
    "            document_fre_list = question_list[articles_index].get(word,None)\n",
    "            N = len(total_sentence_bow[articles_index])\n",
    "            n_qi = 0\n",
    "            if document_fre_list != None:\n",
    "                n_qi = len(document_fre_list)\n",
    "            else:\n",
    "                n_qi = 0\n",
    "            fi = sentence_dict.get(word,0)\n",
    "            qfi = query_dict.get(word,0)\n",
    "            dl = sent_lengthes[articles_index][index]\n",
    "            avgdl = avg_lengthes[articles_index]\n",
    "            \n",
    "            K = k1*(1-b+b*(float(dl)/avgdl)) \n",
    "            W = math.log((N-n_qi+0.5)/(n_qi+0.5))\n",
    "            R = (fi*(k1+1))/(fi+K)*qfi*(k2+1)/(qfi+k2)\n",
    "            score += W*R\n",
    "        scores[index] = score\n",
    "    scores = sorted(scores.items(), key=operator.itemgetter(1),reverse=True)[:n]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "k1_list = [0.78]\n",
    "k2_list = [0]\n",
    "b_list = [0.5]\n",
    "\n",
    "result_sentences = []\n",
    "result_sentences_weight = []\n",
    "test_length = len(dev)\n",
    "for k1 in k1_list:\n",
    "    for k2 in k2_list:\n",
    "        for b in b_list:\n",
    "            for i in range(0,test_length):\n",
    "                #the amount of extract sentences\n",
    "                amount = 4\n",
    "                correct_id,correct_id_weight = BM25_n(i,k1,k2,b,amount)\n",
    "                result_sentences.append(correct_id)\n",
    "                result_sentences_weight.append(correct_id_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Accuracy of BM25 with k1: 1.1  k2: 0  b: 0.18\n",
    "# 0.683639289744"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "location_list = location_list + ['country','county','district','city']\n",
    "number_list = number_list + ['version','size','msa','far','much','ration','time','many','population','large','percent','average','day','decade','big','long']\n",
    "name_list = name_list + ['name','center','president','denominations','denomination','film','broadcaster','pitcher','commentator']\n",
    "year_list = year_list + ['years','year','era']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "import re\n",
    "import sys\n",
    "\n",
    "def transfer_pos_question(pos):\n",
    "    new_pos = []\n",
    "    for (word,wtype) in pos:\n",
    "        if word.lower() == 'what' or word.lower() == 'what\\'s':\n",
    "            new_pos.append((word,'WHAT'))\n",
    "        elif word.lower() == 'do' or word.lower() == 'does' or word.lower() == 'did':\n",
    "            new_pos.append((word,'DO'))\n",
    "        elif word in number_list:\n",
    "            new_pos.append((word,'NUMBER'))\n",
    "        elif word in year_list:\n",
    "            new_pos.append((word,'YEAR'))\n",
    "        elif word in name_list:\n",
    "            new_pos.append((word,'NAME'))\n",
    "        elif word in location_list:\n",
    "            new_pos.append((word,'LOC'))\n",
    "        elif word.lower() == 'is' or word.lower() == 'was' or word.lower() == 'are' or word.lower() == 'were' or word.lower() == 'be':\n",
    "            new_pos.append((word,'BE'))\n",
    "        elif word.lower() == 'when':\n",
    "            new_pos.append((word,'WHEN'))\n",
    "        elif word.lower() == 'where':\n",
    "            new_pos.append((word,'WHERE'))\n",
    "        elif word.lower() == 'can':\n",
    "            new_pos.append((word,'CAN'))\n",
    "        elif word.lower() == 'how':\n",
    "            new_pos.append((word,'HOW'))\n",
    "        elif word.lower() == 'who' or word.lower() == 'whom' or word.lower() == 'whose'  or word.lower() == 'whos':\n",
    "            new_pos.append((word,'WHO'))\n",
    "        elif word.lower() == 'which':\n",
    "            new_pos.append((word,'WHICH'))\n",
    "        elif word.lower() == 'define':\n",
    "            new_pos.append((word,'DEFINE'))\n",
    "        elif word.lower() == 'should':\n",
    "            new_pos.append((word,'SHOULD'))\n",
    "        elif word.lower() == 'why' or word.lower() == 'wy':\n",
    "            new_pos.append((word,'WHY'))\n",
    "        else:\n",
    "            new_pos.append((word,wtype))\n",
    "    return new_pos\n",
    "\n",
    "def get_continuous_chunks(text):\n",
    "    t = copy.deepcopy(text)\n",
    "    pos = pos_tag(nltk.word_tokenize(t))\n",
    "    pos = transfer_pos_question(pos)\n",
    "    #print pos\n",
    "    grammar = r\"\"\"\n",
    "                WHAT: \n",
    "                    {<WHAT>}\n",
    "                    {<WHICH>}\n",
    "                    {<DEFINE>}\n",
    "                WHO:\n",
    "                    {<WHO>}\n",
    "                    {<WHAT><BE>?<DT>?<JJ|RB>*<NAME>}\n",
    "                    {<WHAT><JJ|RB>*<NN>+<NAME>}\n",
    "                NUMBER:\n",
    "                    {<WHICH><NUMBER>}\n",
    "                    {<HOW><NUMBER>}\n",
    "                    {<WHAT><BE>?<DT>?<JJ>?<NN>*<JJ>?<NUMBER>}\n",
    "                WHEN:\n",
    "                    {<WHICH><YEAR>}\n",
    "                    {<WHAT><BE>?<DT>?<JJ>?<NN>*<JJ>?<YEAR>}\n",
    "                    {<WHEN>}\n",
    "                WHERE:\n",
    "                    {<WHERE>}\n",
    "                    {<WHAT><LOC>}\n",
    "                    {<WHAT><BE>?<DT>?<RBS>?<JJ>?<LOC>}\n",
    "                HOW:\n",
    "                    {<CAN>}\n",
    "                    {<DO>}\n",
    "                    {<SHOULD>}\n",
    "                    {<WHY>}\n",
    "                    {<HOW>}\n",
    "\n",
    "                \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar) \n",
    "    result = []\n",
    "    tree = cp.parse(pos)\n",
    "    #record the position of pos\n",
    "    #print pos\n",
    "    flag = 0\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() != 'S':\n",
    "            phrase = u''\n",
    "            for word,pos in subtree.leaves():\n",
    "                if word == ',':\n",
    "                    phrase = phrase + word\n",
    "                else:\n",
    "                    phrase = phrase + u' '\n",
    "                    phrase = phrase + word\n",
    "            result.append((subtree.label(),phrase[1:]))         \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transfer_pos_sentence(pos):\n",
    "    new_pos = []\n",
    "    \n",
    "    for (word,wtype) in pos:\n",
    "        time_pattern = re.compile(r'[0-9]{4}s?')\n",
    "        if word.lower() == 'and' or word.lower() == 'or':\n",
    "            new_pos.append((word,'POSICC'))\n",
    "        elif word.lower() == 'with':\n",
    "            new_pos.append((word,'WITH'))\n",
    "        elif word.lower() == 'a' or word.lower() == 'an':\n",
    "            new_pos.append((word,'A'))\n",
    "        elif word == '\"':\n",
    "            new_pos.append((word,'\"'))\n",
    "        elif word == 'minutes' or word == 'March':\n",
    "            new_pos.append((word,'TIME'))\n",
    "        elif time_pattern.search(word):\n",
    "            new_pos.append((word,'TIME'))\n",
    "        elif word == 'around':\n",
    "            new_pos.append((word,'AROUND'))\n",
    "        elif word == 'mm':\n",
    "            new_pos.append((word,'CD')) \n",
    "        else:\n",
    "            new_pos.append((word,wtype))\n",
    "    return new_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def screen_out_answer_WHAT(result):\n",
    "    answer_list = []\n",
    "    for wtype, word in result:\n",
    "        answer_list.append(word)\n",
    "    return answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_rule_1(entity,query):\n",
    "    #lower scores for content words also appear in the query\n",
    "    count = 0\n",
    "    length = len(entity)\n",
    "    for word in entity:\n",
    "        word = lemmatize(word.lower())\n",
    "        if word not in stopwords:\n",
    "            if word in query:\n",
    "                count += 1\n",
    "    if length == 0:\n",
    "        score = 0\n",
    "    else:\n",
    "        score = 1 - float(count)/length\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_open_class_words(query):\n",
    "    result = []\n",
    "    for index in range(len(query)):\n",
    "        if query[index] not in stopwords:\n",
    "            if query[index] not in string.punctuation:\n",
    "                result.append(query[index])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rank_rule_3(answer_sentence,sentence,entity,query):\n",
    "    #higher scores for closer distance between an entity and the headword\n",
    "    #step 1: using a filter to extract \"useful\" open-class words\n",
    "    results = get_open_class_words(query)\n",
    "    sent = sentence\n",
    "    original_sent = answer_sentence\n",
    "    entity_loc = []\n",
    "    query_loc = []\n",
    "    for word in entity:\n",
    "        if word in original_sent:\n",
    "            entity_loc.append(original_sent.index(word))\n",
    "    for q in results:\n",
    "        if q in sent:\n",
    "            query_loc.append(sent.index(q))\n",
    "    min_dist = len(original_sent)\n",
    "    if query_loc != []:\n",
    "        for i in query_loc:\n",
    "            for j in entity_loc:\n",
    "                dist = abs(i - j)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    \n",
    "    return 1 - float(min_dist)/len(original_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_continuous_chunks_sentence(text,texttype):\n",
    "    t = copy.deepcopy(text)\n",
    "    pos =  pos_tag(nltk.word_tokenize(t))\n",
    "    if texttype==0:\n",
    "        #WHAT\n",
    "        pos = transfer_pos_sentence(pos)\n",
    "        grammar = r\"\"\"\n",
    "                    J:\n",
    "                        {<JJ.*><VBN>}\n",
    "                        {<JJ.*><POSICC><JJ.*>}   \n",
    "                        {<JJ.*>+}\n",
    "                        {<NN.*><POS>}\n",
    "                    N:\n",
    "                        {<CD>+<NN.*>}\n",
    "                        {<A>?<NN.*>?<J>?<NN.*>+}\n",
    "                        <\\\">{<A>?<J>?<NN.*>+}<\\\">\n",
    "                    COMBON:\n",
    "                        {(<N><,>)*<N><,>?<POSICC><N>}\n",
    "                    NWC:\n",
    "                        {<N><WITH><COMBON>}\n",
    "                    \"\"\"\n",
    "    cp = nltk.RegexpParser(grammar) \n",
    "    result = []\n",
    "    poss = copy.deepcopy(pos)\n",
    "    tree = cp.parse(pos)\n",
    "    #record the position of pos\n",
    "    flag = 0\n",
    "    for i in range (len(tree)):   \n",
    "        if type(tree[i]) != tuple:\n",
    "            subtree = tree[i]\n",
    "            if texttype==0 and subtree.label() != 'S':\n",
    "                phrase = u''\n",
    "                for word,pos in subtree.leaves():\n",
    "                    if word == ',':\n",
    "                        phrase = phrase + word\n",
    "                    else:\n",
    "                        phrase = phrase + u' '\n",
    "                        phrase = phrase + word\n",
    "                result.append((subtree.label(),phrase[1:]))\n",
    "                #print subtree.label(),phrase\n",
    "            elif subtree.label() != 'S':\n",
    "                phrase = u' '.join([word for word,pos in subtree.leaves()])\n",
    "                result.append((subtree.label(),phrase))\n",
    "    return poss,result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  0\n",
      "processing:  1\n",
      "processing:  2\n",
      "processing:  3\n",
      "processing:  4\n",
      "processing:  5\n",
      "processing:  6\n",
      "processing:  7\n",
      "processing:  8\n",
      "processing:  9\n",
      "processing:  10\n",
      "processing:  11\n",
      "processing:  12\n",
      "processing:  13\n",
      "processing:  14\n",
      "processing:  15\n",
      "processing:  16\n",
      "processing:  17\n",
      "processing:  18\n",
      "processing:  19\n",
      "processing:  20\n",
      "processing:  21\n",
      "processing:  22\n",
      "processing:  23\n",
      "processing:  24\n",
      "processing:  25\n",
      "processing:  26\n",
      "processing:  27\n",
      "processing:  28\n",
      "processing:  29\n",
      "processing:  30\n",
      "processing:  31\n",
      "processing:  32\n",
      "processing:  33\n",
      "processing:  34\n",
      "processing:  35\n",
      "processing:  36\n",
      "processing:  37\n",
      "processing:  38\n",
      "processing:  39\n",
      "0.196975067943\n",
      "0.360155973059\n"
     ]
    }
   ],
   "source": [
    "total_question = 0.0\n",
    "total_count = 0.0\n",
    "c = 0.0\n",
    "for index in range(len(dev)):#range(len(train)):\n",
    "    print \"processing: \",index\n",
    "    article = dev[index]\n",
    "    qas = article['qa']\n",
    "    sentences = article['sentences']\n",
    "    token_sentences = copy.deepcopy(sentences)\n",
    "    for i in range(len(sentences)):\n",
    "        token_sentences[i] = nltk.word_tokenize(token_sentences[i])\n",
    "    ner_sentences = st.tag_sents(token_sentences)\n",
    "    parse_ner_sentences = analyse_NER(parse_NER(ner_sentences))\n",
    "    total_question += len(qas)\n",
    "    for i in range(len(qas)):\n",
    "        flag = False\n",
    "        answer_list = []\n",
    "        qa = qas[i]\n",
    "        real_answer = qa['answer']\n",
    "        real_answer_sentence_id = qa['answer_sentence']\n",
    "        answers = []\n",
    "        scs = []\n",
    "        amount = 4\n",
    "        #the amount of extract sentences\n",
    "        for ii in range(amount):\n",
    "            if result_sentences_weight[index][i][ii] != 0:\n",
    "                weight = float(result_sentences_weight[index][i][ii])/sum(result_sentences_weight[index][i])\n",
    "            else:\n",
    "                weight = 0\n",
    "            answer_sentence = sentences[result_sentences[index][i][ii]]\n",
    "            answer_sentence_id = result_sentences[index][i][ii]\n",
    "            text_question = qa['question']\n",
    "            result = get_continuous_chunks(text_question)\n",
    "\n",
    "            #print pos\n",
    "            if result != []:\n",
    "                \n",
    "                wtype,word = result[0]\n",
    "                #print wtype,qa['question']\n",
    "                #print 'Answer: ', qa['answer']\n",
    "                if wtype == 'NUMBER':\n",
    "                    #print 'NUMBER',qa['question']\n",
    "                    answer_list = extract_NER(parse_ner_sentences[answer_sentence_id],1)\n",
    "                    if qa['answer'] in answer_list:\n",
    "                        flag = True\n",
    "                        total_count += 1\n",
    "                elif wtype == 'WHO':\n",
    "                    #print 'WHO',qa['question']\n",
    "                    answer_list = extract_NER(parse_ner_sentences[answer_sentence_id],0)\n",
    "                    if answer_list == []:\n",
    "                        answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],2)\n",
    "                        answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],3)\n",
    "                    if qa['answer'] in answer_list:\n",
    "                        flag = True\n",
    "                        total_count += 1\n",
    "                elif wtype == 'WHERE':\n",
    "                    #print 'WHERE',qa['question']\n",
    "                    answer_list = extract_NER(parse_ner_sentences[answer_sentence_id],2)\n",
    "                    if answer_list == []:\n",
    "                        answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],0)\n",
    "                        answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],3)\n",
    "                    if qa['answer'] in answer_list:\n",
    "                        flag = True\n",
    "                        total_count += 1\n",
    "                elif wtype == 'WHEN':\n",
    "                    #print 'WHEN',qa['question']\n",
    "                    answer_list = extract_NER(parse_ner_sentences[answer_sentence_id],4)\n",
    "                    if qa['answer'] in answer_list:\n",
    "                        flag = True\n",
    "                        total_count += 1\n",
    "                else:\n",
    "                    #WHAT\n",
    "                    answer_list = extract_NER(parse_ner_sentences[answer_sentence_id],3)\n",
    "                    if answer_list == []:\n",
    "                        answer_sentence = sentences[answer_sentence_id]\n",
    "                        wpos,wresult = get_continuous_chunks_sentence(answer_sentence,0)\n",
    "                        answer_list = screen_out_answer_WHAT(wresult)\n",
    "                    if qa['answer'] in answer_list:\n",
    "                        flag = True\n",
    "                        total_count += 1\n",
    "                #print answer_list\n",
    "                #print sentences[answer_sentence_id]\n",
    "            else:\n",
    "                #print 'None', qa['question']\n",
    "                #print 'Answer: ', qa['answer']\n",
    "                answer_list = extract_NER(parse_ner_sentences[answer_sentence_id],3)\n",
    "                if answer_list == []:\n",
    "                    answer_sentence = sentences[answer_sentence_id]\n",
    "                    wpos,wresult = get_continuous_chunks_sentence(answer_sentence,0)\n",
    "                    answer_list = screen_out_answer_WHAT(wresult)\n",
    "                if qa['answer'] in answer_list:\n",
    "                    flag = True\n",
    "                    total_count += 1         \n",
    "            if answer_list != []:\n",
    "                query = copy.deepcopy(text_question)\n",
    "                query = nltk.word_tokenize(query)\n",
    "                #token the answer sentence and copy it for further usage\n",
    "                answer_sentence = sentences[answer_sentence_id]\n",
    "                answer_sentence = nltk.word_tokenize(answer_sentence)\n",
    "                sentence = copy.deepcopy(answer_sentence)\n",
    "\n",
    "                for query_index in range(len(query)):\n",
    "                    query[query_index] = lemmatize(query[query_index].lower())\n",
    "\n",
    "                for sent_index in range(len(sentence)):\n",
    "                    sentence[sent_index] = lemmatize(sentence[sent_index].lower())\n",
    "\n",
    "                scores_1 = []\n",
    "                scores_3 = []\n",
    "                scores = []\n",
    "\n",
    "                for entity in answer_list:\n",
    "                    entity = nltk.word_tokenize(entity) \n",
    "                    score1 = rank_rule_1(entity,query)\n",
    "                    scores_1.append(score1)\n",
    "                    #answer_sentence is the original version and sentence is preprocessed\n",
    "                    score3 = rank_rule_3(answer_sentence,sentence,entity,query)\n",
    "                    scores_3.append(score3)\n",
    "                    w1 = 0.2\n",
    "                    w3 = 1 - w1\n",
    "                    if score1 == 0:\n",
    "                        score3 = 0\n",
    "                    total = w1 * score1 + w3 * score3\n",
    "                    scores.append(total)\n",
    "                answer = answer_list[scores.index(max(scores))]\n",
    "                sc = max(scores)*weight\n",
    "            \n",
    "            else:\n",
    "                answer = answer_sentence\n",
    "                sc = 0\n",
    "            answers.append(answer)\n",
    "            scs.append(sc)\n",
    "        ans = answers[scs.index(max(scs))]\n",
    "        if ans == real_answer:\n",
    "            c += 1\n",
    "#             else:\n",
    "#                 if qa['answer'] in answer_list:\n",
    "#                     print '*'*100\n",
    "#                     print 'Question: ',text_question\n",
    "#                     print 'lists: ',answer_list\n",
    "#                     print 'scores: ',scores\n",
    "#                     print 'pred: ',answer\n",
    "#                     print 'actual: ',qa['answer']\n",
    "#                     print 'sentence: ',sentences[answer_sentence_id]\n",
    "print c/total_question\n",
    "print total_count/total_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 0.184095474418 5\n",
    "# 0.184213635827 4\n",
    "# 0.183859151601 3\n",
    "# 0.183032021742 2\n",
    "# 0.177242112726 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([('However', 'RB'), (',', ','), ('particularly', 'RB'), ('intense', 'JJ'), ('near-IR', 'JJ'), ('light', 'NN'), ('(', '('), ('e.g.', 'JJ'), (',', ','), ('from', 'IN'), ('IR', 'NNP'), ('lasers', 'NNS'), (',', ','), ('IR', 'NNP'), ('LED', 'NNP'), ('sources', 'NNS'), (',', ','), ('or', 'POSICC'), ('from', 'IN'), ('bright', 'JJ'), ('daylight', 'NN'), ('with', 'WITH'), ('the', 'DT'), ('visible', 'JJ'), ('light', 'NN'), ('removed', 'VBN'), ('by', 'IN'), ('colored', 'JJ'), ('gels', 'NNS'), (')', ')'), ('can', 'MD'), ('be', 'VB'), ('detected', 'VBN'), ('up', 'RP'), ('to', 'TO'), ('approximately', 'RB'), ('780', 'CD'), ('nm', 'NNS'), (',', ','), ('and', 'POSICC'), ('will', 'MD'), ('be', 'VB'), ('perceived', 'VBN'), ('as', 'IN'), ('red', 'JJ'), ('light', 'NN'), ('.', '.')], [(u'N', u'intense near-IR light'), (u'J', u'e.g.'), (u'N', u'IR lasers'), (u'N', u'IR LED sources'), (u'N', u'bright daylight'), (u'N', u'visible light'), (u'N', u'colored gels'), (u'N', u'780 nm'), (u'N', u'red light')])\n"
     ]
    }
   ],
   "source": [
    "text = 'However, particularly intense near-IR light (e.g., from IR lasers, IR LED sources, or from bright daylight with the visible light removed by colored gels) can be detected up to approximately 780 nm, and will be perceived as red light.'\n",
    "print get_continuous_chunks_sentence(text,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sss\n"
     ]
    }
   ],
   "source": [
    "lists = [u'year','y']\n",
    "if u'y' in lists:\n",
    "    print 'sss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
