{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    Team: \n",
    "        QAOutliers\n",
    "    \n",
    "    Memebers: \n",
    "        Zhanyi Qi\n",
    "        Lianyu Zeng\n",
    "        Yiheng Wang\n",
    "        \n",
    "    This cell contains def that we create for our Basic Q&A System. \n",
    "    Our Basic Q&A System has five parts:\n",
    "        I.   Sentence Retrieval by algorithm BM25\n",
    "        II.  Question Classification with simple rules\n",
    "        III. Named Entity Recognition processing by Stanford NER Tagger\n",
    "        IV.  Answer Ranking rules\n",
    "        V.   Output \n",
    "'''\n",
    "import csv\n",
    "import json\n",
    "import copy\n",
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import re\n",
    "import sys\n",
    "import string\n",
    "from math import log\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from scipy.spatial.distance import cosine as cos_distance\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from nltk.tag import StanfordNERTagger\n",
    "'''\n",
    "    Part I. Sentence Retrieval by algorithm BM25\n",
    "    There are 7 def in this part.\n",
    "    1. get_BOW()\n",
    "                input:  a list of word\n",
    "                output: a dictionary of these words\n",
    "    2. lemmatize()\n",
    "                input:  a word\n",
    "                output: lemmatized word\n",
    "    3. input_data()\n",
    "                input:  None \n",
    "                output: train, test, and dev data sets\n",
    "    4. transform_text()\n",
    "                input:  a sentence\n",
    "                output: a list of words from this sentence by tokenizing, lemmatizing, filtering stopwords and \n",
    "                        punctuations\n",
    "    5. get_Docfrequency_SentenceBOW()\n",
    "                input:  a data set from train, test or dev\n",
    "                output: 5 values which are list, dictionary, dictionary, list and list. These values will be used\n",
    "                        in the calculating process in BM25\n",
    "    6. find_max_score_sentence()\n",
    "                input:  the index of this article, the index of this question, k1, k2 and b which are \n",
    "                        the coefficients in BM25 algorithm\n",
    "                output: the best match sentence id\n",
    "    7. BM25()\n",
    "                input:  the index of this article, k1, k2 and b which are the coefficients in BM25 algorithm\n",
    "                output: the list of sentence id that predicted by BM25\n",
    "'''\n",
    "def get_BOW(text):\n",
    "    BOW = {}\n",
    "    for word in text:\n",
    "        BOW[word] = BOW.get(word,0) + 1\n",
    "    return BOW\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "def input_data():\n",
    "    base_path = os.path.join('data/')\n",
    "    train_file = base_path + 'QA_train.json'\n",
    "    train_data = json.load(open(train_file))\n",
    "    test_file = base_path + 'QA_test.json'\n",
    "    test_data = json.load(open(test_file))\n",
    "    dev_file = base_path + 'QA_dev.json'\n",
    "    dev_data = json.load(open(dev_file))\n",
    "    return train_data,test_data,dev_data\n",
    "def transform_text(text):\n",
    "    text = nltk.word_tokenize(text)\n",
    "    text = [lemmatize(word.lower()) for word in text]\n",
    "    result = []\n",
    "    for word in text:\n",
    "        if word not in stopwords and word not in punctuations:\n",
    "            result.append(word)\n",
    "    return result\n",
    "def get_Docfrequency_SentenceBOW(dataset):\n",
    "    #save dics, each dictionary contains document frequencies for all terms in the same article\n",
    "    question_list = []\n",
    "    #save lists, each list represent an article, saving sentences' bow\n",
    "    total_sentence_bow = []\n",
    "    #save lists, each list represent an article, saving questions' bow\n",
    "    total_question_bow = []\n",
    "    #save lists, each list represent all sentences' lengthes.\n",
    "    sent_lengthes = []\n",
    "    #save a list, each item represents the average length of sentences\n",
    "    avg_lengthes = []  \n",
    "    for article in dataset:\n",
    "        #Docfrequency\n",
    "        article_dic = defaultdict(list)\n",
    "        keyterms = [] #save all distinct terms in questions      \n",
    "        #SentenceBOW\n",
    "        bow_list = []     \n",
    "        #QuestionBOW\n",
    "        que_list = []      \n",
    "        #SentenceLength\n",
    "        sent_len = []    \n",
    "        #TotalLength\n",
    "        total_len = 0     \n",
    "        qas = article['qa']\n",
    "        sentences = article['sentences']\n",
    "        for qa in qas:\n",
    "            question = qa['question']\n",
    "            newquestion = transform_text(question)\n",
    "            #QuestionBOW\n",
    "            que_list.append(get_BOW(newquestion))          \n",
    "            keyterms.extend(newquestion)\n",
    "        keyterms = set(keyterms)       \n",
    "        #save sentences' BOW in list sen_BOW\n",
    "        sen_words = []\n",
    "        for sent in sentences:\n",
    "            sent = transform_text(sent)\n",
    "            #Docfrequency\n",
    "            sen_words.append(sent)           \n",
    "            #SentenceBOW\n",
    "            bow_list.append(get_BOW(sent))            \n",
    "            #SentenceLength\n",
    "            sent_len.append(len(sent))       \n",
    "            #TotalLength\n",
    "            total_len += len(sent)       \n",
    "        #calculate doc frequency    \n",
    "        for term in keyterms:\n",
    "            for i,bow in enumerate(sen_words):\n",
    "                if term in bow:\n",
    "                    article_dic[term].append(i)                   \n",
    "        #Docfrequency\n",
    "        question_list.append(article_dic)\n",
    "        #SentenceBOW\n",
    "        total_sentence_bow.append(bow_list)\n",
    "        #QuestionBOW\n",
    "        total_question_bow.append(que_list)\n",
    "        #SentenceLength\n",
    "        sent_lengthes.append(sent_len)\n",
    "        #AverageLength\n",
    "        avg_lengthes.append(float(total_len)/len(sentences))  \n",
    "    return question_list, total_sentence_bow, total_question_bow, sent_lengthes, avg_lengthes\n",
    "def find_max_score_sentence(articles_index,index,k1,k2,b):\n",
    "    #Get the dictionary of this question from QuestionBOW\n",
    "    query_dict = total_question_bow[articles_index][index]\n",
    "    max_score = 0\n",
    "    guess_sentence = 0\n",
    "    for index in range(len(total_sentence_bow[articles_index])):     \n",
    "        score = 0  \n",
    "        #Get the dictionary of this sentence from SentenceBOW\n",
    "        sentence_dict = total_sentence_bow[articles_index][index]\n",
    "        #Calculate the score of each word in question query\n",
    "        for word in query_dict:\n",
    "            document_fre_list = question_list[articles_index].get(word,None)\n",
    "            N = len(total_sentence_bow[articles_index])\n",
    "            n_qi = 0\n",
    "            if document_fre_list != None:\n",
    "                n_qi = len(document_fre_list)\n",
    "            else:\n",
    "                n_qi = 0\n",
    "            fi = sentence_dict.get(word,0)\n",
    "            qfi = query_dict.get(word,0)\n",
    "            dl = sent_lengthes[articles_index][index]\n",
    "            avgdl = avg_lengthes[articles_index]            \n",
    "            K = k1*(1-b+b*(float(dl)/avgdl))\n",
    "            W = math.log((N-n_qi+0.5)/(n_qi+0.5))\n",
    "            R = (fi*(k1+1))/(fi+K)*qfi*(k2+1)/(qfi+k2)\n",
    "            score += W*R\n",
    "        if score > max_score:\n",
    "            max_score = score\n",
    "            guess_sentence = index\n",
    "    return guess_sentence\n",
    "def BM25(articles_index,k1,k2,b):\n",
    "    result_id = []\n",
    "    for index in range(len(total_question_bow[articles_index])):\n",
    "        #Predict the sentence id of each question query\n",
    "        guess_id = find_max_score_sentence(articles_index,index,k1,k2,b)\n",
    "        result_id.append(guess_id)\n",
    "    return result_id\n",
    "'''\n",
    "    Part II. Question Classification with simple rules\n",
    "    There are 2 def in this part.\n",
    "    1. transfer_pos_question()\n",
    "                input:  pos of a question query\n",
    "                output: edited pos of this question query\n",
    "    2. get_continuous_chunks()\n",
    "                input:  a question query\n",
    "                output: chunk of this question query, like [('WHEN','when ...')...]\n",
    "'''\n",
    "def transfer_pos_question(pos):\n",
    "    new_pos = []\n",
    "    #Edit the pos tag from nltk pos_tag to customized tag\n",
    "    for (word,wtype) in pos:\n",
    "        if word.lower() == 'what' or word.lower() == 'what\\'s':\n",
    "            new_pos.append((word,'WHAT'))\n",
    "        elif word.lower() == 'do' or word.lower() == 'does' or word.lower() == 'did':\n",
    "            new_pos.append((word,'DO'))\n",
    "        #Manully tag some word from number_list as 'TIME' to classisify the question type\n",
    "        elif word.lower() in number_list:\n",
    "            new_pos.append((word,'TIME'))\n",
    "        #Manully tag some word from location_list as 'LOC' to classisify the question type\n",
    "        elif word.lower() in location_list:\n",
    "            new_pos.append((word,'LOC'))\n",
    "        #Manully tag some word from name_list as 'NAME' to classisify the question type\n",
    "        elif word.lower() in name_list:\n",
    "            new_pos.append((word,'NAME'))\n",
    "        elif word.lower() == 'is' or word.lower() == 'was' or word.lower() == 'are' or word.lower() == 'were' or word.lower() == 'be':\n",
    "            new_pos.append((word,'BE'))\n",
    "        elif word.lower() == 'when':\n",
    "            new_pos.append((word,'WHEN'))\n",
    "        elif word.lower() == 'where':\n",
    "            new_pos.append((word,'WHERE'))\n",
    "        elif word.lower() == 'how':\n",
    "            new_pos.append((word,'HOW'))\n",
    "        elif word.lower() == 'who' or word.lower() == 'whom' or word.lower() == 'whose'  or word.lower() == 'whos':\n",
    "            new_pos.append((word,'WHO'))\n",
    "        elif word.lower() == 'which':\n",
    "            new_pos.append((word,'WHICH'))\n",
    "        elif word.lower() == 'why' or word.lower() == 'wy':\n",
    "            new_pos.append((word,'WHY'))\n",
    "        else:\n",
    "            new_pos.append((word,wtype))\n",
    "    return new_pos\n",
    "def get_continuous_chunks(text):\n",
    "    #Get pos tag by nltk pos tagger of this qyestion query\n",
    "    pos = pos_tag(nltk.word_tokenize(text))\n",
    "    #Edit the pos tag to customized pos tag\n",
    "    pos = transfer_pos_question(pos)\n",
    "    #Define different grammar for different types of question\n",
    "    grammar = r\"\"\"\n",
    "                WHAT: \n",
    "                    {<WHAT>}\n",
    "                    {<WHICH>}\n",
    "                WHO:\n",
    "                    {<WHO>}\n",
    "                    {<WHAT><BE>?<DT>?<JJ|RB>*<NAME>}\n",
    "                    {<WHAT><JJ|RB>*<NN>+<NAME>}\n",
    "                WHEN:\n",
    "                    {<WHICH><TIME>}\n",
    "                    {<HOW><TIME>}\n",
    "                    {<WHAT><BE>?<DT>?<JJ>?<NN>*<JJ>?<TIME>}\n",
    "                    {<WHEN>}\n",
    "                WHERE:\n",
    "                    {<WHERE>}\n",
    "                    {<WHAT><LOC>}\n",
    "                HOW:\n",
    "                    {<DO>}\n",
    "                    {<WHY>}\n",
    "                    {<HOW>}\n",
    "\n",
    "                \"\"\"\n",
    "    #Load nltk RegexpParser to analyse the grammar\n",
    "    cp = nltk.RegexpParser(grammar) \n",
    "    result = []\n",
    "    tree = cp.parse(pos)\n",
    "    #Parse the grammar tree\n",
    "    for subtree in tree.subtrees():\n",
    "        if subtree.label() != 'S':\n",
    "            phrase = u''\n",
    "            for word,pos in subtree.leaves():\n",
    "                if word == ',':\n",
    "                    phrase = phrase + word\n",
    "                else:\n",
    "                    phrase = phrase + u' '\n",
    "                    phrase = phrase + word\n",
    "            result.append((subtree.label(),phrase[1:]))         \n",
    "    return result\n",
    "'''\n",
    "    Part III. Named Entity Recognition processing by Stanford NER Tagger\n",
    "    There are 5 def in this part.\n",
    "    1. input_NER()\n",
    "                input:  None\n",
    "                output: StanfordNER model file, StanfordNER jar file\n",
    "    2. analyse_NER()\n",
    "                input:  list of sentences that already tagged by edited NER tool\n",
    "                output: remove 'O' tag and combine continuous same tags as one entity\n",
    "    3. parse_NER()\n",
    "                input:  list of sentences that already tagged by Stanford NER tool \n",
    "                output: list of sentences that correct the NER result manully(by some rules)\n",
    "    4. extract_NER()\n",
    "                input:  NER sentence and the model number\n",
    "                output: result of orderred entities from this NER sentence \n",
    "    5. parse_token()\n",
    "                input:  a token of a sentence\n",
    "                output: correct mistake in tokens, and return the new tokens \n",
    "'''\n",
    "def input_NER():\n",
    "    stanford_dir = os.path.join('stanford-ner-2016-10-31')\n",
    "    jarfile = os.path.join(stanford_dir,'stanford-ner.jar')\n",
    "    modelfile = os.path.join(stanford_dir,'classifiers/english.all.3class.distsim.crf.ser.gz')\n",
    "    return modelfile,jarfile\n",
    "def analyse_NER(ner_sentences):\n",
    "    result_sentences = []\n",
    "    for ner_sentence in ner_sentences:\n",
    "        result_sentence = []\n",
    "        perv_type = u'O'\n",
    "        word = u''\n",
    "        conjunction = u''\n",
    "        conjunc_flag = False\n",
    "        for index,(entity,etype) in enumerate(ner_sentence):\n",
    "            if perv_type == u'O' and etype != u'O':              \n",
    "                perv_type = etype\n",
    "                word = entity + u' '\n",
    "            elif word != u'':\n",
    "                if etype == u'O':\n",
    "                    if entity not in conjunction_word:\n",
    "                        result_sentence.append((word[:-1],perv_type))\n",
    "                        word = u''\n",
    "                        perv_type = u'O'\n",
    "                        if conjunction != u'':\n",
    "                            conjunction = u''\n",
    "                            conjunc_flag = False\n",
    "                    else:\n",
    "                        if conjunction != u'':\n",
    "                            conjunction = u''\n",
    "                            conjunc_flag = False\n",
    "                        else:\n",
    "                            conjunction = entity\n",
    "                            conjunc_flag = True\n",
    "                elif etype != perv_type:\n",
    "                    result_sentence.append((word[:-1],perv_type))\n",
    "                    word = entity + u' '\n",
    "                    perv_type = etype\n",
    "                    conjunction = u''\n",
    "                    conjunc_flag = False\n",
    "                elif etype == perv_type:\n",
    "                    if conjunc_flag:\n",
    "                        if conjunction == u',':\n",
    "                            word = word[:-1] + conjunction + u' ' + entity + u' '\n",
    "                        else:\n",
    "                            word = word + conjunction + u' ' + entity + u' '\n",
    "                        conjunction = u''\n",
    "                        conjunc_flag = False\n",
    "                    else:\n",
    "                        if entity in ['%'] or word == u'$ ':\n",
    "                            word = word[:-1] + entity + u' '\n",
    "                        else:\n",
    "                            word = word + entity + u' '\n",
    "        if word != u'':\n",
    "            result_sentence.append((word[:-1],perv_type))\n",
    "        result_sentences.append(result_sentence)      \n",
    "    return result_sentences\n",
    "def parse_NER(ner_sentences):\n",
    "    pattern_number = re.compile(r'([0-9]+|\\%|\\$)')\n",
    "    result_sentences = []\n",
    "    for ner_sentence in ner_sentences:\n",
    "        result_sentence = []\n",
    "        for index,(entity,etype) in enumerate(ner_sentence):\n",
    "            if entity != u'':\n",
    "                entity.replace(u'\\u2013',u'-')\n",
    "                entity.replace(u'\\u2014',u'-')\n",
    "                entity.replace(u'\\u2212',u'-')\n",
    "                entity.replace(u'\\u2044',u'%')\n",
    "                if etype == u'O':\n",
    "                    if pattern_number.search(entity) or entity in time_word:\n",
    "                        result_sentence.append((entity,u'NUMBER'))\n",
    "                    elif u'-' in entity:\n",
    "                        word_seperate = entity.split(u'-')\n",
    "                        for word in word_seperate:\n",
    "                            if word in time_word:\n",
    "                                result_sentence.append((entity,u'NUMBER'))\n",
    "                                break\n",
    "                    elif entity in location_word:\n",
    "                        result_sentence.append((entity,u'LOCATION'))\n",
    "                    elif index == 0 and entity.lower() not in stopwords:\n",
    "                        result_sentence.append((entity,u'ORGANIZATION'))\n",
    "                    elif index != 0 and entity[0].isupper():\n",
    "                        result_sentence.append((entity,u'ORGANIZATION'))\n",
    "                    else:\n",
    "                        result_sentence.append((entity,etype))\n",
    "                elif entity in ['(',')']:\n",
    "                    result_sentence.append((entity,u'O'))\n",
    "                else:\n",
    "                    result_sentence.append((entity,etype))\n",
    "        result_sentences.append(result_sentence)     \n",
    "    return analyse_NER(result_sentences)\n",
    "def extract_NER(parse_ner_sentence,mode):\n",
    "    result = []\n",
    "    if mode == 0:\n",
    "        #PERSON\n",
    "        for entity,etype in parse_ner_sentence:\n",
    "            if etype == u'PERSON':\n",
    "                result.append(entity)\n",
    "    elif mode == 1:\n",
    "        #NUMBER\n",
    "        for entity,etype in parse_ner_sentence:\n",
    "            if etype == u'NUMBER':\n",
    "                result.append(entity)\n",
    "    elif mode == 2:\n",
    "        #LOCATION\n",
    "        for entity,etype in parse_ner_sentence:\n",
    "            if etype == u'LOCATION':\n",
    "                result.append(entity)\n",
    "    elif mode == 3:\n",
    "        #ORGANIZATION\n",
    "        for entity,etype in parse_ner_sentence:\n",
    "            if etype == u'ORGANIZATION':\n",
    "                result.append(entity)\n",
    "    return result\n",
    "def parse_token(token_sentence):\n",
    "    result = []\n",
    "    for index,word in enumerate(token_sentence):\n",
    "        if index != 0 and index != (len(token_sentence)-1) and word == u'.':\n",
    "            last_word = result[-1]\n",
    "            last_word = last_word + u'.'\n",
    "            result = result[:-1]\n",
    "            result.append(last_word)\n",
    "        else:\n",
    "            result.append(word)\n",
    "    return result\n",
    "'''\n",
    "    Part IV. Answer Ranking rules\n",
    "    There are 3 def in this part.\n",
    "    1. get_open_class_words()\n",
    "                input:  token of the question query\n",
    "                output: removed stopword and punctuation of the list\n",
    "    2. rank_rule_1()\n",
    "                input:  entity and the question query\n",
    "                output: score of rule 1\n",
    "    3. rank_rule_3()\n",
    "                input:  answer sentence, token of the answer snetence, entity and the token of question query \n",
    "                output: score of rule 3\n",
    "'''\n",
    "def get_open_class_words(query):\n",
    "    result = []\n",
    "    for index in range(len(query)):\n",
    "        if query[index] not in stopwords:\n",
    "            if query[index] not in string.punctuation:\n",
    "                result.append(query[index])\n",
    "    return result\n",
    "def rank_rule_1(entity,query):\n",
    "    #lower scores for content words also appear in the query\n",
    "    count = 0\n",
    "    length = len(entity)\n",
    "    for word in entity:\n",
    "        word = lemmatize(word.lower())\n",
    "        if word not in stopwords:\n",
    "            if word in query:\n",
    "                count += 1\n",
    "    if length == 0:\n",
    "        score = 0\n",
    "    else:\n",
    "        score = 1 - float(count)/length\n",
    "    return score\n",
    "def rank_rule_3(answer_sentence,sentence,entity,query):\n",
    "    #higher scores for closer distance between an entity and the headword\n",
    "    #step 1: using a filter to extract \"useful\" open-class words\n",
    "    results = get_open_class_words(query)\n",
    "    sent = sentence\n",
    "    original_sent = answer_sentence\n",
    "    entity_loc = []\n",
    "    query_loc = []\n",
    "    for word in entity:\n",
    "        if word in original_sent:\n",
    "            entity_loc.append(original_sent.index(word))\n",
    "    for q in results:\n",
    "        if q in sent:\n",
    "            query_loc.append(sent.index(q))\n",
    "    min_dist = len(original_sent)\n",
    "    if query_loc != []:\n",
    "        for i in query_loc:\n",
    "            for j in entity_loc:\n",
    "                dist = abs(i - j)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist                   \n",
    "    return 1 - float(min_dist)/len(original_sent)\n",
    "'''\n",
    "    Part V. Output\n",
    "    There are 1 def in this part.\n",
    "    1. output_result()\n",
    "                input:  filename\n",
    "                output: putting result in this file\n",
    "'''\n",
    "def output_result(filename):\n",
    "    predictions_file = open(filename, \"wb\")\n",
    "    open_file_object = csv.writer(predictions_file)\n",
    "    open_file_object.writerow([\"id\",\"answer\"])\n",
    "    for i in range(len(answers)):\n",
    "        open_file_object.writerow([ids[i], answers[i].encode(\"utf-8\")])\n",
    "    predictions_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    This cell is for initializing dataset and tools.\n",
    "'''\n",
    "#Initialize datasets and tools\n",
    "train,test,dev = input_data()\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "model,jar = input_NER()\n",
    "st = StanfordNERTagger(model,jar)\n",
    "\n",
    "#Define dataset for customized function\n",
    "punctuations = [',','\\'\\'','?','\\'','.','%','(',')',';','``']\n",
    "'''\n",
    "NER\n",
    "'''\n",
    "#Customized feature words in question\n",
    "location_list = ['country','county','district']\n",
    "number_list = ['version','size','msa','far','much','year','era','ration','years','time','many','population','large','percent','average','day','decade','big','long']\n",
    "name_list = ['name','center','president','denominations','denomination','film','broadcaster','pitcher','commentator']\n",
    "#Customized feature words in sentence\n",
    "time_word = [\n",
    "    'one','two','three','four','five','six','seven','eight','nine',\n",
    "    'January','February','March','April','May','June','July','August','September','October','November','December',\n",
    "    'million','billion',\n",
    "    'minutes','hours','years','times',\n",
    "    'mm','miles','inches','foot','feet',\n",
    "    'late','early','around','over',\n",
    "    'persons','seasons','square',\n",
    "    'spring','summer','fall','autumn','winter'\n",
    "]\n",
    "location_word = [\n",
    "    'southwest','southeast','northwest','northeast'\n",
    "]\n",
    "conjunction_word = ['and','of']\n",
    "#Initialize data for BM25 processing\n",
    "question_list, total_sentence_bow, total_question_bow, sent_lengthes, avg_lengthes = get_Docfrequency_SentenceBOW(dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "    This cell is for BM25 processing.\n",
    "'''\n",
    "#Set the range of k1, k2 and b\n",
    "k1_list = [0.78]\n",
    "k2_list = [0]\n",
    "b_list = [0.5]\n",
    "#Store the predicting result by BM25 in result_sentences\n",
    "result_sentences = []\n",
    "test_length = len(dev)\n",
    "for k1 in k1_list:\n",
    "    for k2 in k2_list:\n",
    "        for b in b_list:\n",
    "            for i in range(0,test_length):\n",
    "                correct_id = BM25(i,k1,k2,b)\n",
    "                result_sentences.append(correct_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "0.270812437312 0.278342693024 0.211634904714\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    This cell is for \n",
    "        I.   Question Classification\n",
    "        II.  Named Entity Recognition processing\n",
    "        III. Answer Ranking\n",
    "        IV.  Output.\n",
    "''' \n",
    "ids = []\n",
    "answers = []\n",
    "\n",
    "count_contains = 0\n",
    "count_similar = 0.0\n",
    "count_correct = 0\n",
    "total_question = 0\n",
    "\n",
    "for index in range(5):\n",
    "    print index\n",
    "    article = dev[index]\n",
    "    qas = article['qa']\n",
    "    sentences = article['sentences']\n",
    "    token_sentences = copy.deepcopy(sentences)\n",
    "    #Tag the sentence by NER processing\n",
    "    for i in range(len(sentences)):\n",
    "        token_sentences[i] = parse_token(nltk.word_tokenize(token_sentences[i]))\n",
    "    ner_sentences = st.tag_sents(token_sentences)\n",
    "    parse_ner_sentences = parse_NER(ner_sentences)\n",
    "    #Predict the answer of each question\n",
    "    total_question += len(qas)\n",
    "    for i in range(len(qas)):\n",
    "        qa = qas[i]\n",
    "        #Extract the sentence of answer\n",
    "        answer_sentence = sentences[result_sentences[index][i]]\n",
    "        #Extract the id of sentence of answer\n",
    "        answer_sentence_id = result_sentences[index][i]\n",
    "        #Extract question query\n",
    "        text_question = qa['question']\n",
    "        #Get the grammar(POS) result of question \n",
    "        result = get_continuous_chunks(text_question)\n",
    "        #id = qa['id']\n",
    "        if result != []:\n",
    "            wtype,word = result[0]\n",
    "            #According to the different types of question, we have different solutions.\n",
    "            if wtype == 'WHEN':\n",
    "                #If question type is 'WHEN', extract NER 'NUMBER' from sentence\n",
    "                answer_list = extract_NER(parse_ner_sentences[answer_sentence_id],1)\n",
    "                if answer_list == []:\n",
    "                    #If nothing is 'NUMBER', get 'ORGANIZATION' from sentence\n",
    "                    answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],3)\n",
    "            elif wtype == 'WHO':\n",
    "                #If question type is 'WHO', extract NER 'PERSON' from sentence\n",
    "                answer_list = extract_NER(parse_ner_sentences[answer_sentence_id],0)\n",
    "                if answer_list == []:\n",
    "                    #If nothing is 'PERSON', get 'ORGANIZATION' and 'LOCATION' from sentence\n",
    "                    answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],2)\n",
    "                    answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],3)\n",
    "            elif wtype == 'WHERE':\n",
    "                #If question type is 'WHERE', extract NER 'LOCATION' from sentence\n",
    "                answer_list = extract_NER(parse_ner_sentences[answer_sentence_id],2)\n",
    "                if answer_list == []:\n",
    "                    #If nothing is 'LOCATION', get 'ORGANIZATION' and 'PERSON' from sentence\n",
    "                    answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],0)\n",
    "                    answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],3)\n",
    "            else:\n",
    "                #For type 'WHAT' or 'HOW', first extract NER 'ORGANIZATION' from sentence\n",
    "                answer_list = extract_NER(parse_ner_sentences[answer_sentence_id],3)\n",
    "                if answer_list == []:\n",
    "                    #If nothing is 'ORGANIZATION', get 'LOCATION', 'NUMBER' and 'PERSON' from sentence\n",
    "                    answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],0)\n",
    "                    answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],1)\n",
    "                    answer_list += extract_NER(parse_ner_sentences[answer_sentence_id],2)                    \n",
    "        if answer_list != []:\n",
    "            if qa['answer'] in answer_list:\n",
    "                count_contains += 1\n",
    "            #For question that have answer, ranking the answer by rule 1&3\n",
    "            query = copy.deepcopy(text_question)\n",
    "            query = nltk.word_tokenize(query)\n",
    "            #token the answer sentence and copy it for further usage\n",
    "            answer_sentence = nltk.word_tokenize(answer_sentence)\n",
    "            sentence = copy.deepcopy(answer_sentence)         \n",
    "            for query_index in range(len(query)):\n",
    "                query[query_index] = lemmatize(query[query_index].lower())\n",
    "            for sent_index in range(len(sentence)):\n",
    "                sentence[sent_index] = lemmatize(sentence[sent_index].lower())                \n",
    "            scores_1 = []\n",
    "            scores_3 = []\n",
    "            scores = []\n",
    "            if len(answer_list) == 1:\n",
    "                answer = answer_list[0]\n",
    "            else:\n",
    "                for entity in answer_list:\n",
    "                    entity = nltk.word_tokenize(entity) \n",
    "                    score1 = rank_rule_1(entity,query)\n",
    "                    scores_1.append(score1)\n",
    "                    #answer_sentence is the original version and sentence is preprocessed\n",
    "                    score3 = rank_rule_3(answer_sentence,sentence,entity,query)\n",
    "                    scores_3.append(score3)\n",
    "                    w1 = 0.2\n",
    "                    w3 = 1 - w1\n",
    "                    if score1 == 0:\n",
    "                        score3 = 0\n",
    "                    total = w1 * score1 + w3 * score3\n",
    "                    scores.append(total)\n",
    "                answer = answer_list[scores.index(max(scores))]\n",
    "            \n",
    "        else:\n",
    "            #For question that doesn't have answer, return the total sentence\n",
    "            answer = answer_sentence\n",
    "        #ids.append(id)\n",
    "        if answer == qa['answer']:\n",
    "            count_correct += 1\n",
    "            count_similar += 1\n",
    "        else:\n",
    "            answer_token = nltk.word_tokenize(answer)\n",
    "            c_answer_token = nltk.word_tokenize(qa['answer'])\n",
    "            if len(answer_token) > len(c_answer_token):\n",
    "                mark_answer = 0\n",
    "                mark_c_answer = 0\n",
    "                match = 0\n",
    "                while(True):\n",
    "                    if answer_token[mark_answer] == c_answer_token[mark_c_answer]:\n",
    "                        mark_answer += 1\n",
    "                        mark_c_answer += 1\n",
    "                        match += 1\n",
    "                    else:\n",
    "                        mark_answer -= (match-1)\n",
    "                        match = 0\n",
    "                        mark_c_answer = 0\n",
    "                    if match == len(c_answer_token):\n",
    "                        #print answer,'&',qa['answer']\n",
    "                        count_similar += float(len(c_answer_token))/len(answer_token)\n",
    "                        break\n",
    "                    if mark_answer >= len(answer_token):\n",
    "                        break                                        \n",
    "            else:\n",
    "                mark_answer = 0\n",
    "                mark_c_answer = 0\n",
    "                match = 0\n",
    "                while(True):\n",
    "                    if answer_token[mark_answer] == c_answer_token[mark_c_answer]:\n",
    "                        mark_answer += 1\n",
    "                        mark_c_answer += 1\n",
    "                        match += 1\n",
    "                    else:\n",
    "                        mark_c_answer -= (match-1)\n",
    "                        match = 0\n",
    "                        mark_answer = 0\n",
    "                    if match == len(answer_token):\n",
    "                        #print answer,'&',qa['answer']\n",
    "                        count_similar += float(len(answer_token))/len(c_answer_token)\n",
    "                        break\n",
    "                    if mark_c_answer >= len(c_answer_token):\n",
    "                        break     \n",
    "                \n",
    "        #answers.append(answer)\n",
    "print float(count_contains)/total_question, count_similar/total_question, float(count_correct)/total_question\n",
    "                \n",
    "# '''\n",
    "#     IV. Output\n",
    "# '''\n",
    "# for i in range(len(answers)):\n",
    "#     answers[i] = answers[i].replace(',','-COMMA-')\n",
    "#     answers[i] = answers[i].replace('\"','')\n",
    "# output_result(\"result_basic_final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
